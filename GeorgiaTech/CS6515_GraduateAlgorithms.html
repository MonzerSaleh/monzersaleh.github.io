<!DOCTYPE html>

<html>
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>0-CS6515_IntroGraduateAlgorithms</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>




<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration -->
<link href="https://monzersaleh.github.io/css/jlab.css" rel="stylesheet"/>
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet"/>
<link href="https://monzersaleh.github.io/css/mymenu_v1.css" rel="stylesheet"/>
</head>

<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light"><nav class="nav" id="leftMenu" role="navigation"><ul class="nav__list"><li><a href="/">Home Page</a></li><li><input hidden id="group-1" type="checkbox"/><label for="group-1"><span class="fa fa-angle-right"></span>Intro</label><ul class="group-list"><li><a href="#Basic-Details">Basic Details</a></li><li><a href="#Textbooks">Textbooks</a></li><li><a href="#Other-Resources">Other Resources</a></li><li><a href="#Schedule">Schedule</a></li><li><a href="#Grading">Grading</a></li></ul></li><li><input hidden id="group-2" type="checkbox"/><label for="group-2"><span class="fa fa-angle-right"></span>Refresher</label><ul class="group-list"><li><a href="#Pseudo-Code">Pseudo Code</a></li><li><a href="#Big-O-Notation">Big-O Notation</a></li><li><a href="#Summary">Summary</a></li><li><a href="#Graphs">Graphs</a></li></ul></li><li><input hidden id="group-3" type="checkbox"/><label for="group-3"><span class="fa fa-angle-right"></span>DP1 Dynamic Programming</label><ul class="group-list"><li><a href="#Overview">Overview</a></li><li><a href="#Class-Methodology">Class Methodology</a></li><li><a href="#Ex-1:-FIB-Fibonacci-Numbers">Ex 1 FIB-Fibonacci Numbers</a></li><li><a href="#Ex-2:-LIS-Longest-Increasing-Subsequence">Ex 2 LIS-Longest Increasing Subsequence</a></li><li><a href="#Ex-3:-LCS-Longest-Common-Subsequence">Ex 3 LCS-Longest Common Subsequence</a></li><li><a href="#Ex-4:-Contiguous-Subsequence">Ex 4 Contiguous Subsequence</a></li><li><a href="#1.x---Coins">1.x - Coins</a></li><li><a href="#Exercises">Exercises</a></li></ul></li><li><input hidden id="group-4" type="checkbox"/><label for="group-4"><span class="fa fa-angle-right"></span>DP2 Knapsack &amp; Chain Multiply</label><ul class="group-list"><li><a href="#Edit-Distance-(-Bonus-)">Edit Distance  Bonus </a></li><li><input hidden id="group-4" type="checkbox"/><label for="group-4"><span class="fa fa-angle-right"></span>2.0 Knapsack</label><ul class="sub-group-4"><li><a href="#V1---no-repetition">V1 - no repetition</a></li><li><a href="#V2---with-repetition">V2 - with repetition</a></li></ul></li><li><a href="#Ex---Electoral-College">Ex - Electoral College</a></li><li><a href="#2.1:-Chain-Multiply">2.1 Chain Multiply</a></li><li><a href="#Exercises">Exercises</a></li></ul></li><li><input hidden id="group-5" type="checkbox"/><label for="group-5"><span class="fa fa-angle-right"></span>DP3 Shortest Path Problem</label><ul class="group-list"><li><a href="#Intro---DP3">Intro - DP3</a></li><li><a href="#Single-Source---Bellman">Single Source - Bellman</a></li><li><a href="#Algo-Bellman-Ford">Algo Bellman Ford</a></li><li><a href="#Negative-Weights&amp;Cycles">Negative Weights&amp;Cycles</a></li><li><a href="#All-Pairs---Floyd">All Pairs - Floyd</a></li><li><a href="#Algo:-Floyd-Warshall">Algo Floyd-Warshall</a></li><li><a href="#Comparison-Ford-v-Floyd">Comparison Ford v Floyd</a></li><li><a href="#DP3-Exercises">DP3-Exercises</a></li><li><a href="#DP3-Summary">DP3-Summary</a></li></ul></li><li><input hidden id="group-6" type="checkbox"/><label for="group-6"><span class="fa fa-angle-right"></span>DC0 Divide &amp; Conquer</label><ul class="group-list"><li><a href="#Review---Binary-Search">Review - Binary Search</a></li><li><a href="#Review---Merge-Sort">Review - Merge Sort</a></li><li><a href="#Ex1---Max-Sub-Array-Sum">Ex1 - Max Sub-Array Sum</a></li></ul></li><li><a href="#DC1:-Fast-Integer-multiplication">DC1 Fast Integer multiplication</a></li><li><a href="#DC2:-Linear-Time-Median">DC2 Linear Time Median</a></li><li><input hidden id="group-9" type="checkbox"/><label for="group-9"><span class="fa fa-angle-right"></span>DC3 Solving Recurrances</label><ul class="group-list"><li><a href="#Example-Recurrances">Example Recurrances</a></li><li><a href="#Master-Theorem-Generalization">Master Theorem Generalization</a></li></ul></li><li><input hidden id="group-10" type="checkbox"/><label for="group-10"><span class="fa fa-angle-right"></span>DC4 FFT-Fast Fourier Transform</label><ul class="group-list"><li><a href="#P1:-Review-Polynomials-&amp;-Convolutions">P1 Review-Polynomials &amp; Convolutions</a></li><li><a href="#P2:--Review-Complex-Numbers">P2  Review-Complex Numbers</a></li><li><a href="#Roots-of-Unity">Roots of Unity</a></li><li><a href="#Examples-FFT">Examples-FFT</a></li></ul></li><li><input hidden id="group-11" type="checkbox"/><label for="group-11"><span class="fa fa-angle-right"></span>DC5 FFT-Algorithm</label><ul class="group-list"><li><a href="#Example">Example</a></li><li><a href="#FFT-Polynomial-Multiplication">FFT Polynomial Multiplication</a></li><li><a href="#Inverse-FFT">Inverse FFT</a></li><li><a href="#Ex-1">Ex 1</a></li><li><a href="#Ex-2">Ex 2</a></li><li><a href="#FFT-Summary">FFT Summary</a></li></ul></li><li><a href="#DC6:-Summary">DC6 Summary</a></li><li><a href="#RA0:-Randomized-Algorithms">RA0 Randomized Algorithms</a></li><li><input hidden id="group-14" type="checkbox"/><label for="group-14"><span class="fa fa-angle-right"></span>RA1 Modular Arithmetic</label><ul class="group-list"><li><a href="#Euclid's-Rule">Euclids Rule</a></li><li><a href="#Extended-Euclid">Extended Euclid</a></li></ul></li><li><input hidden id="group-15" type="checkbox"/><label for="group-15"><span class="fa fa-angle-right"></span>RA2 RSA Cryptosystem</label><ul class="group-list"><li><a href="#Fermat's-little-theorem">Fermats little theorem</a></li><li><a href="#Euler's-Theorem">Eulers Theorem</a></li><li><a href="#RSA:-Cryptography">RSA Cryptography</a></li><li><a href="#Primality-Testing">Primality Testing</a></li><li><a href="#Fermat-Witness">Fermat Witness</a></li></ul></li><li><a href="#RA3-Bloom-Filters---todo">RA3-Bloom Filters - todo</a></li><li><input hidden id="group-17" type="checkbox"/><label for="group-17"><span class="fa fa-angle-right"></span>GR0 Graph Algorithms</label><ul class="group-list"><li><a href="#Terminology">Terminology</a></li><li><a href="#Introduction">Introduction</a></li><li><a href="#BFS:-Breadth-First-Search---Review">BFS Breadth First Search - Review</a></li><li><a href="#Dijkstra's-Algorithm---Review">Dijkstras Algorithm - Review</a></li></ul></li><li><input hidden id="group-18" type="checkbox"/><label for="group-18"><span class="fa fa-angle-right"></span>GR1 Strongly Connected Components</label><ul class="group-list"><li><a href="#DFS-for-undirected-graphs">DFS for undirected graphs</a></li><li><a href="#DFS-for-directed-graphs">DFS for directed graphs</a></li><li><a href="#DFS-vs-Explore">DFS vs Explore</a></li><li><a href="#SCC---Strongly-Connected-Components">SCC - Strongly Connected Components</a></li><li><a href="#GraphSearch-Summary">GraphSearch-Summary</a></li></ul></li><li><input hidden id="group-19" type="checkbox"/><label for="group-19"><span class="fa fa-angle-right"></span>GR2 Satisfiability</label><ul class="group-list"><li><a href="#GR2-Intro">GR2 Intro</a></li><li><a href="#kSAT-Problem">kSAT Problem</a></li><li><a href="#2SAT-Algorithm">2SAT Algorithm</a></li></ul></li><li><input hidden id="group-20" type="checkbox"/><label for="group-20"><span class="fa fa-angle-right"></span>GR3 Minimum Spanning Tree</label><ul class="group-list"><li><a href="#Kruskal's-Algorithm">Kruskals Algorithm</a></li><li><a href="#Correctness">Correctness</a></li><li><a href="#Graph-Cut-&amp;-Min-Cut-Property">Graph Cut &amp; Min Cut Property</a></li><li><a href="#P1-Proof-T'-is-a-tree">P1-Proof T is a tree</a></li><li><a href="#P2-Proof-T-is-an-MST">P2-Proof T is an MST</a></li><li><a href="#Prim's-Algorithm">Prims Algorithm</a></li><li><a href="#Kruskal-v-Prim">Kruskal v Prim</a></li></ul></li><li><a href="#MF0:-Max-Flow-Problem">MF0 Max Flow Problem</a></li><li><input hidden id="group-22" type="checkbox"/><label for="group-22"><span class="fa fa-angle-right"></span>MF1 Ford-Fulkerson Algorithm</label><ul class="group-list"><li><a href="#Residual-Networks">Residual Networks</a></li><li><a href="#Ford-Fulkerson-Algo">Ford Fulkerson Algo</a></li></ul></li><li><input hidden id="group-23" type="checkbox"/><label for="group-23"><span class="fa fa-angle-right"></span>MF2 Max-Flow Min-Cut</label><ul class="group-list"><li><a href="#MaxFlow---MinCut-Theorem">MaxFlow - MinCut Theorem</a></li><li><a href="#Min-ST-cut-Problem-Formulation">Min ST-cut Problem Formulation</a></li><li><a href="#Properties">Properties</a></li></ul></li><li><input hidden id="group-24" type="checkbox"/><label for="group-24"><span class="fa fa-angle-right"></span>MF4 Edmonds-Karp Algorithm</label><ul class="group-list"><li><a href="#Edmonds-Karp">Edmonds Karp</a></li></ul></li><li><input hidden id="group-25" type="checkbox"/><label for="group-25"><span class="fa fa-angle-right"></span>MF5 Max-Flow Generalized</label><ul class="group-list"><li><a href="#Demand-Constraints">Demand Constraints</a></li><li><a href="#Feasible-iff-Saturating">Feasible iff Saturating</a></li><li><a href="#Example">Example</a></li></ul></li><li><a href="#MF3:-Image-Segmentation">MF3 Image Segmentation</a></li><li><a href="#NP0:-NP-Completeness">NP0 NP Completeness</a></li><li><input hidden id="group-28" type="checkbox"/><label for="group-28"><span class="fa fa-angle-right"></span>NP1 Intro and Examples</label><ul class="group-list"><li><a href="#SAT-Problem">SAT Problem</a></li><li><a href="#K-Colorings-Problem">K-Colorings Problem</a></li><li><a href="#MST-Problem">MST Problem</a></li><li><a href="#Knapsack-Problem">Knapsack Problem</a></li><li><a href="#Knapsack-Search-Modified-Knapsack">Knapsack-Search Modified Knapsack</a></li><li><a href="#Intuition">Intuition</a></li></ul></li><li><input hidden id="group-29" type="checkbox"/><label for="group-29"><span class="fa fa-angle-right"></span>NP2 NP-Complete Proofs - Pt1</label><ul class="group-list"><li><a href="#3SAT-NP-Complete">3SAT NP-Complete</a></li></ul></li><li><input hidden id="group-30" type="checkbox"/><label for="group-30"><span class="fa fa-angle-right"></span>NP3 NP-Complete Proofs - Pt2</label><ul class="group-list"><li><a href="#IS-Independent-Sets">IS Independent Sets</a></li><li><a href="#Clique">Clique</a></li><li><a href="#VC-Vertex-Cover">VC Vertex Cover</a></li><li><a href="#Travelling-Salesman-Problem">Travelling Salesman Problem</a></li><li><a href="#Problems">Problems</a></li><li><a href="#Summary">Summary</a></li></ul></li><li><input hidden id="group-31" type="checkbox"/><label for="group-31"><span class="fa fa-angle-right"></span>NP4 Knapsack</label><ul class="group-list"><li><a href="#Subset-Sum">Subset-Sum</a></li><li><a href="#Knapsack">Knapsack</a></li></ul></li><li><input hidden id="group-32" type="checkbox"/><label for="group-32"><span class="fa fa-angle-right"></span>NP5 Halting Problem</label><ul class="group-list"><li><a href="#Description">Description</a></li><li><a href="#Example">Example</a></li><li><a href="#Proof">Proof</a></li></ul></li><li><a href="#LP0:-Linear-Programming">LP0 Linear Programming</a></li><li><input hidden id="group-34" type="checkbox"/><label for="group-34"><span class="fa fa-angle-right"></span>LP1 Linear Programming</label><ul class="group-list"><li><a href="#2Dim-Example">2Dim Example</a></li><li><a href="#3Dim-Example">3Dim Example</a></li><li><a href="#Standard-Formulation">Standard Formulation</a></li><li><a href="#Simplex-Method">Simplex Method</a></li><li><a href="#Simplex-Example-3Dim">Simplex Example 3Dim</a></li></ul></li><li><input hidden id="group-35" type="checkbox"/><label for="group-35"><span class="fa fa-angle-right"></span>LP2 LP Geometry</label><ul class="group-list"><li><a href="#Feasiblity">Feasiblity</a></li><li><input hidden id="group-35" type="checkbox"/><label for="group-35"><span class="fa fa-angle-right"></span>LP Duality</label><ul class="sub-group-35"><li><a href="#Example">Example</a></li></ul></li></ul></li><li><a href="#LP3:-Duality---Weak-and-Strong">LP3 Duality - Weak and Strong</a></li><li><input hidden id="group-37" type="checkbox"/><label for="group-37"><span class="fa fa-angle-right"></span>LP4 Max-SAT Approximation</label><ul class="group-list"><li><a href="#Simple-Algo">Simple Algo</a></li><li><a href="#Ek-SAT">Ek-SAT</a></li><li><a href="#ILP---Integer-LP">ILP - Integer LP</a></li><li><a href="#Summary">Summary</a></li><li><a href="#Comparison:-Simple-v-LP">Comparison Simple v LP</a></li></ul></li><li><a href="#End-of-Notes">End of Notes</a></li><li><a href="#EX1-:-GR4-Markov-Chains-and-Page-Rank---todo">EX1  GR4 Markov Chains and Page Rank - todo</a></li><li><input hidden id="group-40" type="checkbox"/><label for="group-40"><span class="fa fa-angle-right"></span>Slack Notes</label><ul class="group-list"><li><a href="#DP-vs-Memoization">DP vs Memoization</a></li></ul></li><li><input hidden id="group-41" type="checkbox"/><label for="group-41"><span class="fa fa-angle-right"></span>Exam 2</label><ul class="group-list"><li><a href="#Week-2">Week 2</a></li></ul></li><li><input hidden id="group-42" type="checkbox"/><label for="group-42"><span class="fa fa-angle-right"></span>Appendix</label><ul class="group-list"><li><a href="#Feasible-&amp;-Saturating-Solution">Feasible &amp; Saturating Solution</a></li></ul></li></ul></nav><div id="contentBody"><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="CS6515---Intro-to-Graduate-Algorithms">CS6515 - Intro to Graduate Algorithms<a class="anchor-link" href="#CS6515---Intro-to-Graduate-Algorithms">¶</a></h1>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Intro">Intro<a class="anchor-link" href="#Intro">¶</a></h2><h3 id="Basic-Details">Basic Details<a class="anchor-link" href="#Basic-Details">¶</a></h3><p>Georgia Tech: Spring 2023<br/>
Instructor: Eric Vigoda<br/>
Website: <a href="https://omscs.gatech.edu/cs-6515-intro-graduate-algorithms">https://omscs.gatech.edu/cs-6515-intro-graduate-algorithms</a><br/>
Videos: <a href="https://omscs.gatech.edu/cs-6515-graduate-algorithms-course-videos">https://omscs.gatech.edu/cs-6515-graduate-algorithms-course-videos</a></p>
<p>Old Course Webpage (Use at your own risk - looks pretty good to me)</p>
<ul>
<li><a href="https://gtalgorithms.wordpress.com/">https://gtalgorithms.wordpress.com/</a> </li>
</ul>
<p>Old course on youtube, Looks awesome!!</p>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLAwxTw4SYaPkbWSEj_1iO7rILlWDJImW4">https://www.youtube.com/playlist?list=PLAwxTw4SYaPkbWSEj_1iO7rILlWDJImW4</a></li>
</ul>
<h3 id="Textbooks">Textbooks<a class="anchor-link" href="#Textbooks">¶</a></h3><ul>
<li>Official<ul>
<li>Algorithms by Dasgupta, C. H. Papadimitriou, and U. V. Vazirani</li>
<li><a href="http://algorithmics.lsi.upc.edu/docs/Dasgupta-Papadimitriou-Vazirani.pdf">http://algorithmics.lsi.upc.edu/docs/Dasgupta-Papadimitriou-Vazirani.pdf</a></li>
</ul>
</li>
<li>Secondary<ul>
<li>Algorithm Design by J. Kleinberg and E. Tardos</li>
<li><a href="https://ict.iitk.ac.in/wp-content/uploads/CS345-Algorithms-II-Algorithm-Design-by-Jon-Kleinberg-Eva-Tardos.pdf">https://ict.iitk.ac.in/wp-content/uploads/CS345-Algorithms-II-Algorithm-Design-by-Jon-Kleinberg-Eva-Tardos.pdf</a></li>
</ul>
</li>
</ul>
<p>Lecture/Videos</p>
<ul>
<li><a href="https://omscs.gatech.edu/cs-6515-graduate-algorithms-course-videos">https://omscs.gatech.edu/cs-6515-graduate-algorithms-course-videos</a></li>
</ul>
<p>Topics Covered</p>
<ul>
<li>DP: Dynamic Programming</li>
<li>RA: Randomized Algorithms</li>
<li>DC: Divide and Conquer</li>
<li>GR: Graph Algorithm</li>
<li>MF: MaxFlow Problems</li>
<li>LP: Linear Programming</li>
<li>NP: Nondeterministic Polynomial time Problems</li>
</ul>
<h3 id="Other-Resources">Other Resources<a class="anchor-link" href="#Other-Resources">¶</a></h3><ul>
<li>MIT: <a href="https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-fall-2011/resources/lecture-videos/">https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-fall-2011/resources/lecture-videos/</a> </li>
<li>UWashington: <a href="https://courses.cs.washington.edu/courses/cse417/">https://courses.cs.washington.edu/courses/cse417/</a> </li>
<li>UIllinois: <a href="https://mfleck.cs.illinois.edu/building-blocks/index-sp2020.html">https://mfleck.cs.illinois.edu/building-blocks/index-sp2020.html</a> </li>
<li>UIllinois: <a href="https://jeffe.cs.illinois.edu/teaching/algorithms/#notes">https://jeffe.cs.illinois.edu/teaching/algorithms/#notes</a></li>
</ul>
<h3 id="Schedule">Schedule<a class="anchor-link" href="#Schedule">¶</a></h3><ul>
<li>W01 - Dynamic Programming (Lectures: DP1)</li>
<li>W02 - Dynamic Programming (Lectures: DP2, DP3)</li>
<li>W03 - Divide and Conquer I (Lectures: DC1, DC3, DC4)</li>
<li>W04 - Divide and Conquer II (Lectures: DC5, DC2)<ul>
<li>Exam 1 (W01-W04)</li>
</ul>
</li>
<li>W05 - Modular Arithmetic and RSA (Lectures: RA1, RA2)    </li>
<li>W06 - Graph Algorithm I (Lectures: GR1,GR2)</li>
<li>W07 - Graph Algorithm II and Max Flow I (Lectures: GR3,MF1)</li>
<li>W08 - Max Flow II  (Lectures: MF2,MF3,MF5)</li>
<li>W09 - Max Flow III (Lectures: MF4)<ul>
<li>Exam 2 (W05-W09)</li>
</ul>
</li>
<li>W10 - Np Completeness (Lectures: NP1,NP2,NP3)</li>
<li>W11 - Spring Break</li>
<li>W12 - Linear Programming (Lectures: LP1,LP2,LP3)</li>
<li>W13 - NP and LP  (Lectures: LP4 &amp; NP4)    </li>
<li>W14 - More Complexity  (Lectures: NP5)<ul>
<li>Exam 3 (W10-W13)</li>
</ul>
</li>
<li>W15 - Markov Chains</li>
<li>W16 - Final Week<ul>
<li>Final Exam - Cumulative - Optional</li>
</ul>
</li>
</ul>
<h3 id="Grading">Grading<a class="anchor-link" href="#Grading">¶</a></h3><ul>
<li>14% Homework</li>
<li>7% Coding Projects</li>
<li>7% Mini-Quizzes</li>
<li>3% Logistic Quizzes</li>
<li>69% Exams<ul>
<li>23% a piece</li>
</ul>
</li>
</ul>
<ul>
<li>A [85-100%]</li>
<li>B [70-85%]</li>
<li>C [50-70%]</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Refresher">Refresher<a class="anchor-link" href="#Refresher">¶</a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Pseudo-Code">Pseudo Code<a class="anchor-link" href="#Pseudo-Code">¶</a></h3><p>There doesn't appear to be a definitive guide to pseudo code. In the loosest sense of the phrase it is somewhere between mathematical notation and code. The goal is to describe an algorithm such that any programmer can understand it, and implement it, without needing to know a particular programming language.</p>
<p>Of course what this means is that algorithm written using constructs found only in some programming language would automatically be dis-qualified. ie range(0,5) is found in python and should not be used. There are some exceptions to this, sum/min/max are so prevalent that there existence is virtually guaranteed. Thus they are allowed in this class. Another interesting tip to use the word as a function. length(A) would be allowed as it's meaning can be inferred, while A.len() would not be allowed.</p>
<p><strong>Dot Notation</strong><br/>
What is also not allowed is any form of dot notation. ie T.max(), S.sum() etc etc. This is unique to OOP style and is not found in many languages. But max(T), max(T[$\cdot$]) are both allowed.</p>
<p><strong>Scoping</strong> Is done using indentation, not using {}</p>
<p><strong>Array Indexing</strong> can be done using either () or []</p>
<ul>
<li>however array indexing like A[3:5] is not allowed nor are little tricks like A[-1]</li>
</ul>
<p><strong>Checking</strong></p>
<ul>
<li>Allowed: equality(==,=) and inequality(!=,&lt;&gt;) </li>
</ul>
<pre><code>BAD Examples                        Good examples

# This is basically python
for i in range(1,n+1):              for i = 1 to n 
    t[i] += 1                         t[i] = t[i] + 1

# This looks like C/C++ 
for( i=1; i&lt;=n; i++ ){              for i = 1 -&gt; n
    t[i]++ ;                          t[i] = t[i] + 1
    }
                                    for i = 1...n
                                      t[i] = t[i] + 1

# -1 indexing is                    
# not language agnostic             # returns the last element
return T[-1]                        return T[n]

# dot notation not allowed          # simple functions allowed
x = A.size()                        x = length(A)
y = x.len()</code></pre>
<p>Each algorithm is graded on four parts:</p>
<p>a. Define the entries of your table in words. E.g., T(i) or T(i, j) is ...</p>
<p>b. State recurrence for entries of your table in terms of smaller subproblems.</p>
<p>c. Write pseudocode for your algorithm to solve this problem.</p>
<p>d. State and analyze the running time of your algorithm.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Big-O-Notation">Big-O Notation<a class="anchor-link" href="#Big-O-Notation">¶</a></h3><p>Funny Tid-Bit:<br/>
The O in Big O stands for Ordnung, which is german and roughly translates to Order of approximation.</p>
<p><strong>Formal Definition</strong><br/>
Let f(n) and g(n) be functions from positive integers to positive reals. We say f=O(g), which means that "f grows no faster than g", if there is a constant c&gt;0 such that f(n) &lt;= c*g(n)</p>
<p>Let's illustrate this with a simple example:</p>
<ul>
<li>Let $f_1(n)=n^2$ and $f_2(n)=2n + 20$</li>
</ul>
<p>Which of these two is better? In this case it depends on n. For a small n, say less than 5, $n^2$ will be smaller and thus we may say that $f_2(n)$ is better. But for n&gt;5 it is clear that $f_1(n) \gt f_2(n)$ so we may say that $f_1(n)$ scales better. We can also find a constant c as follows:</p>
$$\large \frac{f_2(n)}{f_1(n)} = \frac{2n+20}{n^2} \le 22 $$<p>for all n. So by the formal definition we say $f_2=O(f_1)$, and this makes sense, as 2n+20 will certainly never grow faster than $n^2$. Conversely, we may try to compute $f_2(n)/f_1(n)$ only to find that this can grow arbitrarily large. So of course we may NOT say $f_1=O(f_2)$. Because $n^2$ can certainly grow faster than 2n+20, and you cannot find a constant upper bound.</p>
<p>Now we suppose we introduce $f_3(n) = n + 1$. Of course $f_3$ is better than $f_2$, but only by a technicallity ... or by a constant. So it really make no difference in terms of complexity. $n+1 \le c * (2n+20)$ is true for say c=1</p>
<p>When we calculate Big-O notation we only care about the dominant term. We will drop constants and minor terms. So for ex: $2n^3-2n+55$ has complexity $O(n^3)$.</p>
<p>A couple of terms you should also be aware of are: $\Omega$, $\Theta$</p>
<ul>
<li>Omega: $f(n)$ is $\Omega(g(n))$ iff for some constant c and $N_0 \; f(N) \ge c*g(n)$ for all N &gt; $N_0$<ul>
<li>this describes the lower bound of complexity</li>
<li>A function f grows at a rate no slower than g as they head toward infinity</li>
</ul>
</li>
<li>Theta: $f(n)$ is $\Theta(g(n))$ iff f(n) is O(g(n)) and f(n) is $\Omega(g(n))$<ul>
<li>this describes the exact bound</li>
<li>A function f grows at a rate equivalent to g as they head toward infinity</li>
</ul>
</li>
<li>Little O: $f(n)$ is o(g(n)) iff f(n) is O(g(n)) and f(n) is not $\Theta(g(n))$<ul>
<li>This describes the upper bound O(n) excluding the exact bound</li>
</ul>
</li>
</ul>
<p><img src="CS6515_images/W00-001.png" width="400;"/></p>
<h3 id="Summary">Summary<a class="anchor-link" href="#Summary">¶</a></h3><p>Review of big-O notation<br/>
Some simple rules</p>
<ul>
<li>Multiplicative constants are ommitted $14n^2$ becomes $n^2$               </li>
<li>$n^a$ dominates $n^b$ if a &gt; b: ie $n^2$ dominates $n$            </li>
<li>Any exponential dominates any polynomial: $3^n$ dominates $n^5$, it also dominates $2^n$           </li>
<li>Polynomials dominate any logarithm, n dominates $(\log n)^3$        <ul>
<li>this also means for example that $n^2$ dominates $(n \log n)$</li>
</ul>
</li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Time_complexity#Table_of_common_time_complexities">Wikipedia Table</a></p>
<p>From Office hours:</p>
<ul>
<li><strong>Expected:</strong> Proper Big-O notation and Sufficient explanation       </li>
<li><p><strong>Not Expected:</strong> Lower bound or average time analysis, nor is Space Complexity Analysis expected</p>
</li>
<li><p>Worst Case Runtime</p>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Graphs">Graphs<a class="anchor-link" href="#Graphs">¶</a></h3><p>A graph is simply a collection of pairs. Integers, people, cities etc etc. We generally talk about the underlying objects as <strong>vertices, or nodes</strong> and the pair themselves as an <strong>edge, or arc</strong>.</p>
<p>Formally a simple graph is a pair of sets (V,E), where V is an arbitrary non-empty finite set, whose elements are called <strong>vertices or nodes</strong>, and E is a set of pairs of elements of V, which we call <strong>edges</strong>. In an undirected graph the edges are unordered pairs, or just sets of size two we write <strong>uv</strong> to denote the edge between u and v. In a directed graph, the edges are ordered pairs of vertices. Here we write $u \to v$ to denote the directed edge. In either case the endpoints are simple the edges, ie u and v. However in a directed edge $u \to v$ we would label u the tail and v the head.</p>
<p>Mathematically we write</p>
<ul>
<li>$\mathcal{V} = \{ v_0, \cdots , v_n \}$</li>
<li>$\mathcal{E} = \{ e_0, \cdots , e_m \}$</li>
<li>$\mathcal{G} = \{ \mathcal{V}, \mathcal{E} \}$</li>
</ul>
<ul>
<li>Simple graph - is one without loops or parallel edges</li>
<li>Degree (vertex) - is the number of edges connecting it </li>
<li>Cycle - a path that starts from a given vertex and ends at the same vertex</li>
<li>Connected ( Graph ) - is one where a path exists between every distinct pair of vertices.</li>
<li>Connected Component of a Graph <ul>
<li>is a proper subgraph of a graph G that is not a proper subgraph of any connected subgraph</li>
<li>ie it's maximally connected subgraph</li>
</ul>
</li>
<li>Tree - is a connected graph with no cycles</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DP1:-Dynamic-Programming">DP1: Dynamic Programming<a class="anchor-link" href="#DP1:-Dynamic-Programming">¶</a></h2><p>Our first topic is dynamic programming. The key to really mastering dynamic programming is to do lots and lots of practice problems. Dynamic programming (DP) is a style of algorithms that relies on combining the solutions of subproblems to arrive at the solution of a larger problem in an efficient manner. A standard example of this is the Fibonnacci numbers which are defined recursively. As we will see without DP a fibonacci solution will generally repeatedly perform a great deal of work in order to arrive at the final solution. Typically, but not always, DP is used to solve optimization problems which can have multiple outcomes, but which also have an ideal or optimal solution.</p>
<p>In general DP algorithms can be designed using the following methodology:</p>
<ol>
<li>Characterize the structure of an optimal solution</li>
<li>Define the value of the optimal solution in recursive terms</li>
<li>Compute the value of an optimal solution (Typically in a bottom up fashion)</li>
<li>[opt] Construct an optimal solution from computed information</li>
</ol>
<h3 id="Overview">Overview<a class="anchor-link" href="#Overview">¶</a></h3><p>We'll start with the toy example,</p>
<ul>
<li>computing Fibonacci numbers, to illustrate the basic idea of dynamic programming.</li>
<li>Then, we'll dive into a variety of example problems to get a feel for the different styles of DP algorithms;<ul>
<li>Longest increasing subs sequence (LIS),</li>
<li>longest common subsequence (LCS) </li>
<li>the classic Knapsack Problem,</li>
<li>chain matrix multiplication </li>
</ul>
</li>
<li>finally<ul>
<li>we'll look at a few shortest path algorithms using DP.</li>
</ul>
</li>
</ul>
<p><strong>Methodology</strong></p>
<p>Here is the approach we will take to design our algorithms, Note that this is an iterative process so you may need to go back a step and reformulate in order to move forward</p>
<ol>
<li>Find and define the subproblem. All DP problems can be deconstructed into many subproblems. This is the key to DP! </li>
</ol>
<ul>
<li>Define the subproblem in words</li>
<li>Define on a subset or prefix of data, ie a smaller problem</li>
<li>Consider possible constraints, or cases.</li>
<li>Note that the final problem need not be the same as the subproblem, but it does need to follow, logically, from it</li>
</ul>
<p>An alternative way to think of this is: Consider Dynamic Programming as using arrays to define the problem - the first step of defining the subproblem would be defining what is going into each index of the array in words.</p>
<ul>
<li>ie For LIS, the subproblem is T(i) = length of the LIS which includes a[i]</li>
</ul>
<p>Each index (i) will have the length of the longest increasing subsequence including the current index of the original array (a[i])</p>
<ol>
<li>find/define the recursive relationship</li>
</ol>
<ul>
<li>What are the base case(s)?</li>
<li>This is much more mathematical than it seems <ul>
<li>May often require hand worked solutions</li>
<li>May require speculation on variations, often these are not presented in the problem definition</li>
</ul>
</li>
<li>in general most situations will fall into one of two types of recursion<ul>
<li>a) 1 dimensional T(i) as a function of T(1),...,T(i-1)</li>
<li>b) 2 dimensional T(i,j) as a function of T(i-1,j),T(i,j-1), etc etc</li>
</ul>
</li>
</ul>
<ol>
<li>Pseudo-code: </li>
</ol>
<ul>
<li>This is where we now define our DP algorithm. Presumably by now you should have solved the problem and subproblem. Just a quick reminder this is more mathematical and is certainly not code. </li>
</ul>
<ol>
<li>Time-Analysis: </li>
</ol>
<ul>
<li>As with any algorithm design you should be able to compute, and articulate, the runtime of your algorithm.</li>
</ul>
<p><strong>MIT Methodology</strong></p>
<p>This comes from the MIT open course ware videos</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=r4-cftqTcdI&amp;list=PLUl4u3cNGP63EdVPNLG3ToM6LaEUuStEY&amp;index=26">https://www.youtube.com/watch?v=r4-cftqTcdI&amp;list=PLUl4u3cNGP63EdVPNLG3ToM6LaEUuStEY&amp;index=26</a></li>
</ul>
<p>Recursive Algorithm Design Paradigm</p>
<ul>
<li>Subproblem definition</li>
<li>Relate subproblems solutions recursively (ie look for a recursive pattern)</li>
<li>Topological order on subproblems to guarantee acyclic<ul>
<li>Subproblem/call should form a DAG, else it can fall into infinity</li>
</ul>
</li>
<li>Base cases of relation</li>
<li>Original problem: solved via subproblems</li>
<li>Time analysis</li>
</ul>
<p><strong>Tips and Tricks</strong></p>
<p>If the input = sequence of X with length n</p>
<p>good subproblems are:</p>
<ul>
<li>prefixes x[:i]</li>
<li>suffixes x[i:]</li>
<li>substrings x[i:j]</li>
<li>all of which are polynomial, first two are linear </li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Class-Methodology">Class Methodology<a class="anchor-link" href="#Class-Methodology">¶</a></h3><p>This is the method advocated by this class</p>
<p><strong>FIRST :</strong> Define the subproblem in words</p>
<ul>
<li><strong>Provide a short explanation in English explaining the meaning of each table entry</strong></li>
<li>almost all DP problems revolve around an array of 1 or more dimensions<ul>
<li>so define the values of this array in words</li>
<li>ie T(i,j) is the min sum of values up to i,j index</li>
</ul>
</li>
<li>consider constraints<ul>
<li>can i,j be negative? </li>
<li>is there a boundary? (There almost always is !)</li>
</ul>
</li>
</ul>
<p><strong>SECOND :</strong> Define the Recurrance relationship in terms of subproblems</p>
<ul>
<li><strong>This is the mathematical definition or specification of the table defined in (1)</strong></li>
<li>What base cases are required? <ul>
<li>Usually T(0) or T(i,0) need to be set to something</li>
<li>without a base case what would your first recurrance execution use?</li>
<li>Also These cannot part be part of your recurrance!</li>
<li>so if T(0) is your base case your recurrance begins at 1</li>
</ul>
</li>
<li>T(i) in terms of T(i-1) or T(i-2)?<ul>
<li>here T(i-1) &amp; T(i-2) are your subproblems. </li>
<li>They have been solved and are used to solve the current case T(i)</li>
</ul>
</li>
<li>you may need to use a max/min here</li>
<li>you may end of with a piecewise function</li>
</ul>
<p><strong>THIRD :</strong> Write out the algorithm in pseudocode</p>
<ul>
<li>Write out your pseudocode (see above refresher)                   <ul>
<li>This includes the base case</li>
<li>This should also include the recurrance            </li>
</ul>
</li>
<li>think of this as blending math and programming               </li>
<li>if your pseudocode would compile chances are it is not good!              </li>
</ul>
<p><strong>FOURTH :</strong> Perform a runtime analysis</p>
<ul>
<li>this must be done in Big-O notation</li>
<li>can use words too, but still use the big-O</li>
<li>Some people like to annotate their pseudocode with Big-O and then reference it</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ex-1:-FIB-Fibonacci-Numbers">Ex 1: FIB-Fibonacci Numbers<a class="anchor-link" href="#Ex-1:-FIB-Fibonacci-Numbers">¶</a></h3><p>Given an integer n, we're going to look at an algorithm for generating the n Fibonacci number. This will be a very simple algorithm, but it will illustrate the idea of dynamic programming and then, we'll look at dynamic programming in general; the techniques for designing a dynamic programming algorithm and we'll look at some more sophisticated examples.</p>
<p>Recall the Fibonacci numbers are the following sequence, 0, 1, 1, 2, 3, 5, 8,13, 21, 34 and so on. There's a simple recursive formula that defines the Fibonacci numbers. The first two numbers in the sequence are 0 and 1, and then the n Fibonacci number (n&gt;0) is the sum of the previous two Fibonacci numbers. We're going to take as input a non-negative integer n. Our goal is to output the n'th Fibonacci number. We want an efficient algorithm and therefore we're aiming for a running time which is polynomial in n.</p>
<p>Now, the Fibonacci numbers are defined by this simple recursive formula. Therefore, we might think a recursive algorithm is a natural algorithm for this problem. Let's look at that natural algorithm, that natural recursive algorithm and then we'll analyze it.</p>
<p>A very simple algo using recursion</p>
<pre><code>Fib1(n)

In : n &gt;= 0
Out: f(n)


if n = 0 then return 0            O(1)
if n = 1 then return 1            O(1)
return (Fib1(n-1)+Fib1(n-2))      T(n-1)+T(n-2)</code></pre>
<p>Let's now consider the runtime of our algorithm. The times are provided at the right.</p>
<ul>
<li>$T(n) \le O(1) + T(n-1)+T(n-2)$</li>
<li>what this really just amounts to is the sum of the recursive steps + the base case run time. </li>
<li>you may also recall from an old math class that </li>
<li>$F_n \approx \frac{\phi^n}{\sqrt{5}}$ which is the golden ratio ($\phi \approx 1.618$)</li>
<li>Which effectively tells us that the runtime of the recursive approach is exponential</li>
</ul>
<p>If you were to draw this out as a tree you would quickly that each recursive branch will perform many of the same computations. For example take n = 5. then both the first branch Fib1(n-1) as well as Fib1(n-2) would require the value Fib1(n-3). And they would compute it independently. Frankly this is unnecassary. if we can devise a method where by we can re-use computation it would reduce the running time by 50%!</p>
<p><strong>$\color{red}{\text{Algorithm}}$</strong></p>
<pre><code>Fib2(n)

F[0]=0
F[1]=1                     O(1)

for i=2 to n               O(n) 
    F[i]=F[i-1]+F[i-2]

return F[n]</code></pre>
<p>Notice key differences here</p>
<ul>
<li>we use matrix notation to denote previous values in contrast to the previous algo which used function</li>
</ul>
<p>Also notice that the run time is O(1)+T(n) which is linear. This is a significant improvement over the recursive approach.</p>
<p>Key Take-aways</p>
<ul>
<li>No recursion in a Dynamic Programming solution</li>
<li>we don't use any hashtables or memoization, which is a popular approach that makes use of a memory</li>
<li>Practice, Practice, Practice!! Is the only way to get better at this.</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ex-2:-LIS-Longest-Increasing-Subsequence">Ex 2: LIS-Longest Increasing Subsequence<a class="anchor-link" href="#Ex-2:-LIS-Longest-Increasing-Subsequence">¶</a></h3><p>Our next example is LIS, longest increasing subsequence, in a given set of n numbers $a_1,a_2,\cdots,a_n$. Note the goal here is just the length of the subsequence. Not the subsequence itself.</p>
<p>Input: 5, 7, 4, -3, 9, 1, 10, 4, 5, 8, 9, 3</p>
<p>Substring: Set of consecutive elements, For example</p>
<ul>
<li>-3, 9, 2, 10       </li>
<li>4         </li>
<li>9, 1, 10, 4, 5, 8, 9, 3          </li>
</ul>
<p>Subsequence: any subset of a elements in order (skipping elements is allowed). For example</p>
<ul>
<li>4, -3, 1, 9</li>
<li>1,</li>
<li>5, 7, 3</li>
</ul>
<p>finally an increasing subsequence should be clear</p>
<ul>
<li>5, 7,  3</li>
<li>4, 9, 10</li>
</ul>
<p>Now just find the longest one which we can see is -3, 1, 4, 5, 8, 9. with length 6</p>
<p>Designing a DP algorithm</p>
<p>Step 1 : define the subproblem in words (ex: F[i] is the i'th fibonnacci number )<br/>
Step 2 : state the recursive relation (ex: F[i] = F[i-1] + F[i-2])</p>
<p>For our LIS:<br/>
Step 1: Let L(i) = length of the LIS on $a_1,a_2,\cdots,a_i$<br/>
Step 2: Now we need to express L(i) in terms of L(1),...L(i-1)</p>
<p>This gets a bit tricky ...<br/>
Consider the input from before</p>
<pre><code>1, 2, 3,  4,  5,  6,  7,  8,  9, 10, 11, 12       index
5, 7, 4, -3,  9,  1, 10,  4,  5,  8,  9,  3          
Let's now consider the length of the LIS up to that point
1, 2, 2,  2, 3, 4,  4, 4, 4, 5   Let's pause and consider 

5, 7, 4, -3, 9, 1, 10, 4, 5,    lis =  5, 7, 9,10    &amp;&amp; -3,1,4,5
but 
5, 7, 4, -3, 9, 1, 10, 4, 5, 8  lis = -3, 1, 4, 5, 8 
why did this happen! Ponder a bit ... 
a few minutes later you'll see that it's the last digit that makes the difference

Now we need to move towards designing an algorithm. There is really no silver bullet approach. In a nutshell if you can do it by hand then you can get a computer to do it.</code></pre>
<p>In this case after some trial and error we came up with the following recurrance<br/>
$L(i) = 1 + max_j\{L(j) : a_j &lt; a_i \&amp; j &lt; i \}$</p>
<p>Now we can design an algorithm that</p>
<p><strong>$\color{red}{\text{Algorithm}}$</strong></p>
<pre><code>LIS(a1,s2,...,an):
for i=1-&gt;n:
    L(i)=1
    for j=1-&gt;(i-1):
        if aj &lt; ai &amp; L(i) &lt; 1+L(j):
            then L(i)=1+L(j)

# v1 presented in video
max = 1
for i=2-&gt;n:
    if L(i) &gt; L(max) 
        then max = i
return L(max)

# v2 
return max(L(:))</code></pre>
<p>What is the running time here? We have T(n^2) for the first loop with an inner loop and T(n) for the second. Therefore we have T(n^2)+T(n) which equates to O(n^2)</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ex-3:-LCS-Longest-Common-Subsequence">Ex 3: LCS-Longest Common Subsequence<a class="anchor-link" href="#Ex-3:-LCS-Longest-Common-Subsequence">¶</a></h3><p>Input 2 Strings:</p>
<ul>
<li>X = x_1,x_2,...,x_n     </li>
<li>Y = y_1,y_2,...,y_n       </li>
</ul>
<p>Goal: the length of the longest string that appears as a subsequence of both X &amp; Y</p>
<p>As an example consider:<br/>
X = BCDBCDA<br/>
Y = ABECBAB<br/>
Then the longest subseq is BCBA with a length of 4!</p>
<p>Similar to before we will now look at our two step process</p>
<p>Step 1: Design subproblem in words</p>
<ul>
<li>Try the same problem on a prefix of input</li>
<li>for $1 \le i \le n$ <ul>
<li>Let L(i)=length of LCS in X[1,i],Y[1,i]</li>
</ul>
</li>
</ul>
<p>Step 2: state the recursive relation, Express L(i) in term of L(1) to L(i-1)</p>
<ul>
<li>???</li>
</ul>
<p>This one turns out to be a bit more challenging then the previous LIS</p>
<pre><code>(X,Y)=BDC,ABE     -&gt; then LCS=B   &amp; L(3)=1      
(X,Y)=BDCB,ABEC   -&gt; then LCS=BC  &amp; L(4)=2    
(X,Y)=BDCBC,ABECB -&gt; then LCS=BCB &amp; L(5)=3</code></pre>
<p>In order to get a smaller subproblem, we're going to look at the last character. So we're going to look at X_i and Y_i.</p>
<p>We're going to look at how X_i and Y_i are used in the solution to l of i and then,
 we can use the solution to the subproblem of size i minus one.
 We take the optimal solution for the subproblem of size i minus one and then
 we append on the solution for X_i and Y_i.</p>
<p>Now, there are two cases to consider,</p>
<ul>
<li>either of these last characters are different,(in this case)</li>
<li>or they're the same.</li>
</ul>
<p>We're going to consider these two cases separately.</p>
<ul>
<li>The first case is when the last characters X_i and Y_i are the same.</li>
<li>The second case is when the last characters X_i and Y_i are different.</li>
</ul>
<p>We're going to do this case first. (X_i = Y_i) The last characters are the same. This turns out to be the easy case. Now, let's modify our example so that the last characters are the same.</p>
<p>So, I append on character C onto the end of both strings. Now, in this case, where the last character is the same,</p>
<ul>
<li>X = BCDBCDAC           </li>
<li>Y = ABECBABC   </li>
</ul>
<p>What do we know about the longest common subsequence? Well, we know it must include and must end in this common character.</p>
<p>Why? Well, it gives me a common subsequence and suppose it does not include this last character. Well, then, I can append on this common character and I get a longer subsequence. So, therefore, the longest common subsequence must include this last character.  So, in this case, where X_i equals Y_i,  what do we know about L(i)? We know that the longest common subsequence includes this last character.</p>
<p>So, we get one in the length for that common character and then we can simply drop this last character and then we can take
this input sequence of length (i-1) and we can take the longest common subsequence in this input sequence of length i-1 and append on this common character C. What is the length of the longest common subsequence in this input? It's simply l(i-1). So, we have a recursive relation: L(i)=1+L(i-1)</p>
<p>We can express l of i in terms of l of i minus one. This handles a case when X_i equals Y_i. Now, let's take a look at the case when X_i is not equal to Y_i.</p>
<p>Let's take a look now, at the case when the last characters are different.
This the situation in our original example,</p>
<ul>
<li>X = BCDBCDA           </li>
<li>Y = ABECBAB  </li>
</ul>
<p>where Xi is not equal to Yi, A is not equal to B.</p>
<p>Now in this case, when Xi is not equal to Yi, the last character of the longest common subsequence can either be A or B, or neither. but not be both.</p>
<p>Suppose A is the last character in The Long common subsequence. And what we know about Yi is B, but B has nothing left to match with in X. Therefore, we know the LCS does not include B Yi. Similarly, if the last character is B says match with this B. Then the LCS cannot include A, because it has nothing left to match within Y.</p>
<p>The key point is that the longest common subsequence for this prefix of length i, either does not include Xi or it does not include Yi or both. So either Xi is dropped or Yi is dropped or both of them are dropped. Now let's consider the three cases.
If both of them are dropped, then we can simply take the longest common subsequence in this prefix of L(i-1). So it's similar to the equal case except we don't get this plus one here. Now what happens if just Xi is dropped?. Well then we have a prefix of L(i-1) in X and a prefix of L(i) in Y. So we have no way of looking this up in our table. The solution to this sub problem is not in our table, because the prefixes are of different length in X and Y. And notice, even if we knew how Yi is matched up with an X, for instance, if we knew this B was match with this B, then we have a prefix of Length 3 and X and we have a prefix of length 6 and y. So there's still a different length.</p>
<p>Similarly, if Yi is not included, So we dropped this last character from Y then we have prefix of length 7 in X and a prefix of length 6 in Y. So the prefixes are of different length in X and Y. And once again, the solution to this sub problem is not in our table because these prefixes are on a different length. So for this case where where Yi is dropped, we need to look up the longest common subsequence in the prefix of L(i) in X with the prefix of L(i-1) in Y. Now this isn't in our table presently and similarly when we try to solve this problem, well then we might chop off the last character from Y and then we get even shorter prefixes in length Y. So for this sub problem definition, we are unable to define a recurrence. We are unable to express L(i) in terms of smaller sub problems, but we got some insight about what is a valid. What is a good sub problem definition. The difficulty here was that the prefixes are of the same length for x and y, but we need to allow them to be of different lengths.</p>
<p>So how do we achieve that?. We're going to change from a single parameter i to a pair of parameters i &amp; j. These correspond to the length of the prefix X and j will correspond to the length of the prefix Y. And, our table will now be a two dimensional table. So L(i,j) will be the length of the longest common subsequence in X1 through Xi with Y1 through Yj. And then, we're going to try all possibilities for i and j.</p>
<p>Let's now try to repeat our steps using this insight. ( With two indices i&amp;j and a 2-Dim table ).</p>
<p>Subproblem Def</p>
<ul>
<li>For i&amp;j where $1\le i\le n$ and $1\le j\le n$<ul>
<li>Let L(i,j)=length(LCS) in X[1,i],Y[1,j]</li>
</ul>
</li>
</ul>
<p>Recurrence</p>
<ul>
<li>for j=0 L(i,0)=0 </li>
<li>for i=0 L(0,j)=0 </li>
<li>This takes care of the base cases</li>
<li>This leads to three cases for unequal last characters<ul>
<li><ol>
<li>$X_i$ is not used in the optimal solution -&gt; L(i,j)=L(i-1,j)</li>
</ol>
</li>
<li><ol>
<li>$Y_j$ is not used in the optimal solution -&gt; L(i,j)=L(i,j-1)</li>
</ol>
</li>
<li><ol>
<li>Neither is used in the optimal solution </li>
</ol>
</li>
<li>In either case we want the max </li>
<li>so L(i,j)=max{L(i-1,j), L(i,j-1), 1+L(i-1,j-1)}</li>
<li>which we can simplify to L(i,j)=1+L(i-1,j-1)</li>
</ul>
</li>
</ul>
<p>We can summarize as<br/>
$ L(i,j) 
\begin{cases} 
max\{L(i-1,j), L(i,j-1)\} &amp; X_i \ne Y_j \\ 
1+ L(i-1,j) &amp; X_i = Y_j \\ 
\end{cases}$</p>
<p>Finally we can implement our algorithm</p>
<p><strong>$\color{red}{\text{Algorithm}}$</strong></p>
<pre><code>LCS(X,Y):
    for i = 0 -&gt; n : L(i,0)=0
    for j = 0 -&gt; n : L(0,j)=0
    for i = 1 -&gt; n:
        for j = 1 -&gt; n:
            if X[i]=Y[j]
                L(i,j)=1+L(i-1,j-1)
            else
                L(i,j)=max{ L(i,j-1), L(i-1,j) }
return L(n,n)</code></pre>
<p>Be sure to observe that L is a table ( not a vector like previous problems )
<img src="CS6515_images/DP1-002.png" width="400;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ex-4:-Contiguous-Subsequence">Ex 4: Contiguous Subsequence<a class="anchor-link" href="#Ex-4:-Contiguous-Subsequence">¶</a></h3><p>This comes from the text book: DPV-Algorithms</p>
<pre><code>6.1 A contiguous subsequence of a list S is a subsequence made up of consecutive
    elements of S. For instance, if S is
                                5, 15, −30, 10, −5, 40, 10,
    then 15, −30, 10 is a contiguous subsequence but 5, 15, 40 is not. Give a
    linear-time algorithm for the following task:

        Input: A list of numbers, a1,a2, . . . ,an.
        Output: The contiguous subsequence of maximum sum (a subsequence
                of length zero has sum zero).

    For the preceding example, the answer would be 10,−5, 40, 10, with a sum of 55.
    (Hint: For each j ∈ {1, 2, . . . , n}, consider contiguous subsequences ending
    exactly at position j.)</code></pre>
<p>We begin by defining our subproblem. We're given a hint so let's try using it</p>
<ul>
<li>Let T(i) = max sum of contiguous subsequences ending at i, where i goes from 1 to n</li>
</ul>
<p>Now we define our recurrance relationship</p>
<ul>
<li>Base Case: <ul>
<li>T(0) = 0 </li>
</ul>
</li>
<li>Recurrance<ul>
<li>T(i) = ...? this is not so simple </li>
<li>so let's work it out by hand</li>
</ul>
</li>
</ul>
<p>case(1) : 5</p>
<ul>
<li>Ans: 5, with a max sum of 5</li>
</ul>
<p>Case(2) : 5,15</p>
<ul>
<li>Ans: 5,15 with a max sum of 20</li>
</ul>
<p>case(3) : 5,15,-30</p>
<ul>
<li>Ans: 5,15 with a max sum of 20</li>
<li>This is rather enlightening! because the last digit is neg it doesn't make sense to end on it. We just take the sum of the previous subproblem. This could be our recurrance</li>
</ul>
<p>we could try</p>
<ul>
<li>T(i) = a(i)+T(i-1)  when a(i) &gt; 0</li>
<li>= T(i-1)             </li>
</ul>
<p>but we could probably just put these together as</p>
<ul>
<li>T(i) = max{ a(i)+T(i-1), T(i-1) }</li>
<li>when a(i) &lt; 0 we will take the latter sum, when a(i) &gt; 0 we take the former</li>
</ul>
<p>but there's a hidden error here, consider the next case</p>
<pre><code> 5, 15, -30, 10
 5, 20,  20, 30  
 - this is not accurate, the final sum should still be 20
 - one last tweak is just to zero out the sum for indices ending with a neg value</code></pre>
<p>Finally, we can define our recurrance relationship</p>
<ul>
<li>Base Case: <ul>
<li>T(0) = 0 </li>
</ul>
</li>
<li>Recurrance<ul>
<li>T(i) = a(i) + max{ T(i-1), 0 }, for 1 &lt;= i &lt;= n</li>
</ul>
</li>
</ul>
<p>let's do one last trial using our formula</p>
<pre><code> 5, 15, −30, 10, −5, 40, 10
 5, 20, -10, 10,  5, 45, 55
 - notice what happened @ -5, interesting eh?</code></pre>
<p>Now we are ready to write our algo</p>
<pre><code>T(i) = 0 for i = 0 ... n           Initialization step + base case

for i: 1 -&gt; n:
    T(i) = a(i) + max( T(i-1), 0 )

return max(T(:))</code></pre>
<p>This should run in just O(n) for the one loop. Yay it's linear!!</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="1.x---Coins">1.x - Coins<a class="anchor-link" href="#1.x---Coins">¶</a></h3><p>This one comes from the MIT lectures<br/>
<a href="https://www.youtube.com/watch?v=KLBCUx1is2c&amp;list=PLUl4u3cNGP63EdVPNLG3ToM6LaEUuStEY&amp;index=26">https://www.youtube.com/watch?v=KLBCUx1is2c&amp;list=PLUl4u3cNGP63EdVPNLG3ToM6LaEUuStEY&amp;index=26</a></p>
<p>We now present the alternating coin game. (Two players, we need to find the optimal strategy)</p>
<p>Problem Description</p>
<ul>
<li>Given: a sequence of coins with values $v_0,v_1,\cdots,v_{n-1}$<ul>
<li>let's say {5,10,100,25}</li>
</ul>
</li>
<li>Rules<ul>
<li>players take turns choosing either left-most or right-most coins</li>
<li>players with the most points at the end wins!</li>
</ul>
</li>
</ul>
<p>What's the subproblem here?</p>
<ul>
<li>we cannot use either suffixes nor prefixes, because players can choose from either side</li>
<li>we cannot use both because that is unheard of (ie a DP problem that uses both suffixes and prefixes)</li>
</ul>
<p>The next best option is a substring approach.</p>
<ul>
<li>Let X(i,j)=max total value that I (player 1) can get from coins with value $v_i,\cdots,v_j$</li>
</ul>
<p>We also need to implement some sort of index or categorical variable to represent the player p</p>
<ul>
<li>new</li>
<li>Let X(i,j,p)=max total value player, p, can get from coins with value $v_i,\cdots,v_j$</li>
<li>p={me,you} you opposing</li>
</ul>
<p>Now we move onto the relation</p>
<ul>
<li>X(i,j,me) = max{X(i+1,j,you)+v_i, X(i,j-1,you)+v_j} = max{left-coin,right-coin}</li>
<li>Similarly </li>
<li>X(i,j,you)= min{X(i+1,j,me), X(i,j-1,me)} = min{ left-coin, right-coin}<ul>
<li>player 2 is working to minimize your score, thereby maximizing theirs</li>
<li>also player 1 doesn't get the points if player 2 is moving hence the absence of $v_i$ and $v_j$</li>
</ul>
</li>
</ul>
<p>Topological order: increasing (j-i)</p>
<p>Base cases</p>
<ul>
<li>X(i,i,me)=v_i</li>
<li>X(i,i,you)=0</li>
</ul>
<p>Original Problem</p>
<ul>
<li>X(0,n,me)</li>
</ul>
<p>Time Complexity</p>
<ul>
<li>T: $O(n^2) \dot O(1) = O(n^2)$</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Exercises">Exercises<a class="anchor-link" href="#Exercises">¶</a></h3><ul>
<li>[DPV] 6.1 (Contiguous subseq)</li>
<li>[DPV] 6.2 (Hotel stops)</li>
<li>[DPV] 6.3 (Yuckdonalds)</li>
<li>[DPV] 6.4 (String of words)</li>
<li>[DPV] 6.11 (longest common subtring)</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DP2:-Knapsack-&amp;-Chain-Multiply">DP2: Knapsack &amp; Chain Multiply<a class="anchor-link" href="#DP2:-Knapsack-&amp;-Chain-Multiply">¶</a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Edit-Distance-(-Bonus-)">Edit Distance ( Bonus )<a class="anchor-link" href="#Edit-Distance-(-Bonus-)">¶</a></h3><p><a href="https://www.youtube.com/watch?v=ocZMDMZwhCY">MIT Youtube</a></p>
<p><strong>Problem</strong><br/>
Given two strings x &amp; y, what the cheapest possible sequence of character edits to turn x -&gt; y given 3 possible edit functions: Insert, Delete, and Replace</p>
<p><strong>Example</strong><br/>
Let S = 〈A,B,C,A,D,A〉<br/>
and T = 〈A,B,A,D,C〉</p>
<p>We could convert S to T using 3 edits: Delete S3(C); Delete S6(A); Insert S5(C).</p>
<p><strong>Assumptions</strong>
What does cheapest mean? For our purposes we will interpret this as the min cost. 
What is the cost? We're not given this so let's assume a use a cost of 1 for each func call.</p>
<p>Suppose we try suffixes? ie we consider turning x[i:] to y[i:]. Then we will be facing a problem with a potential size of O(|x|*|y|).</p>
<p>Option 1<br/>
we try a brute force approach comparing each character at the same index.<br/>
Then the cost is</p>
<ul>
<li>$\large C(i,j) = min\{ replace(x[i],y[j])+D(i+1,j+1),insert(y[j])+D(i,j+1),delete(x[i])+D(i+1,j) \} $</li>
<li>note that func:replace advances both indices but insert &amp; delete advance only the affected string<ul>
<li>this is necassary so that we are unnecassarily replacing a character that is next in the unaffected string</li>
</ul>
</li>
</ul>
<p><strong>$\color{red}{\text{Algorithm}}$</strong></p>
<pre><code>for i=0-&gt;m: E(i,0)=i
for j=1-&gt;n: E(0,j)=j

for i=1-&gt;m:
    for j=1-&gt;n:
        E(i,j)= min{ replace(x[i],y[j])+D(i+1,j+1)
                    ,insert(y[j])+D(i,j+1)
                    ,delete(x[i])+D(i+1,j) 
                    }

return E(n,m)</code></pre>
<p>Time Complexity:  O(nm)</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="2.0:-Knapsack">2.0: Knapsack<a class="anchor-link" href="#2.0:-Knapsack">¶</a></h3><p>The next problem we're going to discuss is the knapsack problem. You can imagine some applications of this are, where we're scheduling jobs and we have limited resources or limited computation time and we want to choose the jobs with most value for us.</p>
<p>In this problem, the input is n objects. For each object were given its weight and its value. And we'll assume that the weights and the values are all integers.</p>
<ul>
<li>weights $w_1,...,w_n$</li>
<li>values $v_1,...,v_n$</li>
</ul>
<p>Now we're given one additional input parameter, B, which is the total capacity available.</p>
<p>Our goal is to find a subset of objects that</p>
<ul>
<li>fit in the backpack; meaning that their total weight is in most capital B (Total weight &lt;= B). </li>
<li>And we're trying to find the subset with maximum total value. </li>
</ul>
<p>So let's try to restate this in more precise mathematical terms.</p>
<p>What do we mean by the total weight is in most capital B. We want to look at those objects which are in our subset or chosen subset. and whose total weight is at most B</p>
<ul>
<li>$\large \sum_{i \in S} w_i \le B$</li>
</ul>
<p>The total value for a subset of objects is the sum over the objects and the subset of their individual values. And we're trying to maximize that sum. We're trying to find the subset of objects with maximum value,</p>
<ul>
<li>$\large max\left\{\sum_{i \in S} v_i \right\}$</li>
</ul>
<p>Let's summarize the problem one more time just to make sure everybody understands.</p>
<p>Find subset S of objects with weights $w_i$ and values $v_i$ such that</p>
<ul>
<li>$\large \sum_{i \in S} w_i \le B$ AND $\large max\left\{\sum_{i \in S} v_i \right\}$</li>
</ul>
<p>Our goal is to find the subset of objects, a subset of 1 through n, where that subset fits in the backpack. So the chosen subset has total weight at most capital B and the subset we chose has maximum total value.</p>
<p>So we're trying to find the subset with maximum value, total value, and fits in the backpack.</p>
<p>There are two natural variants of this problem, and both have different dynamic programming solutions.</p>
<ul>
<li>Version 1: There's one copy of each object. So we're trying to find a subset without repetition.</li>
<li>Version 2: There's unlimited supply of each object.</li>
</ul>
<p>We're going to start up by looking at version 1. So we have at most one copy of each object that we can use. and then we'll go back, and we'll look at the second version of the problem where we have unlimited supply of each object.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="V1---no-repetition">V1 - no repetition<a class="anchor-link" href="#V1---no-repetition">¶</a></h4><p>Now if you are presented with this problem in real life, the first approach you might try is a Greedy approach.</p>
<p>Let's look at an example</p>
<pre><code>objects  1   2   3   4
values  15  10   8   1
weights 15  12  10   5

Total Capacity: 22</code></pre>
<p>Questions</p>
<ol>
<li>What is the optimal solution for this problem?</li>
<li>What does the subset of objects which attain the maximum value while fitting in the backpack?</li>
</ol>
<p>Solutions:</p>
<ul>
<li>the maximum value that we can obtain is 18, that is obtained by using objects two and three.</li>
<li>The total weight of these objects is 22,12 + 10 and the the total value is 10 + 8 is 18.</li>
</ul>
<p>Let's compare this to the greedy algorithm.</p>
<p><strong>Greedy Approach - BAD</strong></p>
<p>A greedy approach would take the most valuable object and try to fill up the backpack as much as possible with that most valuable object. What is the most valuable object? That's not the one with the total maximum total value. It's instead the one with the maximum value per unit of weight.</p>
<p>The greedy approach would sort the objects by their value per unit of weight $r_i=\frac{v_i}{w_i}$, which is this, quantity $r_i$, which is states value divided by its weight.</p>
<p>In this example the objects are already sorted by that ratio. We have that $r_1 \gt r_2 \gt r_3 \gt r_4$. So now what would a greedy approach do? The greedy approach would try to add object one, if it can, in this case it can, then we go to object two, and it would try to add object two if it can put.</p>
<p>In this example, once you add in object one, you have 15 units of weight. You only have seven units of weight remaining, so you can no longer add in object two. Then we go to object three. The next most valuable object. We would try to add it in, does it fit? No it doesn't fit. Then we try to add object four, if it can. In this example it can because 15 + 5 is 20. It fits in the backpack so the greedy approach could obtain the solution using objects one and object four.  Notice that the total value of this solution, object one and object four is 15 + 1, so it has total value 16, whereas our optimal solution has total value 18.</p>
<p><strong>Dynamic Programming Approach</strong></p>
<p>Let's now go back and try the DP approach.</p>
<p>Recall our basic recipe for designing a dynamic programming algorithm.</p>
<ol>
<li>The first step is always to define the sub-problem in words. </li>
</ol>
<ul>
<li>first attempt is always to try the same problem on a prefix of the input.</li>
<li>Therefore, we let K(i) be the max value achievable using a subset of the first i objects.<ul>
<li>$\large K(i) = max\left\{\sum_{i\in S} v_i \right\}$</li>
<li>All we've changed is we've changed the set of objects available to us from the first N objects 1 through N to a subset of objects 1 through I.</li>
</ul>
</li>
</ul>
<ol>
<li>Our second step in our recipe for designing a dynamic programming algorithm is to find a recursive relationship </li>
</ol>
<ul>
<li>K(i) = some_function(K(1),...,K(i-1))</li>
</ul>
<p>Let's give this another try</p>
<pre><code>objects  1   2   3   4
values  15  10   8   1
weights 15  12  10   5

we compute by hand 
i        1   2   3   4
K(i)    15  15  18  18     
w       15  15  22  22 

for i=1,2 we took object 1,
for i=3 the optimal solution requires taking a suboptimal solution at first, 
for i=2, to arrive at objects {2,3}. 
By taking object 2 first we have enough spare capacity in our back pack to take 
object 3, netting a higher total value.

In effect: Take suboptimal to i=2 when capacity &lt;= B - w3
for i=2, B-w3 = 12. Capacity of Object 2 = 12 so we take it. Object 1 capacity is 15 &gt; B-w3.</code></pre>
<p>What may not be so obvious yet though is that taking the suboptimal approach actually violates the subproblem definition stated above. We will need to redifine the subproblem in order to align to the event that the recurrance is not always the previous subproblem solution. This points us in the right direction because what we need to do is limit the capacity available for these subproblems. So in some sense we want to take a prefix of the objects, 1 through i, and we want to take a prefix of the capacity available. This is going to lead us to our second attempt for the design of a dynamic programming algorithm for this problem. We're going to define the subproblems so that it considers a prefix of the objects and it varies the capacity available.</p>
<ol>
<li><p>Subproblem redefined</p>
<ul>
<li>for i and b where $0\le i\le n$ and $0\le b\le B$</li>
<li>let K(i,b)=max value achievable using a subset of objects 1,...,i and with total weight &lt;= b</li>
</ul>
</li>
<li><p>Recurrence</p>
</li>
</ol>
<p>The recurrence is going to have two scenarios.</p>
<ul>
<li>Either we include object i,</li>
<li>or we don't include object i.</li>
</ul>
<pre><code># we have to know whether object i even fits in the backpack or not 
# If it doesn't then we know we cannot exclude it

if w_i &lt;= b:  # then it can fit
    # If we can include object i, then gain v_i so we have the option v_i + K(i-1,b-w_i)
    # If we don't include object i, then we have K(i-1,b)
    then K(i,b)=max{v_i + K(i-1,b-w_i) , K(i-1,b)  }
else          # it cannot fit, the weight of object i is strictly larger than b 
    K(i,b)=K(i-1,b) 

This defines a recurrence, it's still missing the base cases
So let's define them so we may move forward to writing our algorithm 

For the first row of our table, i=0
thus        K(0,b) = 0 is our max value

For the first col of our table, b=0
this        K(i,0) = 0 as we have no space in our backpack to store objects, kinda sucks eh</code></pre>
<p><strong>$\color{red}{\text{Algorithm}}$</strong></p>
<pre><code>for b=0-&gt;B: K(0,b)=0
for i=0-&gt;n: K(i,0)=0
for i=1-&gt;n:
    for b=1-&gt;B:
    if w[i] &lt;= b
        then K(i,b)=max{v_i + K(i-1,b-w_i) , K(i-1,b)  }
    else
        K(i,b)=K(i-1,b)    

return K(n,B)</code></pre>
<p>Our total time complexity is O(nB). O(n) for the outer loop and O(B) for the inner loop</p>
<p>Is this efficient? Where we define efficient as polynomial in the input size.</p>
<p>No! It is not polynomial in the input due to the B term. B is just a number and as such it takes (log B) space in storage. while n, as an array, will change due to the size of the problem, the space needed to store B will not.</p>
<p><strong>Polynomial Runtime</strong></p>
<p>Recall that the run time of Knapsack is O(nB) where n=num of items, and B=max possible weight.<br/>
<strong>Q:</strong> Explain why this is not polynomial in the input size<br/>
<strong>A:</strong> Understanding this requires a careful reading. If n doubles from say 10 to 20 then the size of the input to the algorithm changes as well. But consider what happens if B is changed from 16 to 32. The space required for 32 is one bit greater than that required for 16.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="V2---with-repetition">V2 - with repetition<a class="anchor-link" href="#V2---with-repetition">¶</a></h4><p>Now, we'll look at the version of the problem where we have unlimited supply of every object. Here, we can use an object as many times as we'd like as opposed to the other version of the problem where we can use an object at most once. Now, let's go ahead with our recipe for designing a dynamic programming algorithm.</p>
<ol>
<li>Define the subproblem.</li>
</ol>
<ul>
<li>Our subproblem for the previous version of knapsack was </li>
<li>K(i,b) = max value we can obtain from a multiset of objects {1,...,i} </li>
<li>with total weight $\le$ b.</li>
</ul>
<p>Now in this version, we're allowed to use objects multiple times. So instead of a subset where an object appears at most once,  we're going to consider a <strong>multiset</strong> where an object can appear multiple times in the set. That's the only difference from the previous definition of the subproblem.</p>
<p>Now, let's go ahead and see if we can write a recurrence for this subproblem definition. let's try to express K(i,b) in terms of smaller subproblems.</p>
<p>Similar to before we're going to have two scenarios. Either we include object i or we don't include object i and we're going to take the best of those two so we're going to take the max.</p>
<p>In this version of the problem, we will also have two scenarios.</p>
<ol>
<li>Either we include no more copies of object i</li>
</ol>
<ul>
<li>the solution is k is k(i-1,b)</li>
</ul>
<ol>
<li>or we're going to add in another copy of object i.</li>
</ol>
<ul>
<li>And for that copy of object we get value $v_i$ and we get the optimal solution to the subproblem where the capacity went down by $w_i$.</li>
<li>$v_i + k(i,b-w_i)$ </li>
<li>Notice here the first index is i, whereas in the other version of knapsack it was i-1 because in this version, we're allowed to use object i again even another copy, additional copies.</li>
</ul>
<p>$k(i,b)=max\left\{ k(i-1,b) , v_i + k(i,b-w_i) \right\}$</p>
<p>Is this recurrence in fact a valid recurrence? Are we expressing this current subproblem in terms of smaller subproblems? Previously, when we wrote recurrence for the current entry we always expressed it in terms of entries in previous rows.</p>
<ul>
<li>So k(i,b) would be in row i </li>
<li>and $k(i-1,b)$ would be in a row i-1.</li>
<li>but $k(i,b-w_i)$ is actually using the same row, but since it refers to a previous column. So all is good.</li>
</ul>
<p>So we can use the same recurrance as in the previous situation, with the same caveat (ie: $v_i + k(i,b-w_i)$ appplies if $w_i \le b$).</p>
<p>Our running time is again O(n*B).</p>
$$\large k(i,b)=max\left\{ k(i-1,b) , v_i + k(i,b-w_i) \right\}$$<p>Let's take a look at this algorithm for a moment. Often, when we get a solution which uses a two or three dimensional table,  it's useful to look at it and see if we can simplify it to get a smaller table. And we might get a faster or less space or just a simpler solution.</p>
<p>Now, why do we have this parameter i?</p>
<ul>
<li>The point of the parameter i in the original version of the knapsack problem, was to keep track of which objects we've considered or not. So, after we consider object i, then we can look at the first i minus 1 objects and look at a subset of those. But in this version of knapsack, we're allowed to use the object multiple times.So actually, it's not at all clear that we need to consider this parameter i. And in fact, we can get rid of it.</li>
</ul>
<p>So, let's try to do our dynamic programming solution to this version of knapsack where we have a single parameter. The single parameter is going to be, little b, corresponding to the total weight available. And this little b is going to vary between the maximum capacity available, capital B, and zero (ie $0 \le b \le B$)</p>
<p>Subproblem Definition:</p>
<p>for b where  $0 \le b \le B$:</p>
<ul>
<li>K(b)=max value attainable using weight $\le b$</li>
</ul>
<p>Recurrance: Try all possibilities for the last object to add</p>
<ul>
<li>$\large K(b)=\underset{i}max \left\{ v_i + K(b-w_i) : 1 \le i \le n, w_i \le b \right\}$</li>
</ul>
<p>So, the recurrence for k of b is going to be, we're going to try all possibilities for the last object to add and we're going to take the best of those. How do we get the best? We take the max, and we'll use i to denote the last object that we're trying to add. So, last object that we're going at is going to be object i, and we'll consider all i between one and n. If we add in object i, we gain value, Vi. And in addition, we gain the optimal solution to the subproblem where the total weight goes down by Wi. This is expressed in K(b-Wi). And we're trying all possibilities for i between one and n. But we need that the i'th object fits in the backpack.</p>
<p>Version 1: to solve the initial problem
<img src="CS6515_images/DP1-003.png" width="400;"/></p>
<p>Version 2: if the multiset solution is also required
<img src="CS6515_images/DP1-004.png" width="400;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ex---Electoral-College">Ex - Electoral College<a class="anchor-link" href="#Ex---Electoral-College">¶</a></h3><p>In this problem, we want to determine the set of states with the smallest total population that can
provide the votes to win the electoral college. Formally, the problem is the following: You are given
a list of n states along with their population $p_i$ , and the number of electoral votes $v_i$ , for $1 ≤ i ≤ n.$</p>
<p>Also, you are given Z, the number of electoral votes needed to win. All electoral votes of a state go
to a single candidate. Our goal is to find a set of states S with the smallest total population that
has at least Z electoral votes in total. You only have to output the total population of the set S,
you do not need to output the set itself.</p>
<p>Example: if n = 5, populations are P = [200, 100, 30, 700, 250], electoral votes are V = [5, 1, 2, 7, 6]
and Z = 12, then the solution is 480 since 480 = 200 + 30 + 250 and states 1, 3, 5 have 5 + 2 + 6 = 13
electoral votes. Note in this example: p2 &gt; p3 but v2 &lt; v3, this might occur, but shouldn’t affect
your algorithm. Design a dynamic programming algorithm to solve this problem. (Faster (and
correct) algorithm in big-O notation is worth more credit.)</p>
<p><strong>Intuition</strong><br/>
This should feel familiar as we recognize it as a knapsack type problem. so we will approach it similarly</p>
<pre><code>   0  1  2  3  4  5 ... Z        z-&gt;Z
0
1
2
3
...
n

i-&gt;n
States</code></pre>
<p><strong>1-Define the subproblem</strong><br/>
T(i,z) = min population needed to achieve at least z votes using states 1,...,i</p>
<ul>
<li>this doesn't tell you which states are needed, that's outside the scope of the ask</li>
</ul>
<p><strong>2-Define the Recurrance Relation</strong><br/>
Recall that in the knapsack version of this problem type we ended up with two cases. Case 1 occurred when we had room in our knapsack for an item and we had to choose whether or not to include that item. Case 2 was when we didn't have enough room in our knapsack for that item.</p>
<p><strong>Base Case</strong><br/>
We begin with a base case similar to before, <strong>Don't forget your base case!!</strong><br/>
T(0,z) = $\infty$ for $0 \le z \le Z$</p>
<ul>
<li>is our base case, why?</li>
<li>well because if we have no states than we can never get to Z votes to win</li>
</ul>
<p><strong>Recurrance</strong><br/>
Suppose v[i] &lt; z, ie the number of votes provided is less than that needed</p>
<ul>
<li>we may want to add this to our bag of states $P_i + T(i-1,z-v[i])$</li>
<li>we also may want to skip it and take $T(i-1,z)$</li>
<li>so we take the min of the two: $\min\{T(i-1,z) , P_i + T(i-1,z-v[i]) \}$</li>
</ul>
<p>Suppose v[i] &gt;= z, ie the number of votes provided is greater than that needed</p>
<ul>
<li>in this case we could just take $P_i$</li>
<li>or we could have taken previous states? well that is found in T(i-1,z)</li>
<li>so we take the min of the two min{$P_i, T(i-1,z)$}</li>
</ul>
<p>Finally we can summarize</p>
<ul>
<li>Base: T(0,z) = $\infty$ for $0 \le z \le Z$</li>
<li>if v[i] &lt; z then $\min\{T(i-1,z) , P_i + T(i-1,z-v[i]) \}$</li>
<li>if v[i] &gt;= z then $\min\{P_i, T(i-1,z) \}$</li>
<li>where $1 \le z \le Z$</li>
</ul>
<p><strong>3-Pseudo Code</strong></p>
<pre><code>for z: 0 ... Z : T(0,z) = infty

for z: 1 ... Z :
    for i: 1 ... n :
    if v[i] &lt;= z:
        T(i,z) = min{T(i-1,z) , P_i + T(i-1,z-v[i]) }
    if v[i] &gt;  z:
        T(i,z) = min{P_i, T(i-1,z) }

return T(n,Z)</code></pre>
<p><strong>4-Run Time</strong><br/>
T(n) + T(nZ) = O(nZ)</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="2.1:-Chain-Multiply">2.1: Chain Multiply<a class="anchor-link" href="#2.1:-Chain-Multiply">¶</a></h3><p>Our next dynamic programming problem is chain matrix multiply. This one will be a little different style from some of our early example. Actually, the solution will be a bit more complicated than the earlier examples that we looked at. So, let's look at a specific example so we can motivate this problem and then we'll go back and define the general problem. Our example will have four matrices A, B, C, D. Think of these matrices as having integer values for the entries.</p>
<p>I'm going to assume the reader understands matrix multiplication.</p>
<p>Our goal is to compute the product of these matrices: A<em>B</em>C*D. And we'd like to do this in the most efficient manner possible.</p>
<p>What exactly do we mean by most efficient?<br/>
Suppose</p>
<ul>
<li>A has of size 50 x 20</li>
<li>B is of size 20 x 1</li>
<li>C is of size 1 x 10</li>
<li>D is of size 10 x 100</li>
</ul>
<p>Recall that matrix multiplication is associative, thus there are many ways to compute them.</p>
<ul>
<li><p>$((AB)C)D = (A(BC))D = (AB)(CD) = A((BC)D) = A(B(CD))$.<br/>
Although the order does not affect the end value it does however affect the efficiency.</p>
</li>
<li><p>((AB)C) requires (50x20x1) = 1000 computations</p>
</li>
<li>(A(BC)) requires (20x1x10) = 200  computations</li>
</ul>
<p>In order to figure out which is the best or most efficient method for computing the product of these matrices, we need to assign a cost for each of these operations. So, let's take a look again at matrix multiplication and then we can figure out a reasonable notion of cost.</p>
<p>Suppose we have two matrices W(a x b) and Y(b x c)<br/>
Then Z = WxY is of size (a x c)<br/>
also we can compute $Z_{ij} = \sum_{k=1}^b W_{i,k} \cdot Y_{k,j}$</p>
<p>roughly speaking, for the full multiplications, there will be</p>
<ul>
<li>b multiplications</li>
<li>b-1 additions</li>
<li>acb multiplications</li>
<li>ac(b-1) additions</li>
</ul>
<p>So we will take acb as the cost of our matrix multiplication. (recall that most if not all DP problems are optimization problems. So you will almost always need number to minimize/maximize.)</p>
<p>Let's generalize our initial problem</p>
<ul>
<li>given n matrices $A_1,A_2,\cdots,A_n$</li>
<li>where $A_i \text{ is }m_{i-1} \times m_i$ <ul>
<li>This is our input - we don't care about actual values since we're not computing </li>
</ul>
</li>
<li>find the min cost of computing $A_1 \times A_2  \times \cdots  \times A_n$<ul>
<li>this is our goal </li>
</ul>
</li>
</ul>
<p>To get some intuition for this problem, Let's look at an alternative representation of the problem and instead of looking at it as parenthesization we're going to represent it as a binary tree.</p>
<p><img src="CS6515_images/DP1-005.png" width="500;"/></p>
<p>How these subtrees are structured tells us the parenthesization.</p>
<p>Here is a slightly modified version that articulates the same general problem in binary tree format
<img src="CS6515_images/DP1-006.png" width="500;"/></p>
<p>As you can see we will take the substring approach.</p>
<p>There's going to be two parameters, I and J.</p>
<ul>
<li>I is the start of the substring, J is the end of the substring. </li>
<li>with $1 \le i \le j \le n$</li>
</ul>
<p>And then we're going to define our subproblem as</p>
<ul>
<li>C(i,j) is the minimum cost for computing the product of the matrices Ai through Aj.</li>
</ul>
<p>Recurrance for C(i,j)</p>
<ul>
<li>the simplest case is when i = j<ul>
<li>then C(i,i)=0 , these are simply the diagonals of the final matrix</li>
</ul>
</li>
<li>the next case is when i $\lt$ j representing the substring $A_i,...,A_j$<ul>
<li>which we split at say l, to form two substrings, or branches                   <ul>
<li>Left node is $A_i,...,A_l$ = C(i,l) = cost is m_i-1 x m_l       </li>
<li>and Right node is $A_{l+1},...,A_j$ = C(l,j) = cost is m_l+1 x m_j </li>
<li>final cost of $A_i,...,A_j$ is just m_i-1 x m_l x m_j</li>
</ul>
</li>
<li>now we compute the min cost for each subtree<ul>
<li>you should be able to see where this is going</li>
<li>as we go down the tree we keep splitting </li>
<li>eventually we should get to substrings that are easy to solve locally</li>
</ul>
</li>
</ul>
</li>
<li>finally<ul>
<li>$\large C(i,j)=\underset{l}min\left\{ C(i,l)+C(l+1,j) +(m_{i-1} m_l m_j) \; i \le l \le j-1  \right\}$</li>
<li>minimum l over the sum of the left cost, the right cost, and the merging cost</li>
</ul>
</li>
</ul>
<p>We take the mean over the choices of L where l can vary from i to j-1.  And for that specific L, the cost is the cost for the left subtree C(i,l) plus the costs for the optimal right subtrees C(l+1,j) plus the cost of merging that left subtree with  that right subtree which is $(m_{i-1} m_l m_j)$.  We take the sum of those three terms and we take the L which minimizes that sum some That's our recurrence for C(i,j).</p>
<p>Before we detail the Pseudocode for this dynamic programming algorithm, let's go back and look at our recurrence a little more carefully, and see how we're going to fill the table up.</p>
<p>What we're looking at in this situation is a two-dimensional table, let's call it C. We're trying to compute the upper diagonal of this table. Where the entries where j is at least i. Recall that our base case was diagonal, these are the entries C(i, i). This is the first thing we're going to fill in.</p>
<p>What is the next thing that we're going to fill in? The next entries we're going to fill in are the entries C(i,i+1), which are just the off diagonals. After this we will compute the next diagonal C(i,i+2) and on we go until we get to C(1,n) which is the upper most corner.</p>
<p><img src="CS6515_images/DP1-007.png" width="500;"/></p>
<p>Now, we can go ahead and detail our Pseudocode for our dynamic programming algorithm.</p>
<p><strong>$\color{red}{\text{Algorithm}}$</strong></p>
<pre><code>ChainMultiply(m0,m1,...,mn):
for i=1 -&gt; n, C(i,i)=0                                         O(n)
for s=1 -&gt; n-1:                                                O(n)
    for i=1 -&gt; n-s:                                              O(n)
        Let j=i+s                                              
        C(i,j) = infinity
        for l=i -&gt; j-1:                                            O(n) 
            curr = (m[i-1] * m[l] * m[j]) + C(i,l) + C(l+1,j)
            if C(i,j) &gt; curr 
                C(i,j) = curr
return C(1,n)</code></pre>
<p>Time Complexity</p>
<ul>
<li>O(n)+O(n)O(n)O(n) = $O(n^3)$</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Exercises">Exercises<a class="anchor-link" href="#Exercises">¶</a></h3><ul>
<li>[DPV] - 6.17 (change making)</li>
<li>[DPV] - 6.18 (change making)</li>
<li>[DPV] - 6.19 (change making)</li>
<li>[DPV] - 6.20 (optimal BST)</li>
<li>[DPV] - 6.7 (palindrome subsequence) or try looking for substrings</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DP3:-Shortest-Path-Problem">DP3: Shortest Path Problem<a class="anchor-link" href="#DP3:-Shortest-Path-Problem">¶</a></h2><h3 id="Intro---DP3">Intro - DP3<a class="anchor-link" href="#Intro---DP3">¶</a></h3><p>In this lecture, we'll look at several versions of shortest path problems and we'll use dynamic programming to design fast algorithms for these problems.</p>
<p>The setting is that we have a directed graph $\overset{\rightarrow}G = (V,E)$ And in addition, we have weights on the edges which is denoted by W(e). Some of the edges will have negative weights. We may also have these anti-parallel edges, like (a to d) and (d to a)  So we have an edge from A to D and from D to A and they might have the same or different weights.</p>
<p>Here's a small illustration:
<img src="CS6515_images/DP1-008.png" width="400;"/></p>
<p>In our first problem, we have a designated start for text which will denote as S. We're going to look at the length of the shortest path from S to every other vertex in this graph. To do this let's define the following function: for $z \in V$ let dist(z) = length of the shortest path from s to z. Since z is a multiset we have that dist(z) is defined for every vertex in the graph. So it's an array of length n; our goal is to compute this array.</p>
<p>Let's take a closer look at dist(z):</p>
<ul>
<li>for starters we have the simplest case dist(s) = 0 since the shortest path length from s to s is 0</li>
<li>dist(b) = 5 - since there is only one path</li>
<li>dist(a) = 5+3=8</li>
<li>dist(e) = 5+3-2=6; the alternative path s&gt;b&gt;e yields 5+8=13</li>
</ul>
<p>You may recall that there is a famous solution to similar problems called Dijkstra's algorithm. Which takes a directed graph $\overset{\rightarrow}G=(V,E)$ with edge weights $w(e)$ and $s \in V$ and finds a distance array dist(z) for all $z \in V$. It does so using a Breadth first search approach, which explored the graph in a layered approach. It has a total run time of O((n+m) log n). Recall also that Dijkstra's algo generally uses a min-heap or priority queue data structure, both of whose operations require (log n) time. One downfall or constraint is that it can't handle negative weights. They must be positive and greater than 0. Reason behind this is that it won't explore a vertex twice, consequently if it encounters a negative weight that results in a shorter path it cannot update the previous path weight. We will be working towards allowing negative weights.</p>
<p>Our first question is whether or not the problem is well defined? Well in our example it is but would it be if the negative weight were a bit more extreme? Suppose for example that instead of -2, the edge a -&gt; e was -6.
<img src="CS6515_images/DP1-009.png" width="400;"/></p>
<p>Previously dist(d) = 5+3+3=11. This time around though it can be less, due to the negative cycle that has appeared. Look at the cycle b -&gt; a -&gt; d, it's -1, meaning we can reduce our path length by taking the cycle. Now if we take this path then dist(d) is 10. In fact we could repeatedly walk around this cycle to get a lower path length.</p>
<blockquote><p>Prev Path: s -&gt; b -&gt; a -&gt; d<br/>
New  Path: s -&gt; b -&gt; a -&gt; e -&gt; b -&gt; a -&gt; d</p>
</blockquote>
<p>Note this is technical a walk, paths don't allow for repetition. This situation is called a negative weight cycle.</p>
<p>Now let's redefine our problem, and goal, in more general terms to allow for these negative weights.</p>
<ul>
<li>Given a directed graph  $\overset{\rightarrow}G = (V,E)$ </li>
<li>and edge weights w(e) for all $s \in V$</li>
<li>and a designated start point s</li>
<li><ol>
<li>find the negative weight cycle reachable from S, if it exists</li>
</ol>
</li>
<li><ol>
<li>else find dist(z) for $z \in V$<ul>
<li>This part is known as the single-source shortest path </li>
</ul>
</li>
</ol>
</li>
</ul>
<p>So let's design a dynamic programming algorithm for the single source shortest path problem.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Single-Source---Bellman">Single Source - Bellman<a class="anchor-link" href="#Single-Source---Bellman">¶</a></h3><p>For our first demo let's assume there are no negative weight cycles, making the shortest path length from S to every other vertex a well-defined problem. Since there are no negative weight cycles in the graph, the shortest path from the start vertex S to any other particular vertex z visits every vertex at most once. (NB This does not mean it must visit every vertex, only that if it does visit then the number of visits is at most one)</p>
<p>Let := P denote the shortest path from s to z. (recall that a path is defined by a series of edges). Because every vertex can be visited at most once we know that $|P| \le n-1$ edges.</p>
<p>Usually, we try to use a prefix of the input in our dynamic programming algorithm. Here it's going to be a little different type of solution. Notice that the path length is at most n-1 edges. Let's try to use a prefix of the path. This means that we will try to condition on the number of edges in the path.</p>
<p>Let's introduce a variable $i=0 \to n-1$, representing the number of edges allowed on the paths that we consider.</p>
<ul>
<li>When i=n-1 then we allow the path to be at most length n-1, and that's solves the shortest path problem.    </li>
<li>when i=0 then we have the base case that doesn't allow for any edges</li>
</ul>
<p>More formally</p>
<ul>
<li>Define two parameters i &amp; z such that <ul>
<li>$0 \le i \le n-1$ </li>
<li>and $z \in V$</li>
</ul>
</li>
<li>Let D(i,z) = length of shortest path from s -&gt; z using $\le$ i edges (ie at most i edges)</li>
</ul>
<p>Now let's try to write a recurrence for D(i,z). ie express D(i,z) in terms of D(i-1,z),</p>
<p>Base Case: D(0,s) = 0 and $\forall z \ne s$ D(0,z)=$\infty$</p>
<p>For $i \ge 1$:</p>
<ul>
<li>Consider the shortest path from s -&gt; z using exactly i edges</li>
<li>further suppose that there is a prefix path to connected edge y; <ul>
<li>(ie s -&gt; y -&gt; z) with z as the last edge</li>
<li>s.t. path from s to y is i-1 edges</li>
</ul>
</li>
<li>In order to find D(i,z) we must find the best y, minimum w(y,z), as there may be many with different weights<ul>
<li>ie $\large D(i,z) = \underset{y:\overrightarrow{yz} \in E}{min}\{ D(i-1,y) + w(y,z) \}$<ul>
<li>where D(i-1,y) is the shortest path from s -&gt; y using at most i-1 edges</li>
<li>and w(y,z) is the weight for the path from y -&gt; z</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="CS6515_images/DP1-010.png" width="400;"/></p>
<p><strong>Note:</strong></p>
<ul>
<li><p>$\large D(i,z) = \underset{y:\overrightarrow{yz} \in E}{min}\{ D(i-1,y) + w(y,z) \}$</p>
<ul>
<li>is the shortest path from s to z through y using exactly i edges</li>
</ul>
</li>
<li><p>Now to find dist(z) we must look over all choices of i, $dist(z) = \underset{i} min D(i,z)$</p>
</li>
<li>The algo above looks for exactly i edges but we want at most i edges, we want to allow for less!<ul>
<li>so we will tweak the above to take the min over y or to take the previous solution D(i-1,z)</li>
</ul>
</li>
<li>Our final recurrance relationship is defined as<ul>
<li>$\large D(i,z) = min\{ D(i-1,z) , \underset{y}{min}\{ D(i-1,y) + w(y,z) \} \}$</li>
</ul>
</li>
</ul>
<p>Here's a nice animated gif to help visualize:</p>
<p><img src="CS6515_images/Bellman-Ford_algorithm_example.gif" width="350;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Algo-Bellman-Ford">Algo Bellman Ford<a class="anchor-link" href="#Algo-Bellman-Ford">¶</a></h3><p><strong>$\color{red}{\text{Algorithm - aka: Bellman-Ford}}$</strong></p>
<pre><code>Input: G=(V,E),S,w

for all z in V
    D(0,z) = inf

D(0,s) = 0                                   # dist from s to s is 0

for i = 1 -&gt; n-1:
    for all z in V:
        D(i,z) = D(i-1,z)                    # take prev row as your start values
        for all yz in E:                     # edges into z ( See Note )
            if D(i,z) &gt; D(i-1,y) + w(y,z)    # test for min value
               D(i,z) = D(i-1,y) + w(y,z)    # then we update

Return D(n-1,:)                              # we return the array at the last row

NOTE: 
    "for all yz in E" iterates over all edges into z, 
     but because we generally use an adjacency list which gives edges out from a node,
     we will need to inverse this at a cost of O(n+m)

After reversing we can get the edges into z</code></pre>
<p>Time</p>
<ul>
<li>O(n) for the outer loop : i = 0 -&gt; n-1</li>
<li>O(m) for the inner loop : for all z in V</li>
<li>Total O(nm)</li>
<li>Not as good as dijkstra's version but we will be able to handle neg weights</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Negative-Weights&amp;Cycles">Negative Weights&amp;Cycles<a class="anchor-link" href="#Negative-Weights&amp;Cycles">¶</a></h3><p>How can we find whether a graph has a negative way cycle or not?</p>
<p><img src="CS6515_images/DP1-009.png" width="400;"/></p>
<p>Here's our earlier example on six vertices and it has a negative weight Cycle (A-&gt;B-&gt;C-&gt;A) which is of length -1.</p>
<p>So, what's going to happen for our Bellman Ford algorithm that we just defined on this example?</p>
<p>We're going to have a two dimensional table. The columns of the tables are going to be the vertices of the graph: S, A, B, C, D, E. The rows of the table are going to correspond to the path lengths we consider. We start with the base case, i=0. In the base case we have D(0,s) = 0, and the other entries are infinite.</p>
<p><img src="CS6515_images/DP1-012.png" width="400;"/></p>
<p>Our algorithm will fill up the table from I=1,2,3,4,5, let's work through this</p>
<pre><code>i    s    a    b    c    d    e
0    0  inf  inf  inf  inf  inf &lt;- this is just a result of our initialization and base case
1    0    5  inf  inf  inf  inf &lt;- only a is reachable in i&lt;=1 edges, 
2    0    5    8  inf  inf  inf &lt;- a is the same, b is now reachable w length 8
3    0    5    8    2   12  inf &lt;- c&amp;d now reachable, a&amp;b remain the same
4    0    4    8    2   12    7 &lt;- e reachable, a gets updated, b&amp;c remain the same
5    0    4    7    2   12    7 &lt;- b updated, rest remain the same
6    0    4    7    1   11    7 &lt;- For illustration purposes 
    The current algo stops at n-1 and won't compute this last row,
    But we will need to add it in order to complete the solution</code></pre>
<p>So what have we shown? Well when we do this by hand we notice that the negative weight cycle causes us to have to update former path lengths. So how do we detect a negative weight cycle? Well as soon as the rows values change then we know we've encountered a negative weight cycle. If i=n is different from i=n-1 then that shows us that there is a negative weight cycle. So how do we check that there's a negative weight cycle?  We check if D(n,z) is strictly smaller than D(n-1,z) for some vertex z.</p>
<p>So we backtrack, in doing so can see that that cycle involved is vertex C and then we can see it involves B and A.  So we can detect that cycle: C, B, A.</p>
<p>But our check is just we take our algorithm from before, Bellman Ford algorithm, which ran from i=1 to n-1, and instead we run it from i=1 to n. Then we check if the row i=n is different from i=n-1. If it is different, then we found a negative weight cycle. If it's not different, then we output the row I=n-1 or  row i=n, because they're both the same. That gives us the shortest path length from s to every other vertex. So that completes the dynamic programming algorithm known as Bellman Ford for finding the shortest path from a single source vertex and it allows positive and negative weight edges. And if there is negative weight edges, it can detect whether or not there is a negative weight cycle.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="All-Pairs---Floyd">All Pairs - Floyd<a class="anchor-link" href="#All-Pairs---Floyd">¶</a></h3><p>Let's look at one more variant of shortest path problem and this will give us a chance to look at a slightly different style of dynamic programming solution.</p>
<p>What we did before with Bellman-Ford was we had  a single source and we looked at the shortest path from that single source to all other vertices. Now we're going to look at all pairs shortest path. Once again we're given a directed graph, G, along with edge weights. And these edge weights again can be positive or negative.</p>
<p>Similar to before we have</p>
<ul>
<li>A directed graph  $\overset{\rightarrow}G = (V,E)$ </li>
<li>and edge weights w(e) for all $e \in V$</li>
<li>and we will define a function dist(y,z) = length of the shortest path from y to z, for all $y,z \in V$</li>
</ul>
<p>Note that this time around we are not given a start point s.</p>
<ul>
<li>Goal is to find dist(y,z) for all $y,z \in V$</li>
</ul>
<p>Previously we did this using the bellman-ford algo. Could we do this again? Yes, in fact we could. Just a bit of tweaking needed to run Bellman on each possible vertex y in V. But recall that bellman-ford had a runtime of O(nm). So if we performed BF on each vertex then we would end up with a runtime of O(m*(n^2)). This is not very good and is a rather naive approach. In fact the m term can also be n^2 in some cases, making the worst-cast analysis asymptotic to O(n^4). We will look at a better algorithm called the floyd-warshall that runs in O(n^3)</p>
<p>Let's look at the basic idea behind our DP approach. Recall that in the previous case when we used bellman-ford we conditioned on the number of edges. Of course, we will want to do something different this time around, otherwise we would end up with the naive solution we've already shown. So let's try the vertices this time around.</p>
<p>New Idea: Let V={1,2,...,n} - don't get caught up in the details, this is little more than the indices of our vertices. What this gives us is a way to select a prefix of vertices. In particular we will condition on the intermediate vertices, ie a prefix of the Vertex set V.</p>
<p>More formally:</p>
<ul>
<li>Let's use i where $0 \le i \le n$, be the prefix of the vertex set<ul>
<li>we're going to consider the set of intermediate vertices 1 -&gt; i.</li>
<li>That's going to be the set of allowable vertices </li>
<li>to be used as intermediate vertices on the paths that we consider</li>
</ul>
</li>
<li>also let's use s and t to denote the start vertex and end vertex, respectively<ul>
<li>we want to try all possible start vertices and all possible end vertices (from 1 to n)</li>
<li>So s and t both vary between 1 to n (ie $1 \le s,t \le n$ ) </li>
<li>and we want to try all $n^2$ choices for s and t.</li>
</ul>
</li>
</ul>
<p>Subproblem Definition</p>
<pre><code>for i:0-&gt;n, s:1-&gt;n, t:1-&gt;n
Let D(i,s,t) = Length of shortest path s -&gt; t
               Using a subset of {1,...,i} as intermediate vertices</code></pre>
<p>Recursive Relationship</p>
<pre><code>Base case for i=0, D(0,s,t)
    there are no intermediate vertices, ie it's just an empty set
    1. if s-&gt;t are connected directly, no intermediate vertices are needed and we take w(s,t)
    2. if s &amp; t are not directly connected then we take infinity as it cannot be solved

For i &gt;= 0:
        /* Look for the shortest path P from s-&gt;t using vertices {1,...,i}                  */
        /* If vertex i is not needed/used, this arises when i is not on the path from s-&gt;t  */
    if i is not in path P
        then D(i,s,t) = D(i-1,s,t)

        /* The next situation presents multiple challenges.                 */
        /* The current situation/path looks like                            */
        /* s -&gt; subset{1,...,i-1} -&gt; i -&gt; subset{1,...,i-1} -&gt; t            */ 
    if i is on path P
         then D(i,s,t) = D(i-1,s,i) + D(i-1,i,t)
        /* This is just the sum of weights w(s,i) and w(i,t)                */</code></pre>
<p>Now we put it all together. Recall that we want the shortest path, t/f we will take the min of the two cases above</p>
<blockquote><p>$\large D(i,s,t) = min\{ D(i-1,s,t), D(i-1,s,i) + D(i-1,i,t)  \}$</p>
</blockquote>
<h3 id="Algo:-Floyd-Warshall">Algo: Floyd-Warshall<a class="anchor-link" href="#Algo:-Floyd-Warshall">¶</a></h3><p><strong>$\color{red}{\text{Floyd-Warshall}}$</strong><br/>
Note <strong>this algo allows neg weights - but assumes NO negative weight cycles</strong></p>
<pre><code>Inputs: G, w

for s=1-&gt;n:
    for t=1-&gt;n:
        if (s,t) in E 
            then D(0,s,t)=w(s,t)
        else D(0,s,t) = infty

for i=1-&gt;n:
    for s=1-&gt;n:
        for t=1-&gt;n:
            D(i,s,t)=min{ D(i-1,s,t), D(i-1,s,i) + D(i-1,i,t) }

Return D(n,:,:)</code></pre>
<p>Running Time</p>
<ul>
<li>T(n) = T(n^2)+T(n^3) = O(n^3)</li>
</ul>
<p>How might we detect a negative weight cycle? It deceptively simple. If there is a negative weight cycle in the graph G then there is a vertex a where the path from a to itself it negative.</p>
<blockquote><p>If there exists a vertex a s.t. D(n,a,a) &lt; 0 then there is a negative weight cycle</p>
</blockquote>
<p>In order to detect a neg cycle an extra step is needed</p>
<ul>
<li>Check the diagonal of D <ul>
<li>if there is a neg cycle then there should be a neg entry </li>
<li>then there is a neg length path from a vertex to itself</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Comparison-Ford-v-Floyd">Comparison Ford v Floyd<a class="anchor-link" href="#Comparison-Ford-v-Floyd">¶</a></h3><p>let's take a quick look at some of the differences b/w Bellman-ford and Floyd-Marshall</p>
<p>Consider the following graph<br/>
<img src="CS6515_images/DP1-011.png" width="400;"/></p>
<p>How would bellman-ford handle this for say d = start? Well there are no paths leading out from d and thus it would not find the negative cycle. Floyd-Marshall, however, would look at all vertices and would invariably find it.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="DP3-Exercises">DP3-Exercises<a class="anchor-link" href="#DP3-Exercises">¶</a></h3><ul>
<li>DPV - 4.21 ( Currency Exchange )</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="DP3-Summary">DP3-Summary<a class="anchor-link" href="#DP3-Summary">¶</a></h3><ol>
<li>Find &amp; Define the subproblem</li>
</ol>
<ul>
<li>Consider any constraints that may be necassary<ul>
<li>ie where $1 \le i \le n$</li>
<li>or $n \ge 0$</li>
</ul>
</li>
<li>This should take the form of an array / perhaps a even a matrix or table</li>
<li>ie T(i) = mimimum cost to get to arrive at point i, where $1 \le i \le n$</li>
</ul>
<ol>
<li>Find the recurrance relation ( these become progressively more difficult )</li>
</ol>
<ul>
<li>Always include your base case!</li>
<li>1st approach : try using prefix A[...:i] or suffix A[i:...]<ul>
<li>great for sequences and strings, where there is an ordering</li>
<li>if applicable the problem size will be in linear space  </li>
</ul>
</li>
<li>2nd approach : Try substring A[i:j]</li>
<li>3rd approach : Try windows</li>
<li>When writing the recurrance don't forget your boundaries          </li>
</ul>
<ol>
<li>Write the pseudocode</li>
</ol>
<ul>
<li>begin with the initialization (this should also define boundaries)</li>
<li>then your base case</li>
<li>then your looping using recurrance<ul>
<li>be careful not to reference undefined elements, before the exist</li>
</ul>
</li>
</ul>
<ol>
<li>Perform Time Analysis of the algorithm</li>
</ol>
<p>Step 1 Examples</p>
<ul>
<li>LIS : T(i) = max sum of contiguous subsequence ending at a[i]</li>
<li>LCS : T(i,j) = length(LCS) in X[1,i],Y[1,j]</li>
<li>Knapsack: T(i,j) = max value using a subset of objects [1,...,i] with total weight $\le B$</li>
<li>Path (single source) : T(i,z) = min path from s to z using $\le i$ edges </li>
<li>Path (all pairs) : T(i,s,t) = min path from s to t using edges in {1,2,,...,i}</li>
</ul>
<p><strong>Office Hours #2 DP-Dynamic Programming</strong></p>
<p>Look for patterns</p>
<ul>
<li>prefix/suffix <ul>
<li>prefix: [0,...2],[0,...3],[0,...4],etc, etc</li>
<li>suffix: [n-2...n],[n-3...n],[n-4...4],etc, etc   </li>
</ul>
</li>
<li>subset/substring<ul>
<li>this can take many forms </li>
<li>Ex 1 : i[1:] j[0:i], i[2:] j[0:i], i[3:] j[0:i], here inner loop is "for j:0 -&gt; i"</li>
<li>Ex 2 : </li>
</ul>
</li>
</ul>
<p>Build table by hand to validate the results ( avoid coding! )</p>
<p>Coding v Practicing</p>
<ul>
<li>This class is not about coding!</li>
<li>you will spend more time aliging your indices rather than understanding the algo</li>
</ul>
<p><strong>Office Hours #3 DP-Dynamic Programming</strong></p>
<p>Review of Jumping frog from Homework</p>
<p>Review of Electoral College (from Homework - See above)</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DC0:-Divide-&amp;-Conquer">DC0: Divide &amp; Conquer<a class="anchor-link" href="#DC0:-Divide-&amp;-Conquer">¶</a></h2><p><a href="https://people.eecs.berkeley.edu/~vazirani/algorithms/chap2.pdf">Berkeley</a></p>
<p><a href="https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm">Wikipedia</a></p>
<p>Divide and Conquer algorithms use recursion to break a problem down into two or more similar, yet simpler, subproblems, that can be solved more easily. It then uses recursion to recombine the simpler solutions into a solution for the initial problem.</p>
<p>Application of D&amp;C follows a simple strategy:</p>
<ul>
<li>Break the problem into subproblems that are similar instances, or of a similar type, to the original problem</li>
<li>Recursively solving these subproblems</li>
<li>Appropriately combine their solutions</li>
</ul>
<p>Of course while the strategy is simple, in practice the first step requires a good deal of intuition.</p>
<p>We will look at a fundamental problem multiplying two n bit numbers. Here, we'll assume the numbers are huge (thousands of bits long). This will be the case in an application such as RSA.</p>
<p>Another fundamental problem we'll look at is given n numbers, we'd like to find the median element. The numbers are unsorted, so they are in arbitrary order. Can we find the median without first sorting the list?</p>
<p>Finally, we'll dive into the beautiful FFT algorithm, Fast Fourier Transform. It's impossible to overstate the importance of this algorithm, it's used in many fields, such as signal processing. In fact, it was called the most important numerical algorithm of our lifetime.</p>
<h3 id="Review---Binary-Search">Review - Binary Search<a class="anchor-link" href="#Review---Binary-Search">¶</a></h3><p>Suppose you have a phone book and need to find someones number. You could technically start at the very beginning and work your way, line by line, page by page, until you found the person. Of course this would be very tedious and time consuming. In the best case you find them on the first page. In the worst case they appear on the very last page.</p>
<p>Of course, you're smarter than the average bear. So you decide to start at the middle of the phone book. Since you have this person's last name you can now determine if they are in the first half of the phone book or the second half. Which ever case it is you choose the appropriate half and divide again. What you are performing here is called a binary search. at first you n people to look through. after the first division you have n/2, after the second division you have n/4, keep going and eventually you'll have n/n left meaning you've found the person your looking for.</p>
<p>This is the simplest and easiest of all divide and conquer algorithms. 
<img src="CS6515_images/Binary-search-work.gif"/></p>
<pre><code>Input:  A is the array, 
        n is the number of elements in A or length(A)
        x the target element 

Binary_search(A,n,x)
lpt = 0             # Left point  A[lpt]
rpt = n             # Right point A[rpt]

while lpt &lt;= rpt
    m = floor( (lpt-rpt)/2 )
    if A[m] &lt; T
        L = m+1
    else if A[m] &gt; T
        R = m - 1
    else 
        return m

return -1              # if hit element not found</code></pre>
<p><strong>Running Time</strong><br/>
It turn out that binary search has a run time of $O(log_2 n)$. to understand why let's do a quick review of logarithms.</p>
<p>Recall that logarithms represent the inverse of exponentiation. A logarithm is the power to which a number must be raised in order to get some other number. For example, the base 2 logarithm of 16 is 4, because 2 raised to the power of 4 is 16.
$$ \log_{2} 16 = 4 \text{  since  } 2^4 = 16  $$</p>
<p>Of course we may be working in some other base like say 4 then 
$$ \log_{4} 16 = 2 \text{  since  } 4^2 = 16  $$</p>
<p>Now back to our algorithm. At each step we divide the search space by 1/2, and perform a check. Thus our initial assessment would be T(n/2)+O(1). We focus on T(n/2), as O(1) is insignificant. As we saw in the algorithm the search space decreases at each step by 2 and is of size $n/2^i$, where $0 \le i \le k$. We also know that $n/2^k = 1$, since that's the final element in a worst case scenario. So we can simplify by multiplying both sides to get $n=2^k$. Now we take the base 2 log of each side 
$$ \log_2 n = \log_2 (2^k) = k \log_2 (2) = k $$</p>
<p>Thus T(n/2)=T(k) is bounded by O($\log_2 n$) (note that the base is often ommitted as it is almost always base 2)</p>
<p>You can also use the master theorem, see Recurrance Generalization, to see that this is true.</p>
<h3 id="Review---Merge-Sort">Review - Merge Sort<a class="anchor-link" href="#Review---Merge-Sort">¶</a></h3><p>Another classic examples of DC is the merge sort algorithm for sorting an array of numbers.</p>
<pre><code>MERGESORT(A,p,r):
if p&gt;= r:
    return

q = floor{(p+r)/2}      # midpoint of A[p : r]
MERGESORT(A,p,q)        # recursively sort A[p:q]
MERGESORT(A,q+1,r)      # recursively sort A[q+1 : r]

MERGE(A,p,q,r)          # Merge A[p:q], A[q+1 : r] into A[p,r]</code></pre>
<p>Here's a nice little illustration
<img src="CS6515_images/DC1-001.png" width="400;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ex1---Max-Sub-Array-Sum">Ex1 - Max Sub-Array Sum<a class="anchor-link" href="#Ex1---Max-Sub-Array-Sum">¶</a></h3><p>This example comes from U Washington cse417<br/>
<a href="https://courses.cs.washington.edu/courses/cse417/18wi/lectures/lec08-divide-and-conquer-4.pdf">Link</a></p>
<p><strong>Problem</strong><br/>
Given an array of integers A, find the max possible summation over consecutive elements.<br/>
ie Find $\large max\{ \sum_i^{j-1} A_i \; ; \forall 0 \le i \le j \le n \}$</p>
<p>Example: A=[31, -41, 59, 26, -53, 58, 97, -93, -23, 84]<br/>
Then the max sum is 187 = sum A[2 : 6]</p>
<p><strong>V1 Brute Force:</strong> We could just perform a loop over the i(s) and then the j(s) in a nested manner. In doing so we would compute the sum for each possible start and end point i &amp; j. Then take the max over all elements. This of course would be time consuming, leading to an $O(n^3)$ run time</p>
<p><strong>V2 Divide and Conquer</strong></p>
<p>Sketch of the algorithm</p>
<ul>
<li>Divide A into two halves </li>
<li>Compute the max sum in each half (ie solve the subproblem)</li>
<li>take the max of the sums returned <ul>
<li>ie $max((A[i]+\cdots+A[n/2-1]),(A[n/2]+\cdots+A[j-1]))$</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DC1:-Fast-Integer-multiplication">DC1: Fast Integer multiplication<a class="anchor-link" href="#DC1:-Fast-Integer-multiplication">¶</a></h2><p>What we're going to look at now is multiplying n-bit integers.</p>
<p><strong>Problem formulation</strong></p>
<ul>
<li>Given: two n-bit integers: x, and y</li>
<li>Goal: we want to compute their product, $z=x\cdot y$</li>
</ul>
<p>And we want to look at the running time as a function of the number of bits, that's the input size.</p>
<p>Before we dive into this multiplication problem let's review an interesting result from the famous mathematician Carl Gauss. It may not be obvious at first but this lays the foundation for our algorithm.</p>
<p>We begin by observing that addition and subtraction is cheap (easy), multiplication is expensive (hard) is true in most cases. In terms of running times this means that replacing multiplicative steps by arithmetic steps should decrease our time complexity.</p>
<p>Gauss observed that for two complex numbers (a+bi) and (c+di) their product can be computed using just 3 multiplcations instead of 4.</p>
<p>The most obvious approach is:<br/>
$(a+bi)(c+di)= ac-bd+(bc+ad)i$ which req 4 real number multiplication steps</p>
<p>Can we minimize, reduce, the number of multiplications? It turns out the answer is YES!!</p>
<p>The big aha here is noticing that the third term above (bc+ad) looks like the a term in the product of (a+b)(c+d). Notice how similar this is to our initial terms. In fact it's the same barring the imaginary number i. So let's rewrite this a bit.</p>
<ul>
<li>(a+b)(c+d)=ac+bd+bc+ad=ac+bd+(bc+ad)    </li>
<li>Thus (bc+ad)=(a+b)(c+d)-ac-bd</li>
</ul>
<p>So do you what our 3 terms are?</p>
<ul>
<li>Term 1: ac </li>
<li>Term 2: bd</li>
<li>Term 3: (a+b)(c+d) <ul>
<li>here we add the terms first then perform 1 mutiplication</li>
</ul>
</li>
</ul>
<p>thus we have performed just 3 multiplication</p>
<p>Let's now return to our original problem</p>
<ul>
<li>Input: 2 n-bit integers x &amp; y, where n is a power of 2 (this is for convenience)</li>
<li>Goal: to compute z=x*y</li>
<li>Method: Divide and conquer</li>
</ul>
<ol>
<li>break x into two halves $x_l = x_{Left}$ and $x_r = x_{Right}$ (we'll use n/2 as the breakpoint)</li>
<li>do the same with y to get $y_l$ and $y_r$</li>
</ol>
<p>Example</p>
<ul>
<li>$x=182=(10110110)_2$ so $x_l=1011$ and $x_r=0110$</li>
<li>Observe that <ul>
<li>$182=11 \cdot 2^4 + 6$ </li>
<li>which is interpreted as 11 left-shifted $2^{4/2}$ plus 6</li>
</ul>
</li>
</ul>
<p>in general $x=x_l \cdot 2^{n/2} + x_r$</p>
<p>Now back to our D&amp;C. We now have $x_l \; x_r \; y_l \; y_r$ and we have a general formula to relate them back to their original number. we can now write</p>
<ol>
<li>$x \cdot y = (x_l \cdot 2^{n/2} + x_r) \cdot (y_l \cdot 2^{n/2} + y_r)$    </li>
<li>Simplied: $ 2^n (x_l y_l)+ 2^{n/2}(x_l y_l + x_r y_r) + x_r y_r  $</li>
</ol>
<p>If you notice now we have a recursive algorithm for computing x times y. Now recall x and y are both n-bit numbers, so we're trying to compute the product of these two n-bit numbers. This gives us a natural, albeit naive, recursive algorithm for computing the product of x times y.</p>
<pre><code>Input: n-bit integers x,y $n=2^k$ k is a positive integer
Output: z=xy

EasyMultiply(x,y):   
    x_l= 1st n/2 bits of x 
    x_r = last n/2 bits of x 
    y_l= 1st n/2 bits of y 
    y_r = last n/2 bits of y
    A = EasyMultiply(x_l,y_l)
    B = EasyMultiply(x_r,y_r)
    C = EasyMultiply(x_l,y_r)
    D = EasyMultiply(x_r,y_l)
    Z = 2^n A + 2^(n/2)(C+D)+B    
return Z</code></pre>
<p>Run Time analysis</p>
<ul>
<li>The partitioning steps require O(n) for each input</li>
<li>each EasyMutiply requires O(n/2), for a total of 4*O(n/2)</li>
<li>Computing Z is n-time O(n) due to the multiplication</li>
</ul>
<p>Let T(n)=Worst case run time of easy multiply on input of size n</p>
<ul>
<li>then: $T(n)=4 T(n/2) + O(n)=O(n^2)$</li>
</ul>
<p>Obviously this is not all that great, is it? You may be wondering where did gauss go? why did we write so much about him earlier?</p>
<p>In our easy approach we implemented</p>
<ul>
<li>$ 2^n (x_l y_l)+ 2^{n/2}(x_l y_l + x_r y_r) + x_r y_r  $</li>
</ul>
<p>But we could the result from gauss reduce the number of multiplications</p>
<ul>
<li>$(a+b)(c+d)=(x_l+x_r)(y_l+y_r)$</li>
<li>Then $(x_l y_l + x_r y_r)$ becomes $(x_l+x_r)(y_l+y_r) - x_l y_l - x_r y_r$</li>
<li>allowing us to re-use our other terms</li>
</ul>
<p>Our new and improved algorithm becomes:</p>
<ul>
<li>FastMultiply(x,y):   <ul>
<li>Input: n-bit integers x,y $n=2^k$ k is a positive integer</li>
<li>Output: z=xy</li>
<li>$x_l$= 1st n/2 bits of x, $x_r$ = last n/2 bits of x </li>
<li>$y_l$= 1st n/2 bits of y, $y_r$ = last n/2 bits of y</li>
<li>A = FastMultiply($x_l$,$y_l$)</li>
<li>B = FastMultiply($x_r$,$y_r$)<ul>
<li>we've removed the C,D terms from the easy version</li>
</ul>
</li>
<li>E = FastMultiply($x_l+x_r$,$y_l+y_r$) </li>
<li>Z = $2^n A + 2^{n/2}(E - A - B)+B$    </li>
</ul>
</li>
<li>return Z</li>
</ul>
<p>RunTime Analysis:</p>
<ul>
<li>The partitioning steps require O(n) for each input <strong>(Same as before)</strong></li>
<li>each FastMultiply requires O(n/2), for a total of $3 O(n/2)$ <strong>(Previously $4 O(n/2)$)</strong></li>
<li>Computing Z is n-time O(n) due to the multiplication</li>
</ul>
<p>Thus</p>
<ul>
<li>$T(n) = 3 T(n/2) + O(n)$</li>
<li>$\le cn + 3 T(n/2)$</li>
<li>$\le cn + 3 ( c(n/2) + 3 T(n/2^2)$</li>
<li>$\le cn(1+(3/2)) + 3^2 ( c(n/2^2) + 3 T(n/2^3)$</li>
<li>$\le cn(1+(3/2)+(3/2)^2+\cdots+(3/2)^{log_2 n})$<ul>
<li>as you can see this is a geometric series so our last step is to determine the dominant term</li>
<li>The terms are not equal, nor is it decreasing </li>
<li>Therefore it's increasing meaning that the last term dominates</li>
</ul>
</li>
<li>t/f $ O(n (3/2)^{log_2 n} )$</li>
<li>$= O(3^{log_2 n} )$ (recall that by definition $3=2^{log_2 3}$)</li>
<li>$= O(n^{log_2 3} $ ($log_2 3$ is approx 1.59)</li>
</ul>
<p>What is not so obvious here though is that if we divided by more than say 2 then we could get the exponent lower than 1.59. But this comes at a cost as we have to work harder to recombine the terms</p>
<p>Let's look at another particularly weird example.</p>
<p><strong>Example</strong></p>
<p>Let:</p>
<ul>
<li>$x=182=(1011 \; 0110)_2$</li>
<li>$y=154=(1001 \; 1010)_2$</li>
<li>Then<ul>
<li>$x_l=(1011)_2 = 11$ and $x_r=(0110)_2 = 6$</li>
<li>$y_l=(1001)_2 = 9$ and $y_r=(1010)_2=10$</li>
<li>$x_l y_l=11*9=99$</li>
<li>$x_r y_r=6*10=60$</li>
<li>$(x_l+x_r)(y_l+y_r)=(11+6)(9+10)=(323)$</li>
</ul>
</li>
<li>Finally<ul>
<li>$182 \times 154$</li>
<li>$= (99 \times 256) + (323-99-60)\times 16 + 60$</li>
<li>$= 28028$</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DC2:-Linear-Time-Median">DC2: Linear Time Median<a class="anchor-link" href="#DC2:-Linear-Time-Median">¶</a></h2><p>Let's look now at another nice example of divide and conquer. This is the problem of finding the median.</p>
<p>Description:</p>
<ul>
<li>Given: an unsorted list $A=[a_1,...,a_n]$ of n numbers<ul>
<li>these are an arbitrary order and t/f unsorted</li>
</ul>
</li>
<li>Goal: Find the median of A. Finding the median for an even number of elements can be a bit ambiguous.      <ul>
<li>So let's define the median of A as follows: </li>
<li>$median(A)=\lceil \frac{n}{2} \rceil $'th smallest number </li>
<li>hence: for n odd, say n=2l+1, then median is the (l+1)'st smallest </li>
</ul>
</li>
</ul>
<p>For our purposes we will focus on the more general problem of finding the k'th smallest element, where K is an input given to us. Finding the median is just a specific case of this for k = n/2.</p>
<p><strong>Trivial CASE: Sorted</strong> Given sorted A &amp; integer k where $1 \le k \le n$ find the k'th smallest element of A. Now if A happens to be sorted, then it's easy to find the K smallest. We just output the Kth element of the sorted list. So that gives us a very trivial algorithm for solving this problem.</p>
<p><strong>CASE: A UnSorted</strong>  Given an arbitrary A, we simply sort A and then we output the Kth element of this sorted list. To do this you can use your favourite sorting algo. The best, Merge sort, take O(n log n) time to sort A. So the total runtime of this algorithm will be order (n log n).</p>
<p>Now is it possible to find the Kth smallest without first sorting A? Yes, in fact, we'll find the Kth smallest in order and time instead of order n log n time.</p>
<p>The basic approach to this is very similar to the quick sort algorithm so let's review this quickly before moving forward.</p>
<p><strong>$\color{red}{\text{QuickSort (Review)}}$</strong></p>
<ul>
<li>Choose a pivot p</li>
<li>Partition A into 3 sets: A&lt;p; A=p; A&gt;p </li>
<li>Recursively sort A &lt; p and A &gt; p</li>
</ul>
<p>This critical point in this algo is the choice of pivot: p.</p>
<p>If we chose a terrible pivot, such as the smallest element or the largest element, then one of these two lists is going to be of size n-1.  It's just going to go down by one element and  then the running time of our algorithm is going to be order $n^2$.
So what's a good pivot for quicksort? Surprise, It's the median, or something close to the median.</p>
<p>Quicksort ideally runs in O(n log n) time. We're aiming for an order n time algorithm. The key is that we don't have to recursively sort the "less than p" and "bigger than p". We only have to recursively search in one of these two lists. Searching generally requires less effort than sorting.</p>
<p>Let's look at an example:</p>
<ul>
<li>Let A = [5, 2, 20, 17, 11, 13, 8, 9, 11]</li>
<li>choose p=11 ( Don't think of this as the median, it may be but we don't know that yet )</li>
<li>then <ul>
<li>A &lt; p -&gt; [5, 2, 8, 9] </li>
<li>A = p -&gt; [11,11]</li>
<li>A &lt; p -&gt; [20, 17, 13]</li>
</ul>
</li>
</ul>
<p>Now recall that to find the median we need to find the k'th smallest element. Which must be in one of the three sets above. I wish to iterate that k is the position it's not the element!!<br/>
So</p>
<ul>
<li>if $k &lt; 4$ then we want the k'th smallest in A &lt; p (because it has 4 elements)</li>
<li>if $4 &lt; k \le 6$ then we can just choose 11</li>
<li>else if $k \gt 6$ then we want the (k-6)'th element in A &lt; p</li>
</ul>
<p>Now the key for our algorithm is that we're always recursing on at most one list, either the small list or the big list. Or, in the middle case, we don't even have to recurse at all. Whereas QuickSort has to recursively sort these two lists, we don't. We just need to find the median. So we will identify the right list to recurse on, locate the k'th element, and go on our merry way.</p>
<p><strong>$\color{red}{\text{Algorithm}}$</strong></p>
<pre><code>Select(A,k):
Choose a pivot p           
Partition A in A &lt; p, A = p, A &gt; p          
if  k &lt;= |A_{&lt;p}|        
    then return Select(A_{&lt;p},k)               
if  |A_{&lt;p}| &lt; k &lt;= |A_{&gt;p}| + |A_{=p}|           
    then return p            
if  k &gt; |A_{&gt;p}| + |A_{=p}|  
    then return Select(A_{&gt;p}, k - |A_{&lt;p}| - |A_{=p}| )</code></pre>
<p>What is still missing is the choice of pivot p. a good choice will make our algo linear and a bad choice can prove challenging.</p>
<p>Consider</p>
<ul>
<li>T(n)=T(n/2) + O(n) ( recall: this is O(n) )</li>
<li>ideally we want to know the median and use it for p, but this is also the problem we're trying to solve so ... not realistic</li>
</ul>
<p>Suppose we tried an approx median? One approach is to sort and divide A into 4 sets then we can choose p from the middle two sets. Now instead of T(n/2) we have T(3n/4). This still reduces to O(n). In fact you can even divide A into many more sets. for ex Say 100 subsets, then T(n) = T(0.99n)+O(n) which still solves to O(n). We now have a general intution to build a strategy</p>
<p>Let's say a pivot p is good if $|A_{&lt;p}| \le 3n/4$ and $|A_{&gt;p}| \le 3n/4$.</p>
<ul>
<li>this means that the number of elements less than our pivot is at most 3n/4,</li>
<li>similarly the number of elements greater than our pivot is at most 3n/4</li>
<li>this defines a partition of A into 4</li>
</ul>
<p>Now we must find this "good" p in O(n) time.</p>
<p><strong>Method 1: Random Selection?</strong></p>
<ul>
<li>Let p be a random element of A, then what is the probability that p is good pivot?<ul>
<li>Note that there are (n/2) choices of good pivots. </li>
<li>These are the elements in the middle two sets of our partition</li>
<li>thus Pr(p = good) = (n/2)/n = 1/2 </li>
</ul>
</li>
<li>At first this may seem fine. <ul>
<li>we have a 50% chance of getting a good pivot on the first try</li>
<li>so if the first fails we can try a second time. </li>
<li>the odds of getting a good pivot improve </li>
<li>So it's going to take O(n) expected time to find a good pivot</li>
<li>PROBLEM: Expected values are NOT guaranteed, and we need a guarantee!!</li>
</ul>
</li>
</ul>
<p>Let's think a bit more about this. Our aim is still to find a good pivot p in O(n) time. Our aim is to guarantee that our running time is T(3n/4)+O(n). Recall that for our first term as long as the constant remains less than 1 the run time will still solve for O(n). So we have some room. In particular we have T(0.24n). We will use this extra time to help us find a good pivot.</p>
<p>New Run Time: T(3n/4)+T(n/5)+O(n), note that 3/4 + 1/5 is less than 1</p>
<p>Our new approach is to choose a subset S of A where |S|=n/5, Then set p=Median(S)<br/>
How do we choose S? Well ... it needs to be a good representative of the entire set</p>
<ul>
<li>Opt 1: Take S = first 5 elements, then p=median(S)<ul>
<li>if A happens to be sorted this will be quite bad</li>
<li>p will be the (n/10)'th smallest element of A</li>
</ul>
</li>
<li>This is clearly going nowhere</li>
</ul>
<p>We want S to be representative of A, so what does this mean. Well the median(S) should approximate median(A).<br/>
Ideally, for some x in S</p>
<ul>
<li>some elements in S are less than x            </li>
<li>some elements in S are greater than x              </li>
<li>and of course it will contain x as x is chosen from this set</li>
</ul>
<p>For our problem "some" means 2. Hence this will form a set S of 5 elements.</p>
<ul>
<li>Break A into n/5 groups of 5 elements each</li>
<li>Sort each group<ul>
<li>this only take O(1) since the size is constant, it's fixed at 5 elements</li>
</ul>
</li>
<li>choose the middle element! (which is the median)<ul>
<li>3 elements will be at most the middle </li>
<li>3 elements will be at least the middle </li>
</ul>
</li>
<li>Combine the middle elements from each group into a single set S</li>
</ul>
<p><strong>Observe</strong></p>
<ul>
<li>S now has the desired properties we wanted!</li>
<li>Step 2 sorting a single group only takes O(1) time, because it's fixed at a constant 5, it never grows<ul>
<li>sorting n/5 groups then will take O(1)*O(n/5) = O(n/5) as needed!!</li>
</ul>
</li>
</ul>
<p><strong>$\color{red}{\text{FastSelect Algorithm}}$</strong></p>
<pre><code>Input:   A - an unsorted array of size n
         k an integer with 1 &lt;= k &lt;= n 
Output:  k'th smallest element of A

FastSelect(A,k):

Break A into ceil(n/5) groups         G_1,G_2,...,G_n/5
    # doesn't matter how you break A

For j=1-&gt;n/5:
    sort(G_i)
    let m_i = median(G_i)

S = {m_1,m_2,...,m_n/5}             # these are the medians of each group
p = FastSelect(S,n/10)              # p is the median of the medians (= median of elements in S)
Partition A into A_&lt;p, A_=p, A_&gt;p

# now recurse on one of the sets depending on the size
# this is the equivalent of the quicksort algorithm 

if k &lt;= |A_&lt;p|:
    then return FastSelect(A_&lt;p,k)    
if k &gt; |A_&gt;p| + |A_=p|:
    then return FastSelect(A_&gt;p,k-|A_&lt;p|-|A_=p|)
else return p</code></pre>
<p><strong>$\color{red}{\text{FastSelect Running Time}}$</strong></p>
<ul>
<li>Breaking A into n/5 groups -&gt; take O(n)</li>
<li>Sorting all the groups -&gt; take O(1) per group = O(n)</li>
<li>p = fastselect -&gt; take T(n/5)</li>
<li>recurse on one of the three sets requires T(3n/4)</li>
<li>Finally<ul>
<li>T(3n/4)+T(n/5)+T(n) = O(n)</li>
</ul>
</li>
</ul>
<p>Sketch of the proof that in fact p is a good pivot!
<img src="CS6515_images/DC2-001.png" width="500;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>Excercise</strong></p>
<p>In our FastSelect algorithm we selected groups of 5 elements each. Why 5? Why not 3 or 7? Explain your answer.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DC3:-Solving-Recurrances">DC3: Solving Recurrances<a class="anchor-link" href="#DC3:-Solving-Recurrances">¶</a></h2><p><a href="https://web.stanford.edu/class/archive/cs/cs161/cs161.1168/lecture3.pdf">Stanford Link</a></p>
<h3 id="Example-Recurrances">Example Recurrances<a class="anchor-link" href="#Example-Recurrances">¶</a></h3><p>This is a short refresher lecture on Solving Recurrences, with a focus on the type of recurrences that arise in divide and conquer algorithms.</p>
<p>First let's look at a few that we've encountered already</p>
<ol>
<li><strong>MergeSort</strong>: T(n)=2*T(n/2)+O(n); which you may recall solves to O(n log n)       </li>
<li><strong>Naive Integer Multiplication</strong>: T(n) = 4*T(n/2)+O(n); which solved to O($n^2$)        </li>
<li><strong>Fast Integer Multiplication</strong>: T(n) = 3*T(n/2)+O(n) which solves to O($n^{log_2 3}$)         </li>
<li><strong>Median Finding</strong>: T(n) = T(3n/4)+O(n); Which solves to O(n)           </li>
</ol>
<p><strong>MERGESORT</strong></p>
<p>Let's now take a closer look at #2 to better understand how this solved.</p>
<p>We begin with T(n) = 4T(n/2)+O(n) which is just the original/raw time complexity.<br/>
Recall that O(n) just means that there is a constant c &gt; 0 s.t. O(n) $\le$ c*n<br/>
t/f we can replace O(n) with cn to get an upper bound: $T(n) \le 4T(n/2) + cn$</p>
<p>Now we can move to solve that recurrance</p>
<ul>
<li>$T(n) \le cn + 4T(n/2)$ we've just re-ordered the terms</li>
<li>$\le cn +4[ 4T(n/2^2) + c (n/2)  ] $ we've substituted T(n/2) with the eq from above $T(n) \le 4T(n/2) + cn$     </li>
<li>$= cn(1+4/2)+4^2 T(n/2^2)$ collecting terms</li>
<li>$\le cn(1+4/2)+4^2 [ 4T(n/2^3) + c (n/2^2)  ] $ another substitution</li>
<li>$= cn(1+(4/2)+(4/2)^2)+4^3 T(n/2^3)$ collecting terms again</li>
</ul>
<p>You may observe that this is turning into a geometric series. So let's just cut to the chase</p>
<ul>
<li>$= cn(1+(4/2)+(4/2)^2+\cdots+(4/2)^{i-1}) + 4^i T(n/2^i) $ after subsititution i-times</li>
</ul>
<p>Now Let $i=log_2(n)$ then $n/2^i = 1$ (if i=log 2 n then 2^i=2^{log 2 n} which by definition is n, thus n/2^i = n/n = 1)</p>
<p>Substitute $i=log_2(n)$ back into our expansion to get</p>
<ul>
<li>$cn(1+(4/2)+(4/2)^2+\cdots+(4/2)^{log_2 n - 1})$ <ul>
<li>we replace n-1 with n as this insignificant in the big notation</li>
<li>this is O(n)*O((4/2)^{log_2 n}) </li>
<li>with a further simplificiation O((4/2)^{log_2 n}) = O(n^2 / n) = O(n)</li>
</ul>
</li>
<li>$4^{log_2 n} T(1)$ = O($n^2$)</li>
</ul>
<p>Finally we have</p>
<ul>
<li>O(n)*O(n)+O(n^2) which is just O(n^2)</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>Geometric Series</strong></p>
<p>A quick refresher</p>
<ul>
<li>Consider some alpha $\alpha &gt; 0$</li>
<li>Then <ul>
<li>$\sum_{j=0}^k \alpha^j = 1 + \alpha + \alpha^2 + \cdots + \alpha^k$</li>
<li>$ = \begin{cases} 
O(\alpha^k) &amp; \text{ if } \alpha &gt; 1  \\
O(k) &amp; \text{ if } \alpha = 1 \\
O(1) &amp; \text{ if } \alpha &lt; 1  \\
\end{cases}$</li>
</ul>
</li>
</ul>
<p><strong>Polynomials</strong></p>
<p>How do we convert an exponential logarithm such as $3^{log_2 n}$ to a polynomial like $n^c$?<br/>
First we observe that $3 = 2^{log_2 3}$ by definition of logarithms<br/>
We sub this back into our original problem $3^{log_2 n}$</p>
<ul>
<li>$3^{log_2 n}$</li>
<li>$= (2^{log_2 3})^{log_2 n}$</li>
<li>$= (2^{(log_2 3)(log_2 n)})$</li>
<li>$= (2^{(log_2 n)})^{(log_2 3)}$</li>
<li>$= n^{(log_2 3)}$ since $2^{log_2 n}$ is just n</li>
</ul>
<p>Thus</p>
<ul>
<li>for $c = (log_2 3)$ we have $n^c = 3^{log_2 n}$ as required</li>
</ul>
<p><strong>Example 2</strong></p>
<p>Recall that the complexity for our fast integer multiplication was given as  T(n) = 3*T(n/2)+O(n) which we solved to O($n^{log_2 3}$). Now let's look at how we can derive this.</p>
<p>T(n) = 3*T(n/2)+O(n). Similar to before let's just jump stright into the expansion</p>
<ul>
<li>$T(n) \le cn(1+(3/2)+(3/2)^2+\cdots+(3/2)^{i-1}) + 3^i T(n/2^i) $</li>
<li>Let $i=log_2(n)$ same as before </li>
<li>Sub back $T(n) \le cn(1+(3/2)+(3/2)^2+\cdots+(3/2)^{log_2(n)-1}) + 3^{log_2(n)} T(1) $ </li>
<li>Now we observe that 3/2 &gt; 1 and therefore the last term in the series is dominant</li>
</ul>
<p>Transforming this into big O yields</p>
<ul>
<li>cn becomes O(n)            </li>
<li>the last term in the series is O{$(3/2)^{log_2(n)}$ we drop the minus 1 just as before                    </li>
<li>the final term is just $3^{log_2(n)}$                 </li>
</ul>
<p>Now we expand $\large (3/2)^{log_2(n)} = (3^{log_2(n)} / 2^{log_2(n)}) = (3^{log_2(n)} / n ) = (3^{log_2(n)})(1/n)$</p>
<p>Finally putting it all together</p>
<ul>
<li>$T(n) \le cn(1+(3/2)+(3/2)^2+\cdots+(3/2)^{log_2(n)-1}) + 3^{log_2(n)} T(1) $ </li>
<li>$T(n) \le O(n) O(3^{log_2(n)}) O(1/n) + 3^{log_2(n)} T(1) $ </li>
<li>$T(n) \le O(3^{log_2(n)}) + 3^{log_2(n)} T(1) $ O(n) and O(1/n) cancel out</li>
<li>$T(n) \le O(3^{log_2(n)}) $ since $3^{log_2(n)}$ doesn't exceed the former</li>
<li>$T(n) \le O(n^{log_2(3)}) $ from our result in the previous section on polynomials</li>
</ul>
<h3 id="Master-Theorem-Generalization">Master Theorem Generalization<a class="anchor-link" href="#Master-Theorem-Generalization">¶</a></h3><p>If $T(n) = aT(n/b)+O(n^d)$, for a&gt;0, b&gt;0,d$\ge$0<br/>
Then<br/>
$ T(n) = 
\begin{cases}
&amp; = O(n^d) &amp; \text{ if } d &gt; \log_b a \\
&amp; = O(n^d \log n) &amp; \text{ if } d = \log_b a \\
&amp; = O(n^{ log_b a}) &amp; \text{ if } d &lt; \log_b a \\
\end{cases}$</p>
<p><strong>General Recurrance</strong></p>
<p>Applicable when $T(n)$ is of the form $T(n)=aT(n/b) + O(n)$, and a&gt;0,b&gt;0</p>
<ul>
<li>if a &lt; b then $T(n) \le O(n)$ The first term in the geometric representation dominates        </li>
<li>if a = b then $T(n) \le O(n log n)$   </li>
<li>if a &gt; b then $T(n) \le O(n^{log_b a}) $ The last term in the geometric representation dominates    </li>
</ul>
<p>Examples</p>
<ol>
<li>T(n/2)+O(1)<ul>
<li>so take a=1, b=2, d=0 </li>
<li>then $\log_b a = \log_2 1 = 0 = d$</li>
<li>thus $O(n^0 \log n) = O(\log n)$</li>
</ul>
</li>
<li>2T(n/3)+O(1)<ul>
<li>so take a=2, b=3, d=0 </li>
<li>then $\log_b a = \log_3 2 \gt 0 = d$ so we apply the last row of the master theorem</li>
<li>thus $O(n \log_b a) = O(n^{\log_3(2)})$</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Examples</p>
<p>2T(n/3)+O(1)</p>
<ul>
<li>so take a=2, b=3, d=0 </li>
<li>then $\log_b a = \log_3 2 \gt 0 = d$ so we apply the last row of the master theorem</li>
<li>thus $O(n \log_b a) = O(n^{\log_3(2)})$</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DC4:-FFT-Fast-Fourier-Transform">DC4: FFT-Fast Fourier Transform<a class="anchor-link" href="#DC4:-FFT-Fast-Fourier-Transform">¶</a></h2><p>What we've seen so far is how to use divide and conquer in a clever way to multiply large integers. So for N bit integers, we were able to multiply and compute their product in time better than $O(n^2)$. What we're going to do now is multiply polynomials. To do this, we're going to use the beautiful divide and conquer algorithm known as FFT.</p>
<ul>
<li>FFT stands for fast fourier transform.</li>
</ul>
<p>So here's the set up. We have a pair of polynomials</p>
<ul>
<li>$A(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_{n-1} x^{n-1}$</li>
<li>$B(x) = b_0 + b_1 x + b_2 x^2 + \cdots + b_{n-1} x^{n-1}$</li>
</ul>
<p>The goal is to compute the product polynomial</p>
<ul>
<li>$C(x)=A(x)B(x)$</li>
<li>which we expect to have the form <ul>
<li>$c_0 + c_1 x + c_2 x^2 + \cdots + c_{2n-1} x^{2n-2}$</li>
<li>Since the degree of C of X is at most 2n-2</li>
</ul>
</li>
<li>where $c_k = a_0 b_k + a_1 b_{k-1} + \cdots + a_k b_0 $<ul>
<li>this is an easily provable result left to the reader</li>
</ul>
</li>
</ul>
<p>We will use an alternative notation to help simplify the problem, This doesn't, however, change the problem.</p>
<ul>
<li>Use vector notation for the polynomial coefficients<ul>
<li>$A = (a_0,a_1,a_2,, \cdots ,a_{n-1})$</li>
<li>$B = (b_0,b_1,b_2,, \cdots ,b_{n-1})$</li>
<li>$C = A*B = (c_0,c_1,c_2,, \cdots ,c_{2n-2})$<ul>
<li>the star symbol denotes the convolution</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="P1:-Review-Polynomials-&amp;-Convolutions">P1: Review-Polynomials &amp; Convolutions<a class="anchor-link" href="#P1:-Review-Polynomials-&amp;-Convolutions">¶</a></h3><p>Let's do a quick review of polynomial multiplication</p>
<ul>
<li>Let A = $1+2x+3x^2$ and B = $2-x+4x^2$</li>
<li>Then c = a*b = (2,3,8,5,12) <ul>
<li>since there are k terms for $c_k$ this will take O(k) time for $c_k$, </li>
<li>but it takes a total of $O(n^2)$ time for the original A &amp; B</li>
</ul>
</li>
<li>We want better than $O(n^2)$, in fact our goal is O(n log n)    </li>
</ul>
<p><strong>NOTE</strong> Convolutions are a rather unique type of function with many applications in signal and image processing. I highly recommend googling a bit if you've never encountered this. Now let's get back to our polynomials</p>
<p>There are 2 natural ways to express a polynomial</p>
<ul>
<li><ol>
<li>Coefficients: which we used above $A = (a_0,a_1,a_2,, \cdots ,a_{n-1})$</li>
</ol>
</li>
<li><ol>
<li>Values: at each term, ie $A(x_1),A(x_2),\cdots,A(x_n)$</li>
</ol>
</li>
</ul>
<p><strong>Lemma</strong><br/>
A polynomial of degree n-1 is uniquely determined by it's values at any n distinct points</p>
<ul>
<li>just think of a simple line. It has degree 1 and it determined by it's values at 2 distinct points</li>
</ul>
<p>Point 2 above is better for multiplication purposes. What FFT does is switch between the 2 representations. The key idea for multiplying polynomials is that multiplying polynomials is easy when we have the polynomials in the values representation. It may not be obvious why the value approach makes multiplication easier so let's look at an example.</p>
<p>So suppose that we know the polynomial A(x) and B(x) evaluated at the same 2n points, ($x_1$ -&gt; $x_{2n})$</p>
<ul>
<li>ie given <ul>
<li>$A(x_1),A(x_2),\cdots,A(x_{2n})$</li>
<li>$B(x_1),B(x_2),\cdots,B(x_{2n})$</li>
<li>At the same 2n points</li>
</ul>
</li>
<li>Then we can compute the product polynomial C(x) at these 2n points by just computing the product of two numbers for each i.<ul>
<li>ie $C(x_i) = A(x_i) B(x_i)$</li>
<li>These are just numbers, making $C_i$ just a number</li>
<li>which take just O(1) to compute for each i</li>
<li>t/f it takes O(n) total time to compute this product polynomial.</li>
</ul>
</li>
</ul>
<p>Now why do we take A and B at two n points? Well, C is a polynomial of degree at most 2n-2. So we needed at least 2n minus one points. So two n points suffices.</p>
<p>The summary is that, if we have the value of these polynomials, A(x) and B(x), at n, or two n points, then we compute this product polynomial at the same points in order n total time. So what we're going to do is we're going to use FFT to
convert from the coefficients to the values, this is going to take O(n log n) to convert, and then it'll take O(n) total time to compute the product polynomial at these two n points, and then we do FFT to convert back from the values of C(x), at these two n points, back to the coefficients for C(x), and that again will take O(n log n) time.</p>
<p>So let's dive in now to see how we do FFT, which converts from the coefficients to the values.</p>
<p>Again let's re-iterate our terms here</p>
<ul>
<li>Given a polonomial of degree n <ul>
<li>$A(x) = a_0+a_1 x+a_2 x^2+\cdots+ a_{n-1} x^{n-1}$</li>
<li>we represent this as $A = (a_0,a_1,a_2, \cdots ,a_{n-1})$ </li>
</ul>
</li>
<li>Goal is to compute: $A(x_1),A(x_2),\cdots,A(x_{2n})$<ul>
<li>for any 2n points that we get to choose</li>
</ul>
</li>
</ul>
<p>So let's suppose $x_1,...,x_n$ are the opposties of $x_{n+1},...,x_{2n}$<br/>
Then $x_{n+1}=-x_1, \; x_{n+2}=-x_2,...,x_{2n}=-x_n$<br/>
(let call this the $\pm$ property)</p>
<p>Observe that for the even terms in A the opposite will have the same sign as they are powers of 2k. But for the odd terms their sign will be different.</p>
<p>After splitting we have</p>
<ul>
<li>$A_{even} = (a_0,a_2, \cdots ,a_{n-2})$ </li>
<li>and $A_{even} = (a_1,a_3, \cdots ,a_{n-1})$ </li>
</ul>
<p>Which we can put back into a polynomial (using y to help alleviate any confusion)</p>
<ul>
<li>$A_{even}(y) = a_0+a_2 y + a_4 y^2 +\cdots+ a_{n-2} x^{(n-2)/2} $</li>
<li>$A_{odd}(y) = a_1+a_3 y + a_5 y^2\cdots+ a_{n-1} x^{(n-2)/2} $</li>
</ul>
<p>These are new polynomials! these are not the same as the original A(x) polynomial. However, A(x) can be formed from these. In fact</p>
<ul>
<li>$A(x) = A_{even}(x^2) + x A_{odd}(x^2) $ <ul>
<li>we multiply the odd terms by x, otherwise we would be off-by-one</li>
<li>verify this for a polynomial of degree 3</li>
</ul>
</li>
</ul>
<p>At this point you should be picking up on our divide and conquer approach. We've divided the polynomial and we have yet to conquer.</p>
<p>Now that we know $A(x) = A_{even}(x^2) + x A_{odd}(x^2) $ it's pretty simple to translate A(x) into $A(x_1),A(x_2),\cdots,A(x_{2n})$ all we need to do is</p>
<ul>
<li>compute at some point i, and use our $\pm$ property to compute at n+i<ul>
<li>So $A(x_i) = A_{even}(x_i^2) + x_i A_{odd}(x_i^2) $</li>
<li>and $A(x_{n+i}) = A(-x_i) = A_{even}(x_i^2) - x_i A_{odd}(x_i^2) $</li>
<li>that was easy eh?</li>
</ul>
</li>
</ul>
<p>So now</p>
<ul>
<li>given $A_{even}(y_1),...,A_{even}(y_n)$ and $A_{odd}(y_1),...,A_{odd}(y_n)$</li>
<li>for $y_1=x_1^2,...,y_n=x_n^2$</li>
<li>we compute $A(x_1),A(x_2),\cdots,A(x_{2n})$</li>
<li>in just O(n) time </li>
</ul>
<p><strong>FFT-Summary</strong><br/>
To get $A(x)$ of deg $\le n-1$ at 2n points $x_1,...x_{2n}$ of our choosing.</p>
<ul>
<li>1: We define $A_{even}$ and $A_{odd}$ of deg $\le (n/2)-1$</li>
<li>2: We recursively evaluate $A(x)$ at n points using $A(x) = A_{even}(x^2) + x A_{odd}(x^2) $<ul>
<li>Recall $y_1=x_1^2=x_{n+1}^2,...,y_n=x_n^2=x_{2n}^2$</li>
</ul>
</li>
<li>3: In O(n) time we get A(x) at points $x_1,...x_{2n}$</li>
</ul>
<p>This leads to a runtime of $2T(n)+O(n)=O(n log n)$, The astute student will also notice that so far this is very similar to the merge sort algorithm.</p>
<p>All that is left is to prove the $\pm$ property holds not only for the initial polynomial but also for each level of recursion. Which may come as a bit of surprise. Consider what the $\pm$ prop means</p>
<ul>
<li>Initially we choose $x_{n+1}=-x_1,...$</li>
<li>next we choose $y_1=x_1^2,...$</li>
<li>Which implies that $y_1 = -y_{(n/2)+1}, ...$</li>
<li>but then $x_1^2 = -x_{(n/2)+1}$ which would mean that the square of x is a negative number ... <ul>
<li>if you're beginning to imagine the root of -1 you're on the right track</li>
<li>we will need to venture into the world of complex numbers</li>
</ul>
</li>
</ul>
<p>Let's review complex numbers ...</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="P2:--Review-Complex-Numbers">P2:  Review-Complex Numbers<a class="anchor-link" href="#P2:--Review-Complex-Numbers">¶</a></h3><p>Recall</p>
<ul>
<li>$z=a+bi$ is how we define a complex number</li>
<li>a is said to be the real part </li>
<li>and bi is said to be the imaginary component</li>
<li>i here is defined as $\sqrt{-1}$ which of course does not exist in the real plane of number line</li>
</ul>
<p>You may also recall that when working with complex numbers it is often more convenient to work in the polar co-ordinate system where a point such as (a,b) in the compelx plane is translate to (r,$\theta$) in the polar plane. r describes the points radius or distance from (0,0) and $\theta$ describes the angle from the real axis.</p>
<p>The following illustration may be a better explanation
<img src="CS6515_images/FFT-001.png" width="400;"/></p>
<p>We will state without prrof the following properties</p>
<ul>
<li>$(a,b) = (r cos \theta, r sin \theta)$ This is how we can convert between the two planes</li>
<li>using Euler's formula<ul>
<li>$e^{i\theta} = cos \theta + i sin \theta$</li>
</ul>
</li>
<li>we multiply each side by r to get z<ul>
<li>$z = r(e^{i\theta}) = r(cos \theta + i sin \theta)$</li>
</ul>
</li>
</ul>
<p><strong>Muliplying numbers in Polar</strong><br/>
Consider two complex numbers $z_1,z_2$</p>
<ul>
<li>Multiplication: <ul>
<li>$z_1 \times z_2$</li>
<li>$= (r_1,\theta_1) \times (r_2,\theta_2)$</li>
<li>$= (r_1 r_2, \theta_1 + \theta_2)$</li>
</ul>
</li>
<li>so far so good right? we've not done anything very unusual </li>
<li>but now consider $-z$?<ul>
<li>this is just $(r,\theta+\pi)$</li>
<li>-z is just the reflection of z, so we add pi to get this reflection. the radius doesn't change</li>
</ul>
</li>
</ul>
<p><img src="CS6515_images/FFT-002.png" width="400;"/></p>
<h3 id="Roots-of-Unity">Roots of Unity<a class="anchor-link" href="#Roots-of-Unity">¶</a></h3><p>For our purpose we will need numbers called the n'th complex roots of unity (unity here means 1)</p>
<ul>
<li>n'th complex root examples<ul>
<li>for n=2: the roots are 1, -1</li>
<li>for n=4: the roots are 1,-1,i,-i</li>
</ul>
</li>
<li>formally<ul>
<li>the n'th complex roots of unity are the complex numbers z where $z^n = 1$</li>
</ul>
</li>
</ul>
<p>This can be bit difficult to conceptualize. What it boils down to is that the roots of n'th complex roots of unity are the n'th points on a unit circle in the complex space. Mathematically this means that for $z=(r,\theta)$ we have $z^n = (r^n,n \theta)$. $r^n$ is always 1 as this is a unit circle. $n\theta$ is very simple. Just solve for theta $\theta = 2 \pi j / n$ where j is an integer s.t. $0 \le j \le n$. This basically just amounts to dividing the unit circle into n points, evenly spaced of course.</p>
<p>Here is a visualization 
<img src="CS6515_images/FFT-003.png" width="400;"/></p>
<p>Let's kick it up a notch</p>
<ul>
<li>let $\omega_n = (1,2\pi/n)$ which corresponds to j=1</li>
<li>then $\omega_n^2 = (1,2*2 \pi / n)$ which corresponds to j=2</li>
<li>etc etc</li>
<li>But recall Euler's formula!<ul>
<li>application yields $\omega_n = (1,2\pi/n) = e^{2 \pi i / n}$</li>
<li>thus we generalize $\omega_n^j = e^{2 \pi i j / n}$</li>
</ul>
</li>
</ul>
<p><img src="CS6515_images/FFT-004.png" width="400;"/></p>
<p>Here's a worked example showing the relationship between $\omega_n^8$ and $\omega_n^{16}$ which is just$ (\omega_n^8)^2$</p>
<p><img src="CS6515_images/FFT-005.png" width="500;"/></p>
<p>All of this was just to demonstrate two key properties of the n'th roots of unity which we will need for our FFT algo.</p>
<p><strong>Property 1</strong><br/>
For an even n: the n'th roots satisfy the $\pm$ property.</p>
<ul>
<li>The 1st (n/2) n'th roots are the opposite of the last n/2 roots</li>
<li>ie<ul>
<li>$\omega_n^0 = -\omega_n^{n/2}$</li>
<li>$\omega_n^1 = -\omega_n^{(n/2)+1}$</li>
<li>...</li>
<li>$\omega_n^{(n/2)-1} = -\omega_n^{n-1}$</li>
</ul>
</li>
</ul>
<p><strong>Property 2</strong><br/>
For n a power of 2, $n=2^k$, the $(\text{ n'th roots})^2 = \text{ (n/2)'nd roots }$</p>
<ul>
<li>$\large (\omega_n^j)^2 = (1,\frac{2\pi}{n}j)^2 = (1,\frac{2\pi}{n/2}j) = \omega_{n/2}^2$</li>
<li>similarly <ul>
<li>$(\omega_n^{(n/2)+j})^2 = (-\omega_n^j)^2 = \omega_{n/2}^j$</li>
</ul>
</li>
</ul>
<p>Now we are ready to move forward in our FFT algorithm. It should not come as a surprise when we choose the n't roots of unity as our 2n points to evaluate $A(x)$</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Examples-FFT">Examples-FFT<a class="anchor-link" href="#Examples-FFT">¶</a></h3><p><strong>Q1: What is the sum of the n'th roots of Unity?</strong><br/>
Recall that the n'th roots are defined as $\omega_n^j = e^{2 \pi i j / n}$</p>
<ul>
<li>So we take the sum</li>
<li>$\large \sum_0^{n-1} e^{j(2 \pi i / n)}$ or if you prefer $\large \sum_0^{n-1} \omega_n^j$</li>
<li>which we can expand </li>
<li>$\large 1 + \omega_n^1 + \omega_n^2 + \cdots + \omega_n^{n-1}$</li>
<li>$\large \frac{\omega^n - 1}{\omega - 1}$</li>
<li>which is just 0 ... </li>
<li>in case you forgot why $\omega^n$ is the n'th root of unity meaning $\omega^n - 1 = 0$</li>
<li>in case you're still confused</li>
<li>the complex roots of the equation $z^n - 1 = 0$, are the roots of unity </li>
<li>in case you're still confused ... I'm all out of ideas try google?</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DC5:-FFT-Algorithm">DC5: FFT-Algorithm<a class="anchor-link" href="#DC5:-FFT-Algorithm">¶</a></h2><p>Now that we have all the necassary pieces we can begin to put the puzzle together. So let's recap the problem, then draw out our algorithm</p>
<p><strong>Goal</strong> To Evaluate the polynomial A(x) of deg $\le n-1$ at n points, where n is a power of k</p>
<p>Recall that in order to perform the polynomial multiplication we will need A(x) at 2n points. In order to obtain at two N points instead of N points, we can just pad the polynomial, the coefficients with zeros, so that we view the polynomial as a degree 2n-1 polynomial.</p>
<p>Now what are these n points that we're going to choose? We're going to choose the n'th roots of unity as our end points, which we're going to evaluate the polynomial of A(x) at. Now since n is a power of 2, ie N equals $2^k$ for some positive integer k. Then we know that these n points, the n'th roots of unity, will satisfy the $\pm$ property. So the first N over two are opposite of the last N over two. Also recall that the second poperty demonstrated that the square of the n'th roots are the (n/2)'nd roots.</p>
<p>So we'll take the polynomial A(x) and split into $A_{even}$ and $A_{odd}$. Each with at most degree $(n/2)-1$. Then we'll recursively evaluate $A_{even}$ and $A_{odd}$ at the (n'th roots)^2. Which because of property 2 is just the (n/2)nd roots. Evaluation requires 2T(n/2) time</p>
<p>Finally, once we have $A_{even}$ and $A_{odd}$ at the (n/2)nd roots then it will require O(n) time to get A(x) at the n'th roots. Using our handy little formula: $A(x) = A_{even}(x^2) + x A_{odd}$</p>
<p><strong>$\color{red}{\text{FFT Pseudocode - Version 1}}$</strong></p>
<p>FFT(a,$\omega$):</p>
<p>INPUT: Coefficients $a=(a_0,a_1,...,a_{n-1})$ for polynomial $A(x)$</p>
<ul>
<li>where n is a power of 2</li>
<li>and $\omega$ is a n'th root of unity         </li>
</ul>
<p>OUTPUT: $A(\omega^0),A(\omega),A(\omega^2),\cdots,A(\omega^{n-1})$</p>
<p>If n=1, return A(1)<br/>
Let</p>
<ul>
<li>a_even = $(a_0,a_2,...,a_{n-2})$</li>
<li>a_odd = $(a_1,a_3,...,a_{n-1})$</li>
</ul>
<p>(s_0,s<em>2,...,s</em>{n/2-1}) = FFT(a_even,$\omega^2$) returns A_even($\omega^0$),A_even($\omega^2$),...,A_even($\omega^{n-2}$)</p>
<p>(t_0,t<em>1,...,t</em>{n/2-1}) = FFT(a_odd,$\omega^2$)  returns A_odd($\omega^1$),A_odd($\omega^3$),...,A_odd($\omega^{n-2}$)</p>
<p>for j = 0 -&gt; (n/2)-1:<br/>
      A($\omega^j$) = A_even($\omega^{2j}$) + $\omega^j$ A_odd($\omega^{2j}$)<br/>
      A($\omega^{(n/2)+j}$ = A(-$\omega^j$) = A_even($\omega^{2j}$) - $\omega^j$ A_odd($\omega^{2j}$)
      # notice the subtraction in the second equation</p>
<p>Here's a more compact version</p>
<p><strong>$\color{red}{\text{FFT Pseudocode - Version 2}}$</strong></p>
<pre><code>FFT(a,w):
If n=1, return A(1)                  # Our base case

a_even = $(a_0,a_2,...,a_{n-2})$     # Now the split   O(n)
a_odd = $(a_1,a_3,...,a_{n-1})$

s[:] = FFT(a_even,w^2)               # T(n/2) 
t[:] = FFT(a_odd,w^2)                # T(n/2) 

for j = 0 -&gt; (n/2)-1:                # O(n)
    r[j] = s[j] + w^j t[j]
    r[(n/2)+j] = s[j] - w^j t[j]

return r[:]</code></pre>
<p>Total Run Time:    2*T(n/2)+O(n) = solves to O(n log n)</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Example">Example<a class="anchor-link" href="#Example">¶</a></h3><p><strong>Suppose a=(1,0,0,0) are the coefficients of a polynomial. What is the FFT?</strong><br/>
You don't need to go through the entire fft algorithm. Just compute $A(\omega_n^i)$</p>
<ul>
<li>we take n=4 since we have 4 co-efficients, then </li>
<li>$A(\omega_4^0) = A(1) = 1(1)^0+0(1)^1+0(1)^2+0(1)^3 = 1$</li>
<li>$A(\omega_4^1) = A(i) = 1(i)^0+0(i)^1+0(i)^2+0(i)^3 = 1$</li>
<li>$A(\omega_4^2) = A(-1) = 1(-1)^0+0(-1)^1+0(-1)^2+0(-1)^3 = 1$</li>
<li>$A(\omega_4^3) = A(-i) = 1(-i)^0+0(-i)^1+0(-i)^2+0(-i)^3 = 1$</li>
</ul>
<p>You can even run the inverse FFT to get</p>
<ul>
<li>1/4 * (1,1,1,1)</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="FFT-Polynomial-Multiplication">FFT Polynomial Multiplication<a class="anchor-link" href="#FFT-Polynomial-Multiplication">¶</a></h3><p>Now you may recall way back when we began this section our problem was to multiply two n-degree polyomials a=(a_0,...,a_n-1) and b=(b_0,...,b_n-1) corresponding to the coefficients for a pair of polynomials A(x) and B(x).</p>
<p>The output of this multiplication is c=(c_0,...,c_2n-2) the coefficients for the polynomial C(x)=A(x)B(x). Equivalently C is a convolution of A and B.</p>
<p>In order to multiply these polynomials A of X and B of X,  we want to convert from the coefficients A and B to the values of these polynomials A of X and B of X.</p>
<p>How many points do we need these polynomials at? Well since C is of degree 2n-2, we want C(x) at at least two and 2n-1 points.  In order to maintain that n is a power of 2, we'll evaluate A of X and B of X at two endpoints. In order to do that, we'll run FFT. We will consider A(x) and B(x) as polynomials of degree 2n minus one, by padding them with zeros.</p>
<p>So we run FFT with this vector a and the 2n'th root of unity. And this is going to give us a polynomial A(x) at the two nth root unity.</p>
<ul>
<li>$(r_0,r_1,...,r_{2n-1}) = FFT(a,\omega_{2n})$</li>
</ul>
<p>Similarly for B(x)</p>
<ul>
<li>$(s_0,s_1,...,s_{2n-1}) = FFT(b,\omega_{2n})$</li>
</ul>
<p>Now we can compute C(x) at the two nth roots of unity</p>
<ul>
<li>for j = 0 -&gt; 2n-1:<ul>
<li>$t_j = r_j \times s_j $</li>
</ul>
</li>
</ul>
<p>Now that we have C(x) at the 2n'th roots of unity we need to work back to get the co-efficients. ie we need to go from the values back to the co-efficients. Whereas in previous cases we went from the co-efficients to the values, which is the FFT call. So how do we do this? well turns out there is an inverse to the FFT that does the job for us. The inverse or reverse FFT is very similar to the original FFT. But it requires a bit more math to understand why</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inverse-FFT">Inverse FFT<a class="anchor-link" href="#Inverse-FFT">¶</a></h3><p>So before we explore inverse FFT, it will be useful to explore the linear algebraic view of FFT. In this way, we can look at FFT as multiplication of matrices and vectors. So consider a point x_j, The polynomial A of X evaluated at the point x_j equals a_0 plus a_1 x_j plus a_2 x_j squared and so on. The last term is a_n-1 x_j to the n-1. Notice that this quantity can be rewritten as the inner product of two vectors.</p>
<p>For point $x_j$:</p>
<blockquote><p>$A(x_j) = a_0 + a_1 x_j + a_2 x_j^2 + \cdots + a_{n-1} x_j^{n-1} $<br/>
$= (1,x_j,x_j^2,...,x_j^{n-1}) \times (a_0,a_1,a_2,...,a_{n-1})$</p>
</blockquote>
<p>So for n points it gets rather tedious to write this all out so we switch to matrix notation:<br/>
<img src="CS6515_images/FFT-006.png" width="400;"/></p>
<p>The above is just for an arbitrary x. Watch what happens when we apply our evaluation using the n roots of unity, ie take $x_j=\omega_n^j$
<img src="CS6515_images/FFT-007.png" width="400;"/></p>
<p>Notice the symmetry. Also notice that the central matrix is just powers of $\omega_n$.</p>
<p>Let's simplify this a bit more. Call the middle matrix $M_n(\omega_n)$, the final vector a and the vector at the left is A<br/>
Then we can write this as</p>
<blockquote><p>$A = M_n(\omega_n) a$</p>
</blockquote>
<p>Which is exactly what $FFT(a,\omega_n)$ returns/computes. So what do you the inverse fft is?</p>
<blockquote><p>$InvFFT(a,\omega_n) \to a = M_n^{-1}(\omega_n) A $</p>
</blockquote>
<p>So what is this M inverse? well as you may have guessed the answer lies in yet another theorem!</p>
<blockquote><p>Lemma:<br/>
$M_n^{-1}(\omega_n) = \frac{1}{n} M_n(\omega_n^{-1}) = \frac{1}{n} M_n(\omega_n^{n-1})$<br/>
<br/>
recall that $\omega_n^{-1}$ is the number that satifies the result $\omega_n \times \omega_n^{-1} = 1$<br/>
which you may recall is also $\omega_n \times \omega_n^{n-1} = \omega_n^n = \omega_n^0 = 1$</p>
</blockquote>
<p>So Now let's take our original result</p>
<ul>
<li>$a = M_n^{-1}(\omega_n) A $</li>
<li>$n a = M_n^{-1}(\omega_n^{n-1}) A $ (apply the lemma and multiply each side by n)</li>
<li>Now you should be able to see that ... this is just an FFT computation</li>
<li>the inverse has disappeared </li>
<li>thus $a = \frac{1}{n} FFT(A,\omega_n^{n-1})$</li>
</ul>
<p>So what is all this gibberish saying? Inverse FFT is FFT computed in the reverse order. It's like an inverted clock</p>
<p>Consider: For an even b, what is $1+\omega_n+\omega_n^2+\cdots+\omega_n^{n-1}$<br/>
There's two ways to think about it.</p>
<p>The first is geometrically, recall that each successive exponent simly advances the point along a unit circle. Since we have n points we have effectively traversed the circle and ended where we started 0. which is $\omega_n^0 = 1$</p>
<p>The other approach is to apply the $\pm$ property. Recall that for each j $\omega_n^j = - \omega_n^{(n/2)+j}$</p>
<p>We can actually generalize this even further<br/>
<strong>Claim:</strong> for any n'th root of unity $\omega$ where $\omega \ne 1$, $0=1+\omega_n+\omega_n^2+\cdots+\omega_n^{n-1}$</p>
<p><img src="CS6515_images/FFT-008.png" width="400;"/></p>
<p>Now let's go through a proof of the lemma.<br/>
Recall that we need to show $M_n(\omega_n)^{-1} = \frac{1}{n} M_n(\omega_n^{-1})$</p>
<p>If true then it would mean that $\frac{1}{n} M_n(\omega_n^{-1}) M_n(\omega_n) = I$<br/>
where I is the identity matrix (1's along the diagonal and 0's otherwise)</p>
<p>So we will demonstrate that $M_n(\omega_n^{-1}) M_n(\omega_n)$ results in a matrix with n's along the diagonal and is 0 in all other entries.</p>
<p>for entries (k,k), we will work out the dot product of the k'th row and k'th column 
<img src="CS6515_images/FFT-009.png" width="400;"/></p>
<p>for entries (k,j), where $k \ne j$, again we will work out the dot product of the k'th row and j'th column 
<img src="CS6515_images/FFT-010.png" width="400;"/></p>
<p>Finally we can return to our polynomial multiplication algorithm 
<img src="CS6515_images/FFT-011.png" width="400;"/></p>
<p>Finally the pain is over!!</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ex-1">Ex 1<a class="anchor-link" href="#Ex-1">¶</a></h3>
<pre><code>Problem 1 (Integer multiplication using FFT)
(a) Given an n−bit integer number a = a[n−1]a[n−2] . . . a0 define a polynomial A(x) satisfying A(2) = a.

(b) Given two n−bit integers a and b, give an algorithm to multiply them in O(n log(n)) time. Use
the FFT algorithm from class as a black-box (i.e. don’t rewrite the code, just say run FFT on
...). Explain your algorithm in words and analyze its running time. (Think/investigate why
this is not enough to claim a fast multiplication algorithm).</code></pre>
<p><strong>Part a</strong><br/>
Recall that a polynomial is simply an equation of the form $\large \sum_0^{n-1} a_i x^i$. So this is done by the definition</p>
<p><strong>Part b</strong><br/>
Obviously we will want to make use of the FFT algo</p>
<p><strong>Step 1</strong><br/>
Formulate the n-bit integer inputs as per the definition in part a</p>
<p><strong>Step 2</strong></p>
<ol>
<li>Choose an N for the roots of unity, </li>
</ol>
<ul>
<li>N must be a power of 2 </li>
<li>and $N \ge 2n$ where n is the number of bits</li>
</ul>
<ol>
<li>define A = (a,padding to N)<ul>
<li>ie $a_0,a_1,\cdots,a_{n-1},0,0,...$</li>
</ul>
</li>
<li>do the same for B</li>
</ol>
<p><strong>Step 3</strong><br/>
Run the FFT using $\omega_n$</p>
<ul>
<li>A' = FFT(A,$\omega_n$)</li>
<li>B' = FFT(B,$\omega_n$)</li>
</ul>
<p><strong>Step 4</strong><br/>
Multiply A' and B' to get C'</p>
<ul>
<li>C' = A' x B'</li>
<li>C[i] = A[i] x B[i] for $1 \le i \le N$</li>
</ul>
<p><strong>Step 5</strong><br/>
Run FFT on inverse roots of unity and apply scaling</p>
<ul>
<li>C = FFT(C',$\omega_n^{-1}$) * 1/N</li>
</ul>
<p><strong>Step 6</strong></p>
<ul>
<li>Compute C(2) to get back binary integer</li>
</ul>
<p><strong>Correctness</strong><br/>
By converting the binary numbers we can make use of the FFT algorithm to multiply them.</p>
<p><strong>Runtime</strong></p>
<ol>
<li>Takes T(n)</li>
<li>Takes T(2n)</li>
<li>Takes T(2n log n) recall that FFT is O(n log n)</li>
<li>T(n)</li>
<li>O(n log n)</li>
<li>T(n)</li>
</ol>
<p>Dominant term is O(n log n)</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ex-2">Ex 2<a class="anchor-link" href="#Ex-2">¶</a></h3><p>Consider $A=x+1$ and $B=x^2 + 1$, Find $C=A \times B$</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="FFT-Summary">FFT Summary<a class="anchor-link" href="#FFT-Summary">¶</a></h3><ol>
<li><p>FFT is a transformation algorithm</p>
<ul>
<li>it converts from coefficients to values, when $\omega_{2n}$ is used</li>
<li>it converts from values to coefficients, when $\omega_{2n}^{-1}$ is used (but don't forget to scale! by 1/2n )</li>
</ul>
</li>
<li><p>FFT requires input vector of length $n=2^k$</p>
<ul>
<li>may use padding to ensure this is always true</li>
</ul>
</li>
<li><p>FFT results are with respect to the roots of unity</p>
<ul>
<li>it's return values must always undergo some sort of transformation back into the real number system</li>
<li>this is generally done using the inverse fft </li>
</ul>
</li>
<li><p>Inverse FFT is just FFT evaluated at the inverse roots of unity $\omega_{2n}^{-1}$</p>
<ul>
<li>$\omega_{2n}^{-1}$ go clockwise</li>
<li>$\omega_{2n}^{-1}$ are the mirrored/flipped values of $\omega_{2n}$</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="DC6:-Summary">DC6: Summary<a class="anchor-link" href="#DC6:-Summary">¶</a></h2><p>Approach</p>
<ul>
<li>Break problem into several parts or subproblems</li>
<li>solve each one independently </li>
<li>combine solutions to subproblems into an overall solution</li>
</ul>
<p>Classic DnC Algorithms</p>
<ul>
<li>Binary search: O(log n)</li>
<li>Merge Sort O(n log n)</li>
<li>Finding the median O(n)</li>
<li>FFT O(n log n)</li>
</ul>
<p>DnC solution have three main components</p>
<ol>
<li>Description    </li>
</ol>
<ul>
<li>No pseudo code!!</li>
<li>describe the base case </li>
<li>describe the boundaries</li>
</ul>
<ol>
<li>Correctness</li>
</ol>
<ul>
<li>How do you know the description is correct?</li>
<li>again you must describe this</li>
</ul>
<ol>
<li>Analysis</li>
</ol>
<ul>
<li>Calculate the recurrance</li>
<li>Use the master formula as needed</li>
<li>if a black box algo is used take it for granted<ul>
<li>this is when you use a well known algo like those above </li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="RA0:-Randomized-Algorithms">RA0: Randomized Algorithms<a class="anchor-link" href="#RA0:-Randomized-Algorithms">¶</a></h2><p>Let's now dive into randomized algorithms. Hopefully at the end, you'll appreciate how randomness is a beautiful and powerful algorithmic tool. We'll start with cryptography. We'll look at the amazing RSA Cryptosystem. This is one of the most widely used Cryptosystems. It's extremely elegant.</p>
<p>Once we learn some of the basic mathematics about modular arithmetic, you'll appreciate the ingenuity of the RSA protocol. Then you'll have a basic understanding of how these Cryptosystem that we use everyday, many times a day in fact, actually work. Another beautiful and incredibly useful application of randomized algorithms that we'll study is for hashing. We're going to look at a hashing scheme known as Bloom Filters. This is a simple scheme that's quite popular in many different fields. We'll look at the mathematics behind it. This will involve some basic probability analysis and then you'll do a programming project to implement and study Bloom Filters.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="RA1:-Modular-Arithmetic">RA1: Modular Arithmetic<a class="anchor-link" href="#RA1:-Modular-Arithmetic">¶</a></h2><p>The mathematics of the RSA cryptosystem are very beautiful and they're fairly simple once you have the right mathematical background. So we're going to start with a short primer on the mathematical topics that we need.</p>
<p>The relevant topic that we need is Modular Arithmetic. Then one key concept that we need are multiplicative inverses, and what they mean in modular arithmetic. And then we're gonna look at Euclid's GCD algorithm, greatest common divisor, which is gonna be used in the RSA cryptosystem. Next we're going to learn about Fermat's Little Theorem and it's the key tool in the design of the RSA algorithm. So at this point we'll be able to detail the RSA algorithm. Finally, we'll look at Primality testing, given a number, can we test whether the number is prime or not, or composite?</p>
<p>Once we see how to do that using Fermat's Little Theorem, then we're going to be able to generate random primes. Generating random primes is a key component in the RSA algorithm. So once we complete that, we'll have completed our discussion of the RSA algorithm. Now let's go ahead and dive into the algorithms related to the RSA.</p>
<p>Lets look at the context of the RSA algorithm. In cryptography, we're going to work with N-bit numbers X, Y and N. For example, the number of bits in these numbers is going to be huge. Think of the number of bits as 1024 or 2048.
It's a huge number of bits. So normally, we thought of our arithmetic operations as built into hardware, but that usually restricts the attention to 64 bits or so.  But here we're talking about 1000 or 2000 bits. So we have to go back and review how long does it take for basic arithmetic operations.</p>
<p>Now let's take a look at modular arithmetic. This is the basic mathematics that underlies the RSA algorithm and we're going to see a lot of beautiful mathematics along the way.</p>
<p>Let's look at a simple example,</p>
<ul>
<li>for an integer x, x mod 2 = least significant bit of x </li>
<li>What does the least significant bit of X tell you?<ul>
<li>It tells you whether X is odd or even.</li>
<li>x mod 2 = 1 if X is odd </li>
<li>x mod 2 = 0 if X is even</li>
</ul>
</li>
</ul>
<p>Another way of looking at this least significant bit of X, we can take X divided by two. If it's even then the remainder is going to be zero because it's a multiple of two, and if it's odd, then the remainder when X divided by two is going to be one.</p>
<p>Now let's look at X mod N where N &gt;= 1.</p>
<ul>
<li>Recall x mod 2 is the remainder when x is divided by two</li>
<li>So X mod N is going to be the remainder when we divide X by N</li>
</ul>
<p>Finally let's look at some important notation for modular arithmetic.</p>
<p>Suppose we have two numbers X and Y,
 and we look at these two numbers modular N,
 and suppose that they're same mod N,
 how do we denote that? Well, they're not equal.
 The two numbers are not equal, but they're congruent in this,
 or they're equivalent in this world modular N.</p>
<p>So how do we denote that equivalence or congruence? So the standard indication is three lines instead of two lines.</p>
<ul>
<li>$x \equiv y \mod N$</li>
<li>This notation means that X and Y are congruent modular N, </li>
<li>which means that X/N and Y/N have the same remainder</li>
</ul>
<p><strong>Example</strong></p>
<p>mod 3 has 3 equivalence classes:</p>
<pre><code>0, 3, 6, 9, etc etc but we can also use negatives -3, -6, -9 
1, 4, 7,10, etc etc but also -2,-5,-8
2, 5, 8,11, etc etc but also -1,-4,-7</code></pre>
<p><strong>Handy Fact</strong><br/>
if $x \equiv y \mod N$ &amp; $a \equiv b \mod N$ then $x+a \equiv y+b \mod N$<br/>
Similarly $(xa) \equiv (yb) \mod N$</p>
<p>Example: $321 \times 17 \equiv 1 \times 17 \equiv 17 \mod 320$</p>
<p><strong>Modular Exponentiation</strong><br/>
Given n-bit numbers x,y,N, our goal is to compute $x^y \mod N$ ( Assume that these number are very large, HUGE. )</p>
<p>Naive/Brute force approach<br/>
Compute</p>
<ul>
<li>$x \mod N = a_1$</li>
<li>$x^2 \equiv a_1 x \mod N = a_2$</li>
<li>$x^3 \equiv a_2 x \mod N = a_3$</li>
<li>$\cdots$</li>
<li>$x^y \equiv a_{y-1} x \mod N = a_n$</li>
</ul>
<p>Each of these lines is roughly O(n^2), O(n) to multiply a &amp; x and O(n) to take the mod N. The number of lines or rounds is roughly $y \le 2^n$. For a total run time of roughly $O(n^2 2^n)$</p>
<p>Consider $7^5 mod 23$?</p>
<ul>
<li>$7^2 \equiv 3 \mod 23$</li>
<li>$7^3 \equiv 3(7) = 21 \mod 23$</li>
<li>$7^4 \equiv 21(7) = 9 \mod 23$</li>
<li>$7^5 \equiv 9(7) = 17 \mod 23$</li>
</ul>
<p>Although this is a bit better it's less than ideal. Here's a third approach that is a bit fast</p>
<p><strong>Repeated Squaring Method</strong></p>
<p>Compute</p>
<ul>
<li>$x \mod N = a_1$</li>
<li>$x^2 \equiv a_1^2 \mod N = a_2$</li>
<li>$x^4 \equiv a_2^2 \mod N = a_4$</li>
<li>$\cdots$</li>
<li>$x^y \equiv a_{y-1} x \mod N = a_n$</li>
</ul>
<p>Consider our example from before $7^{25} mod 23$</p>
<ul>
<li>$7^2 \equiv 3 \mod 23$</li>
<li>$7^4 \equiv 3^2 = 9 \mod 23$</li>
<li>$7^8 \equiv 9^2 = 81 \equiv 12 \mod 23$</li>
<li>$7^{16} \equiv 12^2 \equiv 144 \equiv 6 \mod 23$</li>
<li>25 in binary is 11001</li>
</ul>
<p>Thus</p>
<ul>
<li>$7^{25} \equiv 7^{16} \times 7^8 \times 7 \equiv 6 \times 12 \times 7 \equiv 72 \times 7 \equiv 3 \times 7 \equiv 21 \mod 23$</li>
<li>Final answer 21</li>
</ul>
<p><strong>$\color{red}{\text{ModExp Algorithm}}$</strong></p>
<p>First we state a key observation which should be trivial</p>
<ul>
<li>for even y, $x^y = (x^{y/2})^2$ </li>
<li>for odd y, $x^y = x (x^{\lfloor y/2 \rfloor})^2$ ( the leading x is to make up for the term that is dropped by the floor )</li>
</ul>
<pre><code>ModExp(x,y,N):
Input : n-bit integers x,y,N &gt; 0
Out   : x^y mod N

if y=0, return 1
z=ModExp(x, floor(y/2),N)

if y even 
    then return (z^2 mod N)
    else return (xz^2 mod N)</code></pre>
<p><strong>Multiplicative Inverses</strong></p>
<p>What's the inverse of (1/z) mod N ?<br/>
Definition<br/>
x is the multiplicative inverse of z mod N</p>
<ul>
<li>if $xz \equiv 1 mod N$</li>
</ul>
<p>of course if follows that</p>
<ul>
<li>$x \equiv z^{-1} mod N$</li>
<li>$z \equiv x^{-1} mod N$</li>
</ul>
<p>Consider, for N=14</p>
<ul>
<li>x=1: then $1^{-1} \equiv 1 mod N$</li>
<li>x=2: then $2^{-1} \equiv ? mod N$ doesn't exist</li>
<li>x=3: then $3^{-1} \equiv 5 mod N$</li>
<li>x=4: then $4^{-1} \equiv ? mod N$ doesn't exist</li>
<li>x=5: same as above for 3</li>
<li>x=6,7,8 don't exist </li>
<li>x=9: then $11^{-1} \equiv 11 mod N$</li>
<li>x=13: then $13^{-1} \equiv 13 mod N$</li>
</ul>
<p>You may or may not have picked up on a pattern here. It turns out when there is a common divisor then there will not be an inverse. Otherwise the inverse exists.</p>
<p><strong>Theorem</strong> $x^{-1} mod N$ exists iff gcd(x,N) = 1, if they have a gcd &gt; 1 then they must have an inverse</p>
<ul>
<li>This, gcd(x,N) = 1, is often termed relatively prime               </li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Suppose $x^{-1} \mod N$ exists, then how many can there be?</p>
<p>For ex consider x=3,N=11 then $3^{-1} \equiv 4 \mod N$. How many inverses can there be?</p>
<p>We will prove there can be only one, using contradition. Suppose there are two inverses, ie $z \equiv x^{-1} \mod N$ and $y \equiv x^{-1} \mod N$ and that $z \not\equiv y \mod N$, which also means $1 \le y\neq z \le N$. Since z is an inverse of x mod N then we have that $xz \equiv 1 \mod N$ similarly $xy \equiv 1 \mod N$. But then we can equate the two to get $xz \equiv xy \equiv 1 \mod N$. so $xz \equiv xy$. Now multiplying each side by $x^{-1}$ yields $z \equiv y$. Which contradicts our assumption at the start $z \not\equiv y \mod N$. I'll spare you the QED, but that's all there is to it.</p>
<p><strong>NonExistence of Multiplicative Inverses</strong><br/>
Let's now consider the case when the inverse does not exist. Recall our earlier theorem that $x^{-1} mod N$ exists iff gcd(x,N) = 1, meaning they are relatively prime. Of course this means that when gcd(x,N) &gt; 1 then the inverse must not exist.</p>
<p>To build some intuition of the upcoming proof, let's consider a simple case where x &amp; N are even numbers. furthermore, let $z \equiv x^{-1} \mod N$ and thus $xz \equiv 1 mod N$. This would imply that xz = N+1, or 2N+1 or 3N+1 or ... etc etc ... qN+1. But xz must be an even number. whereas qN+1 must be an odd number no matter the choice of q. Hence we have contradiction our initial assumption that x &amp; N are even numbers.</p>
<p>We will now proceed to prove the existence of $x^{-1} \mod N$ by finding it.</p>
<h3 id="Euclid's-Rule">Euclid's Rule<a class="anchor-link" href="#Euclid's-Rule">¶</a></h3><p>Consider the integers x &amp; y with $x \ge y \gt 0$, then gcd(x,y) = gcd(x mod y, y). Let's consider why this must be true. Note that gcd(x,y)=gcd(x-yk,y) for some integer k is equivalent to our earlier statement gcd(x mod y, y). Since the mod function acts like a repeated subtraction. It will now suffice to demonstate the correctness of our second statement. To do this we will search for a divisor common to x,y, &amp; x-y.</p>
<ul>
<li>suppose d divides x &amp; y on the left, then d divides y on the right and x-y as it divides each term</li>
<li>suppose d divides x-y &amp; y on the right, then it follows that d must divide both x and y</li>
<li>thus one these common divisors must be the greatest</li>
</ul>
<p>We will now use this to develop an algorithm<br/>
<strong>$\color{red}{\text{Euclids Algo}}$</strong></p>
<pre><code>Euclid(x,y)

Input  : Integers x,y where x &gt;= y &gt;= 0
Output : gcd(x,y)

if y=0 return x
else return (Euclid(y,x mod y))    
                      # this is not a typo
                      # we flip the parameters to ensure x &gt;= y &gt;= 0</code></pre>
<p>The base case may look a bit weird. Let's consider the base case gcd(x,0) which we got to by taking some multiple of x with x, ie gcd(kx,x). kx mod x is of course 0 and that is passed to the Euclid call. Furthermore, the gcd(kx,x) is x.</p>
<p><strong>Runtime</strong><br/>
Taking the mod of an n-bit number requires $O(n^2)$. How many calls will there be? In general if $x \ge y$ then $x \mod y \lt x/2$. This means our algo will run for at most 2n rounds.</p>
<p>Our final runtime will t/f be $O(n^3)$</p>
<h3 id="Extended-Euclid">Extended Euclid<a class="anchor-link" href="#Extended-Euclid">¶</a></h3><p><strong>$\color{red}{\text{Extended Euclid Algo}}$</strong></p>
<p><strong>Computing Inverses</strong><br/>
This is an extended version of euclid's algorithm. This version however will return and extra two parameters $\alpha$ and $\beta$.</p>
<p>Consider</p>
<ul>
<li>if gcd(x,N) =1 then $x^{-1} \mod N$ exists</li>
<li>t/f $d = 1 = x\alpha + N\beta$</li>
<li>t/f $1 \equiv x\alpha + N\beta \mod N$<ul>
<li>but $N\beta \mod N$ is just 0 so the last term is 0</li>
</ul>
</li>
<li>t/f $1 \equiv x\alpha \mod N$<ul>
<li>which means $\alpha = x^{-1}$</li>
</ul>
</li>
<li>thus $x^{-1} \equiv \alpha \mod N$</li>
</ul>
<p>Extended algorithm</p>
<pre><code>extEuclid(x,y)

 input: integers x,y   where x &gt;= y &gt;= 0
output: integers d,alpha,beta 
    where d = gcd(x,y)
    and d = x*alpha and y*beta



Input  : Integers x,y where x &gt;= y &gt;= 0
Output : gcd(x,y)

if y=0 return(x,1,0)

d,a',b' = extEuclid(y,x mod y)

return(d,b',a'-b'*floor(x/y))</code></pre>
<p>Runtime is $O(n^2)$ per round, with up to 2n rounds similar to before. Hence the runtime is $O(n^3)$.</p>
<p><strong>Example</strong></p>
<p>Q: Compute $7^{-1} \mod 360$
A: Run Ext−Euclid(360,7)</p>
<ul>
<li><p>Let's first look at (x,y)(x,y) in the recursive subproblems.</p>
<ul>
<li>They are (360,7) → (7,3) → (3,1) → (1,0)</li>
<li>(Note, these are the same sequence of subproblems as encountered in Euclid's GCD algorithm.)</li>
</ul>
</li>
<li><p>Now let's look at the pair (α,β) returned for these inputs.</p>
<ul>
<li>For the base case (x,y) = (1,0) we have: (α,β) = (1,0)</li>
<li>Next, for (x,y) = (3,1) we have: (α,β) = (0,1)</li>
<li>Then for (x,y) = (7,3) we have: (α,β) = (1,−2)</li>
<li>Finally, for (x,y) = (360,7) we have: (α,β) = (−2,103)</li>
</ul>
</li>
<li><p>Therefore, $7^{−1} ≡ 103 \mod 360$</p>
</li>
<li>Note, we also get that: <ul>
<li>$360^{−1} \equiv  −2 \mod 7$</li>
</ul>
</li>
<li>Simplifying, we have: <ul>
<li>$360 \equiv 3 \mod 7$ and $−2 \equiv 5$</li>
</ul>
</li>
<li>Therefore, <ul>
<li>$3^{-1} \equiv 5 \mod 7$ </li>
</ul>
</li>
<li><strong>Finally we see</strong> <ul>
<li>Answer: 103</li>
</ul>
</li>
</ul>
<p><strong>Summary</strong></p>
<ul>
<li>Use Euclid to find gcd(x,N)</li>
<li>Use extEuclid to compute $x^{-1} \mod N$</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="RA2:-RSA-Cryptosystem">RA2: RSA Cryptosystem<a class="anchor-link" href="#RA2:-RSA-Cryptosystem">¶</a></h2><p>Before we dive into the RSA cryptosystem, we're going to dive into the mathematical concept which is the basis for the whole cryptosystem.</p>
<h3 id="Fermat's-little-theorem">Fermat's little theorem<a class="anchor-link" href="#Fermat's-little-theorem">¶</a></h3><p>If  p is prime then for every $1 \le z \le p$ we have $z^{p-1} \equiv 1 \mod p$</p>
<ul>
<li>Note that $1 \le z \le p-1$ is equivalent being relatively prime ie gcd(z,p) = 1</li>
</ul>
<p>This is rather easy result to conceptualize. p is prime, means it has no factors. so the only common factors are 1 and p. Another way to think of this is a small example. Let z=2 and say p=7 which is prime. then $z^{p-1}=2^6=64$ mod this by 7 to get 1.</p>
<p>This is the basis of the RSA algorithm, as well as the basis of a primality testing algorithm. If given a number x how can we quickly determine if it's prime. We will go through the proof of fermat's little theorem. Which also makes some interesting observations.</p>
<p><strong>Proof: Fermat's little theorem</strong><br/>
Let's begin by defining a set S={1,2,3,...,p-1} which is the elements between 1, and p-1.<br/>
Let's also look at another set S' which is z*S mod p, ie S'={1*z mod p, 2*z mod p,...,(p-1)*z mod p}</p>
<p>Ex for p=7, z=4, then S={1,2,3,4,5,6} and S'={4,1,5,2,6,3}<br/>
If you stare at S' long enough you may notice something rather amusing. It's a permutation of S. It's the exact same elements with a different ordering. So in fact S'=S. This is an example and not a rigorous proof, but since we all love proofs so much let's not skip this one.</p>
<p>S=S'<br/>
<strong>Proof</strong><br/>
Let S={1,2,...,p-1} and S'={1*z mod p,2*z mod p,...,(p-1)*z mod p}. We will show that the elements of S' are distinct, non-zero, and that there are p-1 of them, |S'|=(p-1).</p>
<p>To see that there are (p-1) elements is straight forward as each element in S has a corresponding mapping to S'. To see that they are distinct suppose that for some i,j where $i \ne j$, the elements in S' are the same. This means that $iz \equiv jz mod p$. So we multiply each side by the inverse of z, $z^{-1}$. This yields $i \equiv j \mod p$ which contradicts our assumption that i and j are not equal. How to we know $z^{-1}$ exists? well p is prime so therefore any number between 1 and p-1 must be relatively prime to it. And of course z must be a number between 1 and p so it must be in S, and it must have a prime.</p>
<p>All that is left to demonstrate that they are all non-zero. Again we proceed by contradition. Assume that there is a zero. then for some i, $iz \equiv 0 \mod p$, again we multiply each side by $z^{-1}$. This yields $i \equiv 0 \mod p$. Consider what this means? since i is the index of some element in S, i=0 would mean the 0'th index. But the set begins at 1. Hence a contradiction.</p>
<p>Now back to our originally scheduled proof (Fermat's little theorem)</p>
<p>Recall that we need to show that $z^{p-1} \equiv 1 \mod p$. Knowing that S=S' will help us to demonstrate this. Now consider multiplying the elements of S together ie $1 \times 2 \times 3 \times \cdots \times (p-1)$. Similarly for S' we get ie $1 \times z \times 2 \times z \times 3 \times z \times \cdots \times z \times (p-1) \mod p$. Let's see if these are equal! Multiplying  the elements of S together yields (p-1)!, Multiplying the elements of S' together yields, $(p-1)! z^{p-1} \; mod \; p$. Now we need to cancel out the (p-1)!, if your guessing we need an inverse you'd be correct. But recall that p is prime, therefore every number between 1 and p-1 must have an inverse, so the inverse of p-1 is just the factorial of the inverses $1^{-1} \times 2^{-1} \times 3^{-1} \times \cdots \times (p-1)^{-1}$. Now mutiply each side by $(p-1)!^{-1}$ and you'll find that $1 \equiv z^{p-1} \; mod \; p$ .... well well well this is exactly what we wanted to prove is it not?</p>
<h3 id="Euler's-Theorem">Euler's Theorem<a class="anchor-link" href="#Euler's-Theorem">¶</a></h3><p>Is a more general form of fermat's theorem.</p>
<p>For any N,z where gcd(z,N) = 1 then $z^{\phi(N)} \equiv 1 \mod N$<br/>
Where we define $\phi(N)$, Euler's Totient function, as</p>
<ul>
<li>all integers between 1 &amp; N that are relatively prime to N.</li>
<li>equivalently it is the cardinality |{$x : 1 \le x \le N, \; gcd(x,N)=1 $}|</li>
</ul>
<p>Consider what {$x : 1 \le x \le N, \; gcd(x,N)=1 $} means. if N=p prime then by fermat's theorem we could say that all numbers between 1 and p-1 and relatively prime to p. Thus for p prime $\phi(p)=p-1$, pretty simple right? Furthermore if $\phi(p)=p-1$ is virtually the same statement as fermat.</p>
<p>What if N=pq, for primes p and q, then what is $\phi(N)$? Well, well, well, lemme tell ya, it's just (p-1)(q-1).</p>
<p>Conceptually</p>
<ul>
<li>for the numbers 1,2,3, ... pq<ul>
<li>we have p, 2p, 3p, ... qp: q multiples of p </li>
<li>sim we have q, 2q, 3q, ... , pq: p multiples of q </li>
<li>this is pq - q - p + 1 (we add back one because the last term in each is the same)</li>
<li>this simplifies to (p-1)(q-1)</li>
</ul>
</li>
</ul>
<p>if we put this result back into Euler's theorem we get the basis of the RSA algorithm<br/>
$$\large Z^{(p-1)(q-1)} \equiv 1 \mod pq$$</p>
<p>Which allows us to use (p-1) &amp; (q-1) to generate an encryption and decryption key. Let's use an example to build our intuition of RSA.</p>
<p>Suppose we take b,c where $bc \equiv 1 \mod p-1$, meaning b &amp; c are inverses to each other in modulo p-1, or more simply bc = 1 + k(p-1) for some integer k.</p>
<p>Consider $z^{bc}$, we can work this out as follows $z^{bc} \equiv z*((z^{p-1})^k \equiv z*1 \mod p$ .... pretty interesting right? we got back the number z despite raising it to the power of bc. This is exactly what the RSA uses. take a message and encode it to form your z, then raise it to the power of b, then it will take someone with the power of q to raise yet again. The result will be the original message z. There is still one small weakness we want to get rid of. In the above formulation p is known, so if they know p then they certainly know p-1. We will move to conceal p by using Euler's theorem.</p>
<p>To use Euler's theorem, we choose primes p &amp; q, and let N=pq. Find d,e that are inverses to each other in mod (p-1)(q-1). ie $de \equiv 1 \mod (p-1)(q-1)$. Then $z^{de} \equiv z \times (z^{(p-1)(q-1)})^k \equiv z \mod N$. Now we can send a message encrpted with e along with N. Only the receiver who knows d and thus q will be able to decode it back to z. Even if someone knows e and N they still be unable to decode without d.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="RSA:-Cryptography">RSA: Cryptography<a class="anchor-link" href="#RSA:-Cryptography">¶</a></h3><p><strong>Setting</strong><br/>
There are 3 people: Alice the sender, Bob the reciever, and Eve the aptly named evesdropper. Alice takes her message m and encrypts then sends it, e(m) is the message she sends. Bob receives e(m) and decrypts it using his decryption function d(), so he gets the message m = d(e(m)). This is what is often called a public-key cryptosystem. Everyone can see e(m). Furthermore Bob will give everyone the keys to send him a message. He publishes the private key N and e. Now anyone can use these to encrypt their messages. What bob keeps safeguarded is his private key d, which is the inverse of e in modulo (p-1)(q-1)</p>
<p>Here's a sketch of the process</p>
<ol>
<li><strong>RSA Protocol - Reciever - Bob</strong> </li>
</ol>
<ul>
<li>Bob needs to pick 2 n-bit random primes p &amp; q<ul>
<li>to do this generate an n-bit sequence of random 0s and 1s</li>
<li>then test it to ensure it's prime (primality test we will look at soon)</li>
</ul>
</li>
<li>Bob choose e relatively prime to (p-1)(q-1)<ul>
<li>Generally these are small so that others can encrypt easily, ie e=3,5,7, or your favourite prime number</li>
<li>he also needs to ensure this e is relatively prime so he runs Euclid's extended algorithm</li>
</ul>
</li>
<li>Now Bob has N = pq    </li>
<li>Now bob publishes his public key N &amp; e</li>
<li>and can compute his private key <ul>
<li>which is the d that satifies $d \equiv e^{-1} \mod (p-1)(q-1)$</li>
</ul>
</li>
</ul>
<ol>
<li><strong>RSA Protocol - Sender - Alice</strong> </li>
</ol>
<ul>
<li>Looks up/locates Bob public key (N,e)</li>
<li>She encodes her message $y = m^e \mod N$<ul>
<li>If she took this course she would use the fast modular exponentiation algorithm</li>
</ul>
</li>
<li>finally she sends her message y</li>
</ul>
<ol>
<li><strong>RSA Protocol - Reciever - Bob</strong></li>
</ol>
<ul>
<li>Bob recieves the message y and gets to work</li>
<li>Decrypts: computes $y^d \mod N = m$</li>
</ul>
<p>Recall: for $N = pq$</p>
<ul>
<li>$d \equiv e^{-1} \mod (p-1)(q-1)$<ul>
<li>$\implies de = 1 + k(p-1)(q-1)$</li>
</ul>
</li>
<li>$y^d \equiv (m^e)^d  $<ul>
<li>$\equiv m^{ed}$</li>
<li>$\equiv m \times (m^{(p-1)(q-1)})^k = m \times 1$</li>
<li>$\equiv m \mod N$</li>
</ul>
</li>
</ul>
<p>This is the whole RSA random algorithm. What remains is to look at how to create and test random prime numbers. but first let's look at some possible pitfalls.</p>
<p><strong>Issue 1</strong><br/>
Suppose m and N are not relatively prime, ie gcd(m,N) &gt; 1, where N=pq, p &amp; q prime. This would imply that the gcd of m and N is either p or q so let's suppose it's p, thus gcd(m,N) = p. Let's think of how this will affect the RSA algorithm. According to our RSA algo we would expect $(m^e)^d \equiv m \mod N$ by the Chinese Remainder Theorem (CRT) this will hold true for gcd(m,N) &gt; 1, (where as Euler's theorem expects gcd(m,N)=1). But there is a problem here that is not so obvious. if we look at the encrypted message $y = m^e \mod N$, you may observe that all of these are divisable by p. Since gcd(m,N) = p, then it follows that gcd(y,N) = p also. Thus the encrypted message is breakable. The moral of the story is that finding two primes is not always sufficient, if they're not relatively prime your RSA implementation can break down</p>
<p><strong>Issue 2</strong><br/>
m shouldn't be too large, in particular we want m &lt; N. In general messages can be a large string text, so we first convert it into a number, this can be done by</p>
<ul>
<li>take the binary representation of the text, this will result in an even larger message length, </li>
<li>in this scenario we will want $m &lt; 2^n$, which is segments of length $2^n$ this is little n</li>
<li>where we constrain n to we find p and q such that $N \ge 2^n$</li>
<li>which put together implies that m &lt; N as needed</li>
</ul>
<p>m also shouldn't be too small. Suppose for example if we take e=3 and $m^3 &lt; N$</p>
<ul>
<li>then $m^e \mod N$ is just $m^e$. </li>
<li>Being less than N means that modding it won't have any effect</li>
<li>meaning the encypted message is just $y^3$</li>
<li>which can be easily decrypted by taking the cubed root $y^{1/3}$</li>
</ul>
<p>One way to avoid the second issue is to choose some random number r, then look at m plus r, m+r, or look at m exclusive or r, $m \bigoplus r$. This effectively pads the message. If our padded message is sufficiently large, then we send both the padded message and r.</p>
<p>Suppose we want to send the same message m to 3 different people using the same e=3? Each person may have a different public key: Person 1 ($N_1,3$); Person 2 ($N_2,3$); Person 3 ($N_3,3$). When we encrypt the same message for each person we would end up with 3 encrypted messages, say $y_1,y_2,y_3$. It turns out that if someone possesses all three encrypted messages then they should be able to decrypt m from thing. This is a consequence of the Chinese remainder theorem and is left as an exercise.</p>
<p><strong>Summary</strong></p>
<p><strong>Fermat's Little Theorem</strong> For primes p &amp; z, with gcd(z,p)=1, $z^{p-1} \equiv 1 \mod p$<br/>
<strong>Euler's Theorem</strong> For primes p &amp; q, let N=pq. For z where gcd(z,N)=1 $z^{(p-1)(q-1)} \equiv 1 \mod p$<br/>
<strong>RSA Algorithm</strong></p>
<ul>
<li>Choose primes p &amp; q, and let N=pq                </li>
<li>Find e where gcd(e,(p-1)(q-1))=1                     </li>
<li>let $d \equiv e^{-1} \mod (p-1)(q-1)$ use Extended Euclid algo - <strong>KEEP PRIVATE</strong> </li>
<li>Publish public key (N,e)</li>
<li>Encrypt message m: $y \equiv m^e \mod N$</li>
<li>Decrypt y, $m \equiv y^d \mod N$, to get back the message m - use fast modular exponentiation</li>
</ul>
<p><strong>Example</strong></p>
<p>Consider p=13, q=31</p>
<p>What is the smallest e?</p>
<ul>
<li>This is basically asking for the smallest relatively prime number        </li>
<li>(p-1)(q-1) = 360, which is divisable by 2,3,5 it is not divisible by 7 so we choose 7 as the lowest relatively prime         </li>
</ul>
<p>What is the public key (N,e)?</p>
<ul>
<li>N=pq=403, and e=7 from last question</li>
<li>thus (N,e) = (403,7)</li>
</ul>
<p>What is d?<br/>
For this we use the extended euclidean algorithm</p>
<ul>
<li>(p-1)(q-1) = 360, and e = 7</li>
</ul>
<pre><code># to find d =e^-1 mod (p-1)(q-1)
# to compute this we need to find the inverse of e in mod 360
# ie solve x = e^-1 mod 360

# Begin by finding the gcd(360,7)
    360 = 7*(51) + 3                #1 
      7 = 3*(2)  + 1                #2 we've gotten to 1 so at least we know the gcd exists

# now the Extended portion of the algorithm
# using the lines above but working from bottom - upwards
    1 = 7 - 3*2                     # last line from gcd  
      = 7 - 2*(360 - 7*51)          # sub 2nd last line into last line
      = 7 + 2*7*51 - 2*360          # simplification steps
      = 7 + 2*7*51                  # mod out the 360 
      = 7 + 7*102  
      = 7*(1+102)
      = 7*103

t/f (7*103 = 1 mod 360) so 103 is the inverse of 7
t/f (d = 103 mod 360 = 103)</code></pre>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Primality-Testing">Primality Testing<a class="anchor-link" href="#Primality-Testing">¶</a></h3><p>We want to choose a random pair of prime numbers p &amp; q. let r be a random n-bit number. To do this generate a random string of integers equal to 0/1. This of course is not necasarily prime, it's just random number. So we will check it by running a primality test. If it is prime we are confident it is also a random prime. otherwise we repeat the process to generate another random number. The run time of this can be tricky as it is difficult to predict when a random number will be prime.</p>
<p><strong>Density of the Primes</strong> It turns out that prime numbers are dense. The probability of a random number being a prime number is $\approx \frac{1}{n}$</p>
<p><strong>Fermat's test</strong><br/>
Recall: Fermat's theorem: If r is prime then for all z in {1,2,...,r-1} we have that $z^{r-1} \equiv 1 \mod r$. This will form the basis of our primality test. But first let's also consider the non-prime numbers, ie composite, that we are bound to run into. These are simply the numbers which are not relatively prime, ie  $z^{r-1} \not\equiv 1 \mod r$. In a humourous stroke we will call such a z, that proves r is composite, <strong>Fermat's witness</strong>. Does every composite r have a Fermat witness? Yes, in fact they do. They always have at least 1, and may even have more. These are much easier to find.</p>
<h3 id="Fermat-Witness">Fermat Witness<a class="anchor-link" href="#Fermat-Witness">¶</a></h3><p>Define the Fermat witness for r as the number z such that $1 \le z \le r-1$ and $z^{r-1} \not\equiv 1 \mod r$</p>
<p><strong>Proof: Every Composite r has at least 1 fermat witness</strong><br/>
Since r is composite then it must have at least 2 divisors. Let's choose one of them and call it z. Then we know that gcd(r,z)=z&gt;1. Of course this works for any divisor of r, so if it has 100 divisors then we would have 100 witnesses. You may also recall from our properties of inverses that if the gcd of two numbers, in mod n, is not 1, then the inverse does not exist. This is our current situation gcd(r,z)=z implies that the inverse does not exist.</p>
<p>Let's proceed by contradicition. Suppose there is an inverse, ie $z^{r-1} \equiv 1 \mod r$. This would imply that $z*z^{r-2} \equiv 1 \mod r$, which in turn implies that z has the inverse, $z^{r-2}$ in mod r. But the inverse cannot exist, therefore we have a contradiction.</p>
<p><strong>DONE</strong></p>
<p>Now we call this Fermat witness that we just derived a trivial Fermat witness. Why is it a trivial Fermat witness? This is a Fermat witness, z, where it also has the property that the gcd(z,r) &gt; 1. So z and r are not relatively prime. Notice, z already proves that r is composite. If z and r have a non-trivial divisor, then we know that r has a non-trivial divisor, and therefore, it's not prime. So, any z where this is the case, proves that r is composite, there's no reason to run Fermat's test. So that's why we called such a z, a trivial Fermat witness. Now there always exists a trivial Fermat witness for composite numbers. Why? Because every composite number has at least two non-trivial divisors. The question is whether a composite number r, has a non-trivial Fermat witness. This is a number z which is relatively prime to r. Now it turns out that some composite numbers have no non-trivial Fermat witnesses, these are called pseudo primes, but those are relatively rare.</p>
<p>For all the other composite numbers, they have at least one non-trivial Fermat witness, and if they have at least one, then in fact, they have many Fermat witnesses. And therefore it will be easy to find a Fermat witness and that's the key point. Trivial Fermat witnesses always exist. Every composite number has at least two trivial Fermat witnesses. But if a composite number has a non-trivial Fermat witness, then there are many Fermat witnesses, they're dense and therefore they're easy to find. And that's what we'll utilize for our primality testing algorithm.</p>
<p>Can there be no non-trivial witnesses? Yes, this can in fact happen. This happens when for some z $z^{r-1} \not\equiv 1 \mod r$ and gcd(z,r) = 1, ie they're relatively prime. We know there's always a trivial witness for a composite, but is there always a nontrivial witness? Such numbers are called <strong>Carmichael Numbers</strong> and resemble psuedo-primes in many ways.</p>
<p><strong>Define: Carmichael Numbers</strong><br/>
A Carmichael number is a composite number r, which has no non-trivial Fermat witnesses. Therefore, such a number is going to be inefficient to use Fermat's test, to prove that r is a composite number. Some Examples: 561, 1105, 1729. Thankfully they're pretty rare so we'll ignore them for the time being.</p>
<p>So if it has a non-trivial witness then how many does it have?<br/>
<strong>Lemma</strong> If r has $\ge 1$ non trivial fermat witnesses the at least 1/2 of the set of possible witnesses, ie the numbers z$\in$ {1,2,...,r-1}, are fermat witnesses. This will help us in designing our algorithm. The reader may also recall that these are the same numbers that must all be relatively prime for prime number p. So it makes that some are witnesses to a composite r.</p>
<p><strong>$\color{red}{\text{Primality Test - Simple}}$</strong></p>
<pre><code>Input: n-bit number r

1. Choose z randomly from {1,2,3,...,r-1}
2. Compute z^(r-1) \equiv 1 mod r
3. If z^(r-1) \equiv 1 mod r
    then r is prime     - output true
    else r is composite - output false</code></pre>
<ul>
<li>for prime r <ul>
<li>Pr(algo says r is prime) = 100%</li>
</ul>
</li>
<li>for composite r (&amp; not a Carmichael number)<ul>
<li>Pr(algo says r is prime) $\le$ 50%</li>
<li>false positive</li>
</ul>
</li>
</ul>
<p>This is a pretty high false positive rate so let's improve it. we will tweak our algo above as follows</p>
<ul>
<li>Instead of choosing just 1 z we will repeatedly choose z, k times at random, from the set {1,2,...,r-1}<ul>
<li>If any of these $Z_i$'s is a Fermat witness then we know for sure that r is composite</li>
<li>if none of them is a witness then there is a good chance r is prime.</li>
</ul>
</li>
<li>In step 2 we must now iterate over these z(s)<ul>
<li>and we perform the fermat test same as before</li>
</ul>
</li>
</ul>
<p><strong>$\color{red}{\text{Primality Test - Better}}$</strong></p>
<pre><code>Input: n-bit number r

1. Choose z1,z2,...,zk randomly from {1,2,3,...,r-1}

2.  for i=1-&gt;k
    Compute (z_i)^(r-1) \equiv 1 mod r

3. If for all i, z^(r-1) \equiv 1 mod r
    then r is prime     - output true
    else r is composite - output false</code></pre>
<ul>
<li>for prime r <ul>
<li>Pr(algo says r is prime) = 100% - same as before </li>
</ul>
</li>
<li>for composite r (&amp; not a Carmichael number) - false positive<ul>
<li>Pr(algo says r is prime) $\le$ (1/2)^k</li>
<li>as k grows larger this gets much better than before</li>
</ul>
</li>
</ul>
<p>How can we do this if we allow for Carmichael numbers?</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="RA3-Bloom-Filters---todo">RA3-Bloom Filters - todo<a class="anchor-link" href="#RA3-Bloom-Filters---todo">¶</a></h2><p>In this section we will discuss and illustrate a popular hashing technique known as bloom filters. To motivate our discussion we will look at a toy problem "Balls into bins" which will help build some intuition about the design of hashing schemes. Will consider the "Power of 2 choices", which is a very common technique used to build more sophisticated hashes. Then we will look at a classic hashing scheme called Chain hashing. Afterwards we will have the ability to look at bloom filters, which is the subject we have been driving at.</p>
<p><strong>Balls into Bins</strong><br/>
We have n identical balls and n bins $B_1,B_2,...,B_n$. Each ball is thrown into random bin, Independent of the other balls. We want to look at the number of balls that are assigned to each bin. t/f we look at the random variable load(i)</p>
<ul>
<li>Let load(i) = # of balls assigned to $B_i$</li>
<li>How can we determine the max load?<ul>
<li>where max load = $\underset{i}{max}$ load(i)</li>
<li>which is the maximum number of balls in any particular bin.</li>
</ul>
</li>
<li>In the worst case all n balls are assigned to just 1 bin<ul>
<li>1 ball assigned to $B_i$ is (1/n)</li>
<li>for all balls =&gt; Pr(<strong>All balls</strong> $\in B_i$) = $(1/n)^n$</li>
<li>for the first $log n$ balls Pr(<strong>balls(1,...,$log n$)</strong> $\in B_i$) = $(1/n)^{log n}$</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="GR0:-Graph-Algorithms">GR0: Graph Algorithms<a class="anchor-link" href="#GR0:-Graph-Algorithms">¶</a></h2><p><strong>Great Links and resources</strong></p>
<ul>
<li><a href="https://cs.slu.edu/~esposito/teaching/1080/webscience/graphs.html">https://cs.slu.edu/~esposito/teaching/1080/webscience/graphs.html</a></li>
</ul>
<p>Assume all graphs are simple there are no loop edges, (u,u)</p>
<h3 id="Terminology">Terminology<a class="anchor-link" href="#Terminology">¶</a></h3><p>Formally a graph is defined or specified as a set of vertices, aka Nodes, and a set of Edges that connect them. This is written mathematically as G={V,E}. One of the most characteristics of a graph is whether or not it is directed. Consider the following illustration.</p>
<p><img src="CS6515_images/GA1-001.png" width="400;"/></p>
<p>In these examples $V=\{ V_1, V_2, V_3 \}$. For the undirected graph on the left we would write $E = \{ (V_1,V_2), (V_1,V_3), (V_2,V_3)  \}$ as unordered pairs. But for the directed graph on the right we would write $E = \{ (V_1,V_2), (V_1,V_3), (V_2,V_3)  \}$, which appears the same but here these are ordered pairs and this quality must be remembered.</p>
<p>There are two standard methods for representing a graph:</p>
<ul>
<li>A: adjacency matrix</li>
<li>B: adjacency list</li>
</ul>
<p>Consider the following example
<img src="CS6515_images/GA1-002.png" width="400;"/></p>
<pre><code>Undirected
Adj List            Adj Mtx 
1 -&gt; 2 -&gt; 3         0  1  1  - think of the cols as the end pt and the rows as the start point
2 -&gt; 1 -&gt; 3         1  0  1  - the diag is 0 since it doesn't make sense for undirected graph node
3 -&gt; 2 -&gt; 1         1  1  0  - to have an edge to itself

Directed
Adj List            Adj Mtx 
1 -&gt; 2 -&gt; 3         0  1  1  - again we think of the cols as the end pt and the rows as the start pt
2 -&gt; 3              0  0  1  - the diag is 0 because there are no edges that loop back (but it can happen!)
3 -&gt; 2              0  1  0  - Loops: An edge with identical endpoints</code></pre>
<p>Some Handy terms and their definitions</p>
<ul>
<li>Two vertices are called adjacent if they share a common edge<ul>
<li>Undirected: all vertices are adjacent as they all share an edge</li>
<li>Directed: Here we must qualify the adjacency as the direction matters<ul>
<li>1 is adjacent <strong>to</strong> 2 &amp; 3, and both 2 &amp; 3 are adjacent <strong>from</strong> 1, but neither is adjacent <strong>to</strong> 1</li>
<li>2 &amp; 3 are adjacent as they are both adjacent <strong>from</strong> and <strong>to</strong> each other</li>
</ul>
</li>
</ul>
</li>
<li>the Neighborhood of a vertex v in a graph G is the set of vertices adjacent to v        </li>
<li>The degree of a vertex, deg(v), is the total number of vertices adjacent to the vertex   <ul>
<li>in short it's the number of connections a vertex has </li>
<li>Equivalently: the degree of a vertex as the cardinality of its neighborhood <ul>
<li>for any vertex v, deg(v) = |N(v)|.</li>
<li>where N is the neighborhood of a vertex v in a graph G </li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">¶</a></h3><p>We begin with a review of some common graphing algorithms, Breadth first search and depth first search. You may recall that we've already looked at Dijkstra's single source shortest path algorithm in the final section on dynamic programming. I will refer the reader to do a quick review of they are so inclined, see above DP2 3.0.</p>
<p>We'll then move to a quick review of how DFS is used to find the connected components of an undirected graph, and then we'll build on that to look at connectivity in directed graphs. We'll use DFS to find the strongly connected components of directed graphs. These are the analog of connected components in directed graphs. And then we'll see an application of our strongly connected component algorithm to solve the two set problem.</p>
<p>Next, we'll look at the minimum spanning tree problem, MSTs. You've likely seen that Kruskal's and Prim's algorithm before for finding an MST. We'll look at the correctness behind these algorithms.</p>
<p>Finally, we'll look at the PageRank algorithm. This is the algorithm that looks at the Web graph and assigns weights to vertices or webpages. It's a measure of their importance. This algorithm was devised by Brin and Page, and it's at the heart of Google's search engine. To understand the PageRank algorithm, We will need a quick primer on Markov chains, and then you'll see how it relates to strongly connected components.</p>
<p>Let's now do a quick review of some common graph algorithms that we will be building off of.</p>
<ul>
<li>DFS for depth-first search</li>
<li>BFS for breadth-first search</li>
</ul>
<p><img alt="dfs &amp; BFS Animation" src="CS6515_images/animated_DFS-BFS.gif" title="DFS v. BFS Algo Animated"/></p>
<h3 id="BFS:-Breadth-First-Search---Review">BFS: Breadth First Search - Review<a class="anchor-link" href="#BFS:-Breadth-First-Search---Review">¶</a></h3><p>This is probably one of the simplest algos for searching a graph and is used as a building block for many advanced graphs algorithms. It works on both directed and undirected graphs. BFS is named as such because it operates by expanding the frontier between discovered and undiscovered vertices uniformly across the breadth of the frontier.</p>
<p>Given a graph G=(V,E) and a distinguished source vertex s, bfs systematically explores the edges of G to discover every vertex reachable from s.  In a manner of speaking it computes the minimum distance from s to each reachable vertex v, this call also be thought of as the minimum number of edges needed to get from s to v, or just the path with the smallest number of edges.</p>
<p>In order to keep track of these discovered vertices it generally uses a single first-in first-out queue which contains some vertices at a distance k, k+1, etc etc from the source. In doing so BFS will color the vertices of the graph as white, black, and grey.</p>
<ul>
<li>white is the initial default color</li>
<li>grey is the assigned color to a vertex upon first discovery, it must of course be reachable to be discovered</li>
<li>black is the assigned color to vertices once the frontier of discovery is pushed forward</li>
<li>see gif below</li>
</ul>
<p><img alt="BFS Animation" src="CS6515_images/Animated_BFS.gif" title="BFS Algo Animated"/></p>
<p><strong>$\color{red}{\text{BFS Algorithm}}$</strong></p>
<ul>
<li>unweighted edges</li>
</ul>
<pre><code>Input  : G=(V,E) and s in V; s is the start point
Output : 2 arrays, dist &amp; prev
    dist(v_i) = min num of edges to get from s to v_i
    prev(v_i) = previous edge along the min path

for all v in V:
    dist(v)= min num of edges from s to v
    # using \infinity if not defined/unreachable
    prev(v)

Runs in O(n+m)</code></pre>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Dijkstra's-Algorithm---Review">Dijkstra's Algorithm - Review<a class="anchor-link" href="#Dijkstra's-Algorithm---Review">¶</a></h3><p>This is sometimes also known as the Single Source Shortest Path problem, or Just SSSP.</p>
<p>Helpful resources</p>
<ul>
<li><a href="https://www.cartagena99.com/recursos/alumnos/apuntes/dijkstra_algorithm.pdf">https://www.cartagena99.com/recursos/alumnos/apuntes/dijkstra_algorithm.pdf</a></li>
<li><a href="https://www.cs.cmu.edu/afs/cs/academic/class/15210-s15/www/lectures/shortest-paths-notes.pdf">https://www.cs.cmu.edu/afs/cs/academic/class/15210-s15/www/lectures/shortest-paths-notes.pdf</a></li>
</ul>
<p><strong>$\color{red}{\text{Dijkstra's Algorithm}}$</strong></p>
<ul>
<li>a sophisticated version of BFS</li>
<li>can handle positive weights only, no negatives</li>
<li>G can be either directed, or undirected</li>
</ul>
<pre><code>Input  : G=(V,E),                       # is our graph
         s in V ,                       # is our source vertex
         w(e) for all e in E            # are our weights, and must be positive
Outputs: dist(i) for all v in V

For all v in V
    dist(v) = length of shortest path from s -&gt; v
    ? prev(v) = previous edge along the min path
    # Although not explicit in the lecture video, 
    # this was mentioned in ed discussion

Uses a min-heap datastruct

inserts &amp; removals on a min-heap cost O(log n)
    there will be O(V) removals (updates are removal+insert)
    and O(V+E) inserts</code></pre>
<p><strong>Total Runtime:</strong> O((|V|+|E|) log |V|)</p>
<p>There's a couple of lemmas that come from Dijkstra's algo</p>
<ol>
<li>The subpath of any shortest path is itself a shortest path</li>
<li>Triangle Inequality</li>
</ol>
<ul>
<li>If dist(u,v) is the shortest path length from u to v </li>
<li>then dist(u,v) $\le$ dist(u,x)+dist(x,v)</li>
</ul>
<p>Almost anytime you need to determine the optimal path from one node to another, Dijkstra can be used.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="GR1:-Strongly-Connected-Components">GR1: Strongly Connected Components<a class="anchor-link" href="#GR1:-Strongly-Connected-Components">¶</a></h2><p><a href="https://cs6505.wordpress.com/schedule/scc/">Handy ref link</a></p>
<p>Our goal in this section is to look at graph connectivity via DFS-based algorithms. first let's do a quick review</p>
<ul>
<li>DFS for undirected graphs</li>
<li>DFS for directed graphs (DAGs-Directed acyclic graphs)</li>
</ul>
<p>In doing so we will see</p>
<ul>
<li>How to topologically sort a DAG</li>
<li>How to find the SCC (strongly connected components) of general graphs<ul>
<li>which you'll see is done using 2 applications of the DFS algo</li>
</ul>
</li>
</ul>
<p>How do we get the connected components in an undirected graph G?</p>
<ul>
<li>Run DFS and keep track of the component number</li>
</ul>
<h3 id="DFS-for-undirected-graphs">DFS for undirected graphs<a class="anchor-link" href="#DFS-for-undirected-graphs">¶</a></h3><p>We will use the DFS as our base algo and make a few small tweaks to support our use case. We want to be able to find a path between connected vertices. So in this case we will need to keep track of the predecessor vertex when we first visit a vertex.</p>
<p><strong>$\color{red}{\text{DFS Algo for Undirected Graphs}}$</strong></p>
<pre><code>DFS(G):
input : G=(V,E) in adjacency list representation (linked list)
output: vertices labelled by connected components

    cc=0    # current connected component number 
            # this acts as a counter of connected components
            # which DFS also uses as a label

    # we create an array to keep track of the vertices that have been visited
    # initialized to FALSE since we haven't visited yet
    for all v in V: 
        visited(v) = FALSE
        prev(v)    = NULL      # used to track previous vertex

    # iterate over the vertices in whatever order
    for all v in V:
        if not visited(v) then 
            cc++            # increment our connected component counter
            EXPLORE(v))     # then we explore the vertex

------------------------------------------------        
Now let's take a look at the explore algo    
# takes as input a vertex z
EXPLORE(z):
    # this is our first time visiting z 
    # so assign it the current connected component number cc
    ccnum(z) = cc

    # now we can mark it as visited
    visited(v) = TRUE

    # now we explore all edges adjacent to z
    for all (z,w) in E:
        if not visited(w) then
            # this time we do not increment
            EXPLORE(w)
            prev(w)   = z            # set the predecessor vertex, so we can backtrack to get the path</code></pre>
<p>Recall that DFS has a linear runtime, so O(n+m) where n=|V|, and m=|E|.</p>
<h3 id="DFS-for-directed-graphs">DFS for directed graphs<a class="anchor-link" href="#DFS-for-directed-graphs">¶</a></h3><p>Now we need to get the connectivity information for a directed graph G. This will be done using the standard DFS but we will need to modify such that it will also track the pre and post order numbers for the explored edges</p>
<ul>
<li>we will<ul>
<li>copy over the undirected algo</li>
<li>remove the connected component numbering</li>
<li>add pre/post numbers</li>
</ul>
</li>
</ul>
<p><strong>$\color{red}{\text{DFS Algo for directed Graphs}}$</strong></p>
<pre><code>DFS(G):
input : G=(V,E) in adjacency list representation (linked list)
output: vertices labelled by connected components

    clock = 1      # NEW - initialize our pre/post clock

    # we create an array to keep track of the vertices that have been visited
    # initialized to FALSE for unvisited
    for all v in V: 
        visited(v) = FALSE
        prev(v)    = NULL      # NEW - used to track previous vertex

    # iterate over the vertices in whatever order
    for all v in V:
        if not visited(v) then 
            EXPLORE(v))     # then we explore the vertex


Now let's take a look at the explore algo    
# takes as input a vertex z
EXPLORE(z):
    pre(z) = clock                   # NEW - our pre number, value of clock before further exploration
    clock++                          # NEW 
    visited(v) = TRUE                # and we set it to visited

    # now we explore all edges adjacent to z
    for all (z,w) in E:
        if not visited(w) then
            # this time we do not increment
            EXPLORE(w)
            prev(w)   = z            # NEW - we set the predecessor vertex

    post(z) = clock                  # NEW - post number, value of clock after further exploration
    clock++</code></pre>
<p>For our purposes we will only care about the post numbers, and will not be using the pre numbers. They have been included only for completeness sake.</p>
<p>Let's look at an example. It should be noted our example is effectively a single tree as each vertex is reachable. But this is not mandatory. It could very well be a forest of trees where not all vertices reachable from all other vertices. 
<img src="CS6515_images/GA1-003.png" width="400;"/></p>
<p>Let's run DFS supposing we start at B.</p>
<ul>
<li>the black arrows indicate a newly explored path, or first time exploration<ul>
<li>you'll notice that the pre number is being incremented at each step</li>
</ul>
</li>
<li>when we reach G there are no paths out so we begin moving back through the nodes<ul>
<li>when this happens we begin incrementing the post number</li>
</ul>
</li>
<li>as we move back we may encounter vertices that have an path out (such as E)<ul>
<li>in such situations we need to search this path to see if the ending vertex has been explored </li>
<li>in the case of E the endpt is A which already lies on the path to E<ul>
<li>we mark this as a blue line</li>
</ul>
</li>
<li>in the case of D we find an explored vertex H<ul>
<li>we must explore H, incrementing the clock as we do so</li>
<li>but in H we also find a path to G, but G is already explored so it gets a blue line</li>
</ul>
</li>
</ul>
</li>
<li>eventually we have worked our way all the way back to the root vertex B which is our starting pt<ul>
<li>we now set off on the unexplored vertices from B</li>
<li>we continue to increment our clock as we do so</li>
</ul>
</li>
</ul>
<p>Let's consider the edge (z,w), for arbitrarily chosen vertices z &amp; w. We can say that in general post(z)&gt;post(w). It's important to recall that edges are directed meaning that z must have come before w. but our blue lines indicate that there may be exceptions to this rule</p>
<p>Let's take a moment to look at the blue edges, and characterize them. You may notice that they fall into three categories</p>
<ul>
<li>Back edges: (e,a),(f,b): decendent to ancestor<ul>
<li>post(z) &lt; post(w)</li>
<li>this must be remembered as it is contrary to the natural behaviour </li>
</ul>
</li>
<li>forward: (d,g),(b,e): can jump over nodes <ul>
<li>post(z) &gt; post(w)</li>
</ul>
</li>
<li>cross: (h,g), (f,h): reachable without decendent to ancestor<ul>
<li>post(z) &gt; post(w)</li>
</ul>
</li>
</ul>
<p>Back edges bring about an interesting observation...</p>
<p><strong>Cycles in a DFS Tree</strong><br/>
for G=(V,E), G has a cycle iff its dfs tree has a back edge.</p>
<p>You should be able to see why this is true. But we all know how you'd like to see a proof.<br/>
<strong>Proof</strong><br/>
forward implication:<br/>
Consider a cycle C=v0-&gt;v1-&gt;v2-&gt;...-&gt;v0. Some vertex of C is first visited by DFS. Let v_i denote the first vertex of C that is visited. Form v_i we can reach every other vertex in C, thus the rest of the vertices in C are in the subtree rooted at v_i and t/f they are all descendents of v<em>i in the DFS tree. therefore the edge v</em>{i-1} -&gt; v_i will be a back edge</p>
<p>Backwards implication:<br/>
Suppose the back edge is e=v-&gt;w. Since v is a descendent of w in the DFS tree there is a path P from w to v. Thus P is a cycle.</p>
<p><strong>Topological Sorting</strong><br/>
What does it mean for a Directed Acyclic Graph, DAG, to be sorted. It means that the vertices are ordered such that all edges for from lower to higher. (Side note DAGs may not contain cycles, simply by definition ). Since a DAG contains no back edges this effectively means that a sorted DAG is one where post(v) &gt; post(w) for all v -&gt; w, v leading/pointing to w. Conequently, in a sorted DAG the <em>source</em> vertex will always be first, and the last vertex will have no outbound paths. The last vertex is called the sink vertex. Note that there may be more than one sink vertex. A simpler way to think of source vs sink is that a source has no incoming vertices, a sink has no outgoing vertices.</p>
<p>How do we construct/sort a dag?</p>
<ul>
<li>Run DFS on a DAG - get back the pre&amp;post numbers<ul>
<li>we know that for all z-&gt; w, post(z) &gt; post(w) will be true</li>
</ul>
</li>
<li>Now all that is left is to sort by post numbers<ul>
<li>we take the highest post number as our source</li>
<li>repeatdly taking the next highest vertex</li>
<li>continue until all vertices are in descending order by post number</li>
<li>this takes linear time   </li>
</ul>
</li>
<li>this whole thing will take O(n+m) time<ul>
<li>O(n+m) to run DFS</li>
<li>O(n) to perform the sorting</li>
</ul>
</li>
</ul>
<p>Let's do a small example. There is actually 3 possible topological orderings in this example. Can you spot them?
<img src="CS6515_images/GA1-004.png" width="400;"/></p>
<p>The full ordering is: x -&gt; y -&gt; z -&gt; u -&gt; w<br/>
Note that z will always come before u &amp; w as it has an outgoing edge. But u &amp; w are sinks with no outgoing vertex.</p>
<p>There is an alternative topological sorting algo that is useful for general directed graphs</p>
<ul>
<li>Find a sink vertex, output and delete</li>
<li>repeat (1) until graph is empty</li>
</ul>
<p>We will now turn our attention to performing the same tasks in a general directed graphs. The analog to a topological sorting in undirected graphs is SCC. Strongly connected components.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="DFS-vs-Explore">DFS vs Explore<a class="anchor-link" href="#DFS-vs-Explore">¶</a></h3><p>Let's take a moment to focus on the <strong>EXPLORE</strong> Subroutine in the DFS algorithm.</p>
<ul>
<li>Input  G=(V,E); $v \in V$</li>
<li>Output visited(u) boolean value TRUE for all nodes u reachable from v</li>
</ul>
<pre><code>Explore(G,v)

visited(v) = true          
previsit(v)
for each edge vu in E:
    if not visited(u): 
        explore(u)
postvisit(v)</code></pre>
<p><strong>Takeaway:</strong> EXPLORE finds all vertices reachable from s. (doesn't visit unconnected vertices!)</p>
<p>On the other hand we have DFS which uses, builds upon, explore</p>
<pre><code>DFS(G)

for all v in V
    visited(v) = false

for all v in V
    if not visited(v)
        explore(v)</code></pre>
<p><strong>Takeaways</strong></p>
<ul>
<li>DFS doesn't have/use a source vertex s<ul>
<li>but you can order the vertices putting s at the head of the line to trick it </li>
</ul>
</li>
<li>DFS will visit all vertices! <ul>
<li>when DFS finishes running the visited array will be true for all vertices</li>
<li>Meaning it doesn't make explicit all reachable vertices</li>
</ul>
</li>
</ul>
<p>BUT you can use it's output to find all reachable vertices! You simply look at the ccnum's.</p>
<ul>
<li>If s and t have the same ccnum<ul>
<li>then they are in the same component</li>
<li>t/f t is reachable from s</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="SCC---Strongly-Connected-Components">SCC - Strongly Connected Components<a class="anchor-link" href="#SCC---Strongly-Connected-Components">¶</a></h3><p><strong>Define: Strongly Connected</strong><br/>
Vertices v &amp; w are strongly connected if there is a path v -&gt; w &amp; w -&gt; v.</p>
<p>Given this definition we define Strongly Connected Components, denoted as a SCC, as a maximal set of strongly connected vertices. Let's look at an example</p>
<p><img src="CS6515_images/GA1-005.png" width="400;"/></p>
<ul>
<li>How many Strongly Connected Components, SCCs, does this have?             </li>
<li>What are the SCCs ?</li>
</ul>
<p><img src="CS6515_images/GA1-006.png" width="400;"/></p>
<p>Let's now consider some important properties of SCCs. Begin by creating a meta-vertex for every SCC, these are the set of vertexes inside the purple partitions. Construct a new graph where each of the SCCs are a single vertex and look at the edges between that is inherited.</p>
<ul>
<li>Metagraph: a single vertex for each SCC</li>
</ul>
<p><img src="CS6515_images/GA1-007.png" width="400;"/><br/>
Notice anything about this meta-graph? There are no cycles. It's a DAG, despite the underlying graph not being one. In fact any metagraph on SCC is a dag. Suppose there was a cycle in the metagraph on SCC. ie Suppose S -&gt; S' and S' -&gt; S where both S and S' are SCC. Then this would imply that is a path from every vertex in S to every vertix in S', and vice versa. But there a SCC than contains both S and S'. So we should merge these SCC into a larger component to form our metagraph.</p>
<p><strong>Every directed Graph is a DAG of it's SCCs</strong></p>
<p>Now all we need to do it determine how to find, and sort, these SCCs to come up with a DAG. Which surprisingly is not as difficult as it may seem. In fact it is done with just 2 runs of the dfs algorithm</p>
<p>Here's a general outline of the SCC algorithm<br/>
Recall our alternative approach DFS sorting algo from above</p>
<ul>
<li>find the sink node and remove it from the graph (you can also take a source approach)</li>
<li>place in storage</li>
<li>repeat until all nodes are removed (graph is empyt)</li>
<li>storage now contains a sorted list of vertices</li>
</ul>
<p>However this is an SCC algorithm, so we make a few modifications</p>
<ul>
<li>Find sink SCC S, output it, remove it, repeat <ul>
<li>You may be wondering if we can use source SCC. Perhaps, but SCC sinks are much easier to work with then source vertices. </li>
</ul>
</li>
<li>Take any $v \in S$, where S is a sink SCC</li>
<li>Run Explore(v)<ul>
<li>this will visit all nodes in S &amp; nothing else (A source vertex will not have this characteristic)</li>
<li>if we can choose a vertex in a sink SCC then we will be able to identify all SCC vertices.</li>
</ul>
</li>
</ul>
<p>So how do we guarantee a sink vertex? Recall that in a DAG the vertex with the lowest post order number must be a sink! In a general directed graph G, however, the lowest post order number is not guaranteed to be a DAG.</p>
<p>Consider a small graph:  B &lt;-&gt; A -&gt; C, you can manually compute the pre/post numbers as A(1,6), B(2,3), C(4,5). The lowest post number here is B but B lies in a source SCC {A,B}. Here C is the sink SCC despite not having the lowest SCC.</p>
<p>A takeaway though is the source vertex here is in the source SCC, and it has the highest postnumber. We will demonstrate that this always true for any directed graphs. We will use this fact to find the sink SCC. How will we do this? All it take is a change in the way we think and define the problem. Originally we defined our ordering from left to right as source to sink. Now we can think of right bound as our source, rather than the previous sink. Similarly our ordering will end when we get to the source which we will now place first, but which will come last. This is pretty confusing but all we've done is just redefine our terminology.</p>
<ul>
<li>Old: source -&gt; sink , start at left move right</li>
<li>New: sink &lt;- source , start at right move left</li>
</ul>
<p>For a directed graph $G=(V,E)$ we flip it, reverse it, and look at $G^R = (V,E^R)$</p>
<ul>
<li>$G^R$ is the reverse of G</li>
<li>$E^R = \{ \overset{\rightarrow}{wv} : \forall \overset{\rightarrow}{vw} \in E \}$<ul>
<li>this is the reverse of every edge in E</li>
</ul>
</li>
<li>this effectively flips our sources and sinks, the kitchen is on fire!<ul>
<li>source SCC in G becomes sink SCC in $G^R$</li>
<li>and sink SCCs become source SCCs</li>
<li>but the paths retain their strongly connected components</li>
</ul>
</li>
</ul>
<p>So our initial goal of finding a sink SCC in G is solved by finding a source SCC in $G^R$.<br/>
Summary<br/>
To find a vertex w in a sink SCC of G</p>
<ul>
<li>Take the input Graph $G=(V,E)$</li>
<li>reverse it to get $G^R = (V,E^R)$ </li>
<li>run DFS on $G^R$ to get the pre&amp;post numbers</li>
<li>take the vertex with the highest order post number in $G^R$, <ul>
<li>this is in a source SCC of $G^R$ </li>
<li>this is in a sink SCC of $G^R$ </li>
</ul>
</li>
</ul>
<p>In a manner of speaking this is like a transformation approach. we can't find a sink in G so we transform it to $G^R$, find the source, and transform back to get the sink of G. it's pretty simple when you think of it.</p>
<p>Consider our previous example:<br/>
<img src="CS6515_images/GA1-008.png" width="500;"/></p>
<p>look at what happens when we run DFS on this from a randomly chosen starting vertex C, D, L.<br/>
<img src="CS6515_images/GA1-009.png" width="500;"/></p>
<p>This illustrates one complete run of DFS on the inverse of G, with the final ordering from highest to lowest post order number. The vertices C,D,L are abitrarily chosen, and your algo may choose some other random starting point.</p>
<p>The next step is to run DFS again but this time we use the ordering provided by the last step. The first 5 vertices are marked as 1 to indicate that they come from the 1st visited SCC. Next we choose D as it is the vertex with the highest remaining post order number remaining in our unvisited list sorted in topological order. This will get marked 2 as it is in the 2nd visited SCC, then it is removed. Similarly {C,G,F} are marked as 3 and then removed. We continue this process until all our vertices are marked and have been removed. Voila we have our SCCs.</p>
<p><img src="CS6515_images/GA1-010.png" width="500;"/></p>
<p>You'll notice the ordering looks a bit wonky right? It's in reverse topological order, thankfully we can work with this.</p>
<p><strong>$\color{red}{\text{SCC Algorithm}}$</strong></p>
<pre><code>StronglyConnectedComponents(G):
    Input  : directed graph G(V,E) in adjacency list rep.
    Output : 

1. Construct G^R
2. Run DFS(G^R)
3. Order V by post numbers descending
    # this comes from the very first section in this chapter
4. Run undirectedGraph_DFS(G)</code></pre>
<p>Run Time</p>
<ul>
<li>2 has a run time of O(n+m)</li>
<li>4 has a run time of O(n+m)</li>
<li>Total run time is O(n+m)</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>You may recall that we took for granted the guarantee that the vertex with the highest post number lies in a source SCC. Let's now sketch out a quick proof of this claim.</p>
<p>A simpler claim, or lemma, would be that for SCCs S &amp; S', if $v \in S \rightarrow w \in S'$, then max(post_num in S) &gt; max(post_num in S').  This actually gives us a way to topologically sort the SCCs, by sorting according to the post_num in that SCC.</p>
<p><strong>Proof: Lemma</strong><br/>
Using the claim we can topologically sort the SCC's by the mac post order number in each SCC. The claim then implies that all edges go from earlier SCCs to later SCCs in this ordering and thus is a valid topological ordering. Consider the first SCC in this ordering, let's call this S. S must contain the vertex v with the maximum post order number, and because is first it must also be the source SCC. Hence v lies in a source SCC and that proves the lemma.</p>
<p><strong>Proof: Claim</strong><br/>
Let u be the first vertex in $S \cup S'$ that is visited. if $u \in S'$, then we visit and finish all of S' before visiting any of S because S' has no path to S, S &amp; S' are distinct. Hence post(w) &gt; post(z) for all $z \in S \cup S' - \{u\}$. This proves the claim.</p>
<p><strong>Proof: Claim</strong><br/>
Let u be the first vertex in $S \cup S'$ that is visited. if $u \in S'$, then we visit and finish all of S' before visiting any of S because S' has no path to S, S &amp; S' are distinct. Hence post(w) &gt; post(z) for all $z \in S \cup S' - \{u\}$. This proves the claim.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><img src="CS6515_images/GA1-009.png" width="500;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="GraphSearch-Summary">GraphSearch-Summary<a class="anchor-link" href="#GraphSearch-Summary">¶</a></h3><ul>
<li>DG Directed Graph</li>
<li>UG Undirected Graph</li>
</ul>
<p>BFS: Breadth First Search</p>
<ul>
<li>runtime: O(|V|+|E|)</li>
<li>Input: G(V,E) (DG &amp; UG) +  source vertex s</li>
<li>Output: <ul>
<li>dist(v) = number of edges on path from s to v, $\infty$ if unreachable</li>
<li>prev(v) = parent of v, the previous node on the path to v</li>
</ul>
</li>
<li>Gotcha's<ul>
<li>doesn't use weight/ nor does it factor them in </li>
</ul>
</li>
</ul>
<p>Dijkstra, like BFS with weights</p>
<ul>
<li>Runtime: O((|V|+|E|) log |V|)</li>
<li>Input: G(V,E,w(e)) (DG &amp; UG) +  source vertex s + w(e) &gt; 0 $\forall e \in E$</li>
<li>Output: <ul>
<li>dist(v) = number if edges on path from s to v, $\infty$ if unreachable</li>
<li>prev(v) = parent of v, the previous node on the path to v</li>
</ul>
</li>
<li>Gotcha's<ul>
<li>very much a brute force approach </li>
</ul>
</li>
</ul>
<p>DFS: Depth First Search - The mother of all advance graph algo's</p>
<ul>
<li>Runtime: O(|V|+|E|)</li>
<li>Input: G(V,E) (DG &amp; UG)</li>
<li>Output: <ul>
<li>pre = first visit order number </li>
<li>post = left order number, the order number when the recursion moves back</li>
<li>ccnum = connectivity number</li>
<li>prev(v) = parent of v, the previous node on the path to v</li>
</ul>
</li>
<li>Gotchas<ul>
<li>unknown</li>
</ul>
</li>
</ul>
<p>SCC: aka DFS for SCC</p>
<ul>
<li>Runtime: O(|V|+|E|)</li>
<li>Input: G(V,E) directed only </li>
<li>Output: <ul>
<li>$G_{scc} = (V_{scc}, E_{scc})$ this is the metagraph, or DAG of Strongly connected components<ul>
<li><strong>NOTE</strong> </li>
<li>$V_{scc}, E_{scc}$ refer to $G_{scc}$ the metagraph</li>
<li>These provide a grouping number to the vertices in the underlying graph G</li>
<li>which you can also see by ccnum, which will repeat in a SCC graph network</li>
</ul>
</li>
<li>ccnum(v) = connected component number $\forall v \in V$</li>
</ul>
</li>
<li>Gotchas:<ul>
<li>unknown</li>
</ul>
</li>
</ul>
<p>Explore: Actually a function within DFS but this class allows it's use on it's own</p>
<ul>
<li>Runtime: O(|E|) if within DFS, else  O(|V|+|E|) if on it's own<ul>
<li>this is due to the overhead provided by the DFS algo</li>
</ul>
</li>
<li>Input: G(V,E) (DG &amp; UG)</li>
<li>Output: <ul>
<li>visited(v) = true/false value if the vertex v has been visited</li>
</ul>
</li>
<li>When<ul>
<li>handy when you need to check for the existence of a path, or connectedness</li>
</ul>
</li>
<li>Gotchas<ul>
<li>unknown</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="GR2:-Satisfiability">GR2: Satisfiability<a class="anchor-link" href="#GR2:-Satisfiability">¶</a></h2><h3 id="GR2-Intro">GR2 Intro<a class="anchor-link" href="#GR2-Intro">¶</a></h3><p>Let's now turn our attention to an application of the SCC algorithm. We will be looking at the satisfiability problem, aka SAT problem. This problem will haunt us ... returning when we study the idea of NP completeness.</p>
<p>Some terms we will need<br/>
We will be working  with boolean formulas composed of the following</p>
<ul>
<li>n 1-bit variables $x_1,x_2,...,x_n$, here each variable can be either 1 or 0 only</li>
<li>2n literals $x_i,\bar{x_i}$ for $1 \le \le n$<ul>
<li>these refer to the variable $x_i$ and it's complement</li>
</ul>
</li>
<li>our formula's will be composed of ANDs, $\wedge$, and ORs $\vee$    </li>
<li>our formulas will also be written in conjuctive normal form (aka CNF)<ul>
<li>ie it is conjunction of one or more clauses, where a clause is a disjunction of literals</li>
<li>or more simply it is the ANDing of one of more OR'ing clauses </li>
<li>example<ul>
<li>a clause: OR of several literals: $(x_3 \vee \bar{x_5} \vee \bar{x_1} \vee x_2)$ </li>
<li>AND'ing of multiple clauses</li>
<li>$(x_2)\wedge(\bar{x_3} \vee x_4)\wedge(x_3 \vee \bar{x_5} \vee \bar{x_1} \vee x_2)\wedge(\bar{x_2} \vee \bar{x_1})$ </li>
</ul>
</li>
<li>using this example <ul>
<li>to satisfy the formula we would need to satisfy each clause </li>
<li>this means that at least 1 literal in each clause needs to be true, </li>
<li>then the entire formula is true and is satisfied</li>
</ul>
</li>
<li>Using this example<ul>
<li>take $x_1=F, \;x_2=T, \;x_3=F$ will satisfy the formula and return TRUE</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Let's Now formalize the SAT Problem</p>
<ul>
<li>In : a formula $f$ in CNF form with n variable and m clauses</li>
<li>out: an assignment satisfying $f$ if one exists ( assignment meaning a boolean value for each variable )<ul>
<li>null/no if an assignment doesn't exist</li>
</ul>
</li>
</ul>
<p>Another Example:</p>
<ul>
<li>$f = (\bar{x_1} \vee \bar{x_2} \vee x_3)\wedge(x_2 \vee x_3)\wedge(\bar{x_3} \vee \bar{x_1} )\wedge(\bar{x_3})$</li>
<li>find the satisying assignment</li>
</ul>
<p>For our purposes we will be looking at a restricted form of the SAT problem. For us it will be the k-SAT problem which for all intents is just a restriction of the general SAT problem.</p>
<h3 id="kSAT-Problem">kSAT Problem<a class="anchor-link" href="#kSAT-Problem">¶</a></h3><ul>
<li>For k-SAT each clause must be size $\le k$ for some integer k</li>
<li>our example above would thus be a 3-SAT problem</li>
<li>you'll understand later why we did this, but here's a hint <ul>
<li>SAT is an NP-complete problem</li>
<li>k-SAT is an NP-complete $\forall k \ge 3$</li>
<li>$\forall k \le 2$ it becomes a polynomial time algo</li>
</ul>
</li>
</ul>
<p>Let's consider some arbitrary 2-SAT problem</p>
<ul>
<li>because k=2 we know that each clause must contain 1 or two literals</li>
<li><p>We begin by removing the uni-literals (contain just 1 literal)</p>
<ul>
<li>locate the unit-clause, and determine it's literal (say $a_i$)<ul>
<li>this clause can be either a literal or it's complement</li>
</ul>
</li>
<li>find the value satisfies it, there's only two possible option TRUE or FALSE, (say $a_i=TRUE$)</li>
<li>remove clauses containing $a_i$ </li>
<li>drop the complement, from all remaining clauses, $\bar{a_i}$<ul>
<li>this will simplify remaining clauses</li>
<li>call this new formula $f'$</li>
</ul>
</li>
<li>f' is similar to f in that all clauses will be less than or equal to 2<ul>
<li>one key difference is that in f' there may exist empty clauses</li>
<li>this arises when the literal is dropped but the clause is not</li>
<li>the existence of an empty clause means the formula cannot be satisifed</li>
<li>return NO</li>
</ul>
</li>
</ul>
</li>
<li><p>Claim: <strong>$f$ is satifiable iff $f'$ is satifiable</strong></p>
<ul>
<li>btw you may notice a bit of DP style problem coming into view. </li>
</ul>
</li>
</ul>
<p>You may wondering what any of this has to do with graphs? Well your curiosity is about to be satisfied. We will construct a graph from the CNF formula $f$.</p>
<p>Recall: we have n variables, and m clauses with all clauses of size 2<br/>
Creating a DAG</p>
<ul>
<li>Define V as the 2n vertices corresponding to $x_i, \; \&amp; \;\bar{x_i}$ for $1 \le i \le n$</li>
<li>Define E as the 2m edges corresponding to 2 "implications" per clause<ul>
<li>each clause as 2 implications, </li>
<li>1 edge for each implication leads to 2 edges per clause </li>
<li>for a total of 2m edges for m clauses</li>
<li>"implication" meaning<ul>
<li>with 2 variables in each clause there are 2 possible implications to satisfaction</li>
<li>consider the clause $\bar{x_1} \vee \bar{x_2}$</li>
<li>implication 1: if $x_1$ is true then to satisfy the clause we must have $x_2$ FALSE</li>
<li>implication 2: Similarly if $x_2$ is true then we must have $x_1$ FALSE</li>
</ul>
</li>
<li>in general for $(\alpha \vee \beta)$ <ul>
<li>we will need the following 2 edges</li>
<li>$E_1 = \overset{\rightarrow}{\alpha} \rightarrow \beta$</li>
<li>$E_2 = \overset{\rightarrow}{\beta} \rightarrow \alpha$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Example $f = (\bar{x_1} \vee \bar{x_2})\wedge(x_2 \vee x_3)\wedge(\bar{x_3} \vee \bar{x_1})$<br/>
<img src="CS6515_images/GA1-011.png" width="500;"/></p>
<p>Let's dig into this example</p>
<ul>
<li>observe the path of $x_1$<ul>
<li>$x_1 \rightarrow \bar{x_2} \rightarrow x_3 \rightarrow \bar{x_1}$</li>
<li>reduces to $x_1 \rightsquigarrow \bar{x_1}$</li>
<li>this is clearly not good is it? </li>
</ul>
</li>
<li>The paths here are implications <ul>
<li>so if $x_1$ = true then $\bar{x_1}$ must also be true but then this means that $x_1$ is false which is a contradiction</li>
<li>what if $x_1$ is false? Notice that there are no edges outwards from $\bar{x_1}$<ul>
<li>thus there is no implication here so it may be ok, it will now depend on the other variables in the path </li>
</ul>
</li>
</ul>
</li>
<li><p>side note</p>
<ul>
<li>should you ever encounter a situation where there are paths</li>
<li>$x_1 \rightsquigarrow \bar{x_1}$ and $\bar{x_1} \rightsquigarrow x_1$ </li>
<li>you can immediatley say that f is not satisfiable</li>
<li>A contradiction will always arise</li>
<li>when can this happen? .... if you guessed an SCC you're on the path to an A</li>
<li>if $x_1$ and $\bar{x_1}$ are in the same SCC then f is not satisfiable</li>
</ul>
</li>
<li><p>side note</p>
<ul>
<li>we've used $x_1$ and $x_2$ in our example above but this can be generalized to i and j where $i \ne j$</li>
</ul>
</li>
</ul>
<p><strong>LEMMA</strong>:</p>
<ul>
<li>if for some i, $x_i$ and $\bar{x_i}$ are in the same SCC then $f$ <strong>is not</strong> satisfiable              </li>
<li>if for some i, $x_i$ and $\bar{x_i}$ are in a different SCC then $f$ <strong>is</strong> satisfiable              </li>
<li>these can be wrapped together into a single statement<ul>
<li>$f$ is satisfiable $\iff \forall i, x_i \text{ and } \bar{x_i}$ are in different SCCs</li>
</ul>
</li>
</ul>
<p>We've already discussed the first statement, and will leave this to the reader as an excercise. The second part requires some extra building blocks before we can prove it.</p>
<p><strong>Idea: Approach 1</strong><br/>
Suppose $x_i \text{ and } \bar{x_i}$ are in different SCCs. Let S be a sink SCC and take S=TRUE, to satisfy all literals in S. Because S is a sink it has no outgoing edges, and thus no implications. What does this say about $\bar{S}$? It must be FALSE to satisfy f. In order to be false we don't want it to have any incoming edges. This means that it should be a source SCC. (You may recall that a false can imply anything even a true). This is super important as don't need to worry about the outgoing implications of false.</p>
<ul>
<li>take a sink SCC, say S, set S=T<ul>
<li>this will satisfy all literals in S</li>
<li>because S=TRUE it will also satisfy all edges leading to S</li>
<li>because it is a sink we don't have to worry about any implications, there are no outgoing edges</li>
</ul>
</li>
<li>remove S from the graph</li>
<li>repeat this procedure</li>
</ul>
<p><strong>Idea: Approach 2</strong><br/>
Similarly, say S' is a source SCC and take S'=FALSE. S' cannot have incoming edges, so we can safely remove it. But then $\bar{S'}$ must be true, which will satisfy all the literals in $\bar{S'}$. If the literals are satisfied then later implications will be satisfied as well. Does $\bar{S'}$ have any outgoing edges?</p>
<p>These two approaches are saying relatively the same thing:</p>
<ul>
<li>Sink SCC should be set to True and Source SCCs should be false</li>
<li>the complement of a sink is a source and vice versa</li>
</ul>
<p>Would you like to see a proof? You know you do</p>
<p><strong>Claim</strong>:<br/>
If for all i, $x_i \text{ and } \bar{x_i}$ are in different SCCs<br/>
then $S$ is a sink $\iff \bar{S}$ is a source SCC</p>
<h3 id="2SAT-Algorithm">2SAT Algorithm<a class="anchor-link" href="#2SAT-Algorithm">¶</a></h3><p><strong>$\color{red}{\text{2SAT Algorithm}}$</strong></p>
<ol>
<li>Given f in CNF form</li>
<li>construct a graph G for f<ul>
<li>this provides us with the implications</li>
</ul>
</li>
<li>run a SCC algo on this graph G, SCC=strongly connected component algorithm from previous section<ul>
<li>this provides us with the strongly connected components in topological order</li>
</ul>
</li>
<li>Take a sink SCC S, which is the last element from step 2 <ul>
<li>set $S$=True, to satisfy all literals in $S$<ul>
<li>it follows that $\bar{S}$ will be a source and thus False (according to our claim)</li>
</ul>
</li>
<li>remove $S$ and $\bar{S}$</li>
<li>repeat until we're left with the empty graph</li>
</ul>
</li>
<li>we should have now satisfied the formula (or know it can't be satified)</li>
</ol>
<p>Finally, let's tie up our loose end. We must prove our claim</p>
<ul>
<li>If for all i, $x_i \text{ and } \bar{x_i}$ are in different SCCs then $S$ is a sink $\iff \bar{S}$ is a source SCC </li>
</ul>
<p>We will prove this using a simpler claim</p>
<ul>
<li>If we have a pair of literals $\alpha$ and $\beta$ </li>
<li>and if there is a path from alpha to beta, $\alpha \rightsquigarrow \beta$</li>
<li>then there's a path from $\bar{\beta} \rightsquigarrow \bar{\alpha}$</li>
<li>and the inverse is also true</li>
</ul>
<p>In short: $\alpha \rightsquigarrow \beta \iff \bar{\beta} \rightsquigarrow \bar{\alpha}$</p>
<p>We begin by proving: if $S$ is a sink $\iff \bar{S}$ is a source SCC<br/>
Take a sink SCC S. Suppose $\alpha,\beta \in S$. Thus they are strongly connected, and because they are in the same SCC the existence of paths $\alpha \rightsquigarrow \beta$ and $\beta \rightsquigarrow \alpha$ are guaranteed. Similarly there must be a path between their complements, which means they are also in the same SCC. Since $\alpha,\beta \in S$ then $\bar{\alpha},\bar{\beta} \in \bar{S}$. This demonstrates that for any pair of vertices in S, then their complements form an SCC, and if a pair of vertices in S are strongly connected the so must thier complements be.</p>
<p>Now all that is left is to show that the complement of a sink SCC is a source SCC. Let S be a sink SCC and choose an abritrary literal in S, say $\alpha \in S$. We know that there cannot be an edge leading to $\beta$ outside S s.t. $\alpha \rightsquigarrow \beta$, (if there were it wouldn't be a sink). if $\alpha \rightsquigarrow \beta$ cannot exist then $\bar{\beta} \rightsquigarrow \bar{\alpha}$ cannot exist either. Because no outgoing edges from $\alpha$ means no incoming edges to $\bar{\alpha}$. And what does no incoming edges mean? Well it means it's in a source SCC. Thus we've shown that the complement of a sink SCC is a source SCC. Using the same logic and flipping terms will lead to inverse conclusion, namely, the complement of a source SCC is a sink SCC.</p>
<p>Finally we demonstrate: $\alpha \rightsquigarrow \beta \iff \bar{\beta} \rightsquigarrow \bar{\alpha}$</p>
<p>Take a path $\alpha \rightsquigarrow \beta$, say $\lambda_0 \rightarrow \lambda_1 \rightarrow \lambda_2 \rightarrow \cdots \rightarrow \lambda_l$, where $\lambda_0 = \alpha$ and $\lambda_l = \beta $</p>
<ul>
<li>Consider the subpath $\lambda_i \rightarrow \lambda_{i+1}$<ul>
<li>this comes from the clause $(\bar{\lambda_i} \vee \lambda_{i+1})$</li>
<li>which has another edge $\bar{\lambda_{i+1}} \rightarrow \bar{\lambda_{i}}$</li>
</ul>
</li>
<li>if you do this to the entire path you get the reverse path of complements <ul>
<li>$\bar{\lambda_l} \rightarrow \cdots \rightarrow \bar{\lambda_0}$</li>
<li>which simplifies to </li>
<li>$\bar{\beta} \rightarrow \cdots \rightarrow \bar{\alpha}$</li>
<li>as needed</li>
</ul>
</li>
<li>Similarly for $\Leftarrow$, just repeat the process starting with the complements</li>
</ul>
<p>That completes our proof and this section is over</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="GR3-Minimum-Spanning-Tree">GR3 Minimum Spanning Tree<a class="anchor-link" href="#GR3-Minimum-Spanning-Tree">¶</a></h2><p>We're going to look now at the greedy approach for optimization problems. So we're going to take a locally optimal move. In particular, we have a partial solution and we're going to take the optimal move for the next step and the question is, when does this approach lead to the global optimum?</p>
<p>We saw earlier for the knapsack problem, that this greedy approach doesn't work. There are examples where the greedy does not lead to the optimal solution. Nevertheless, we were able to use dynamic programming to find the optimal solution for the knapsack problem.</p>
<p>What we're going to work on now, is the minimum spanning tree problem and we're going to see that for this problem, that the greedy approach does work. One particular algorithm that we're going to use for the minimum spanning tree problem is the Kruskal's algorithm.</p>
<p>The important thing that we're going to stress in this lecture, is the proof of correctness of Kruskal's algorithm for the minimum spanning tree problem. Why exactly does the greedy approach work for this problem? We're going to see a general lemma known as the cut property. This lemma is going to imply that Kruskal's algorithm works correctly for the minimum spanning tree problem.  We're also going to see that Prim's algorithm for the minimum spanning tree problem also works correctly. So, this general lemma implies that Kruskal's algorithm works correctly and Prim's algorithm works correctly.</p>
<p>We will review the precise formulation of the minimum spanning tree problem and then we'll look at the general formulation of the cut property, its statement and its proof, and then we'll see that it immediately implies that Kruskal's algorithm works correctly and Prim's algorithm work.</p>
<p><strong>From Wikipedia:</strong><br/>
A minimum spanning tree (MST) or minimum weight spanning tree is a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight. That is, it is a spanning tree whose sum of edge weights is as small as possible. More generally, any edge-weighted undirected graph (not necessarily connected) has a minimum spanning forest, which is a union of the minimum spanning trees for its connected components.</p>
<p><strong>Formulation: Mimimum Spanning Tree</strong><br/>
Given: An undirected G=(V,E) with weights w(e) for $e \in E$<br/>
Goal: Find a minimal size, connected subgraph of min weight</p>
<ul>
<li>a minimal size, connected subgraph is a tree (not a forest!) </li>
<li>the minimal weight of a spanning tree of G can be formally defined as <ul>
<li>for a tree T in E, $\large T \subset E, w(T) = \sum_{e \in T} w(e)$</li>
<li>now take the minimum over the edges</li>
</ul>
</li>
</ul>
<p><strong>Properties of Trees</strong></p>
<p>A Tree is a connected acyclic graph</p>
<ul>
<li>a Tree with n vertices has n-1 edges <ul>
<li>if the number of edges is above n-1 then there's a cycle</li>
</ul>
</li>
<li>In a tree, there is exactly one path between every, or any, pair of vertices    <ul>
<li>of there were 0 then that vertice would not be connected</li>
<li>if there were more 2 or more then there would be a cycle</li>
</ul>
</li>
<li>Any connected G=(V,E) with |E|=|V|-1 is a tree<ul>
<li>This follows from the above two properties</li>
<li>it basically says that </li>
<li>any connected graph with n-1 edges and n vertices is a tree</li>
</ul>
</li>
</ul>
<p>A few more from the class textbook.</p>
<ul>
<li>Removing a cycle edge cannot disconnect a graph. </li>
<li>An undirected graph is a tree iff there is a unique path between any pair of nodes.</li>
</ul>
<h3 id="Kruskal's-Algorithm">Kruskal's Algorithm<a class="anchor-link" href="#Kruskal's-Algorithm">¶</a></h3><p>We will work through an example but the algo is quite simple to verbalize</p>
<blockquote><p>repeatedly add the lightest edge that does not produce a cycle</p>
</blockquote>
<p>Consider the following example 
<img src="CS6515_images/GA1-012.png" width="500;"/></p>
<p>Imagine that we are given the graph and must construct a minimum spanning tree. We will take a greedy approach and begin by adding edges one at a time according to the minimum weight. First we add the edge 1, then 2, then 3, etc, but when we get to 6 we must move outwards. Adding 6 would create a cycle and our tree may not have cycles!</p>
<p>We keep going and keep adding edges. Until finally we get to the following situation. Notice that the edge from v to w does not get added. Nor do any of the edges with weight 12. What we are left with is in fact the minimum spanning tree.
<img src="CS6515_images/GA1-013.png" width="500;"/></p>
<p>What we have just done is perform Kruskal's algorithm which we will now formalize.</p>
<p><strong>$\color{red}{\text{Kruskal's Algorithm}}$</strong></p>
<pre><code>Kruskal's(G):
Input  Undirected graph G=(V,E) with weights w(e)

1. sort E by increasing weight (from lowest to highest)
2. Set X = null                                           // empty set
3. for e=(v,w) in E
    if X union e doesn't have a cycle
        then add e to X
4. return X</code></pre>
<p><strong>Running Time</strong><br/>
step 1: requires O(|E| log |V|) time (recall merge sort)<br/>
step 2: requires constant time<br/>
step 3: is a bit tricky</p>
<ul>
<li>let (v,X) be a subgraph of v and X</li>
<li>Let c(v) be the component containing v in (v,X)</li>
<li>Let c(w) be the component containing w in (v,X)<ul>
<li>if c(v) $\ne$ c(w) then add e to X - this implies that they are in different components </li>
<li>to check the component containing v &amp; w, </li>
<li>we use a data struct called a UNION-FIND </li>
<li>whose operations run in O(log n) time</li>
</ul>
</li>
<li>So the for loop over the edges has a worst case of |E|-times, <ul>
<li>and each operation requires O(log |V|)</li>
<li>for a total of O(|E| log |V|)</li>
<li>which you'll notice is the same as step 1</li>
</ul>
</li>
</ul>
<p>Finally we can claim that the total runtime is O(|E| log |V|) for the whole agorithm.</p>
<h3 id="Correctness">Correctness<a class="anchor-link" href="#Correctness">¶</a></h3><p>Let's use our example from before, and let's freeze the algo after we've included the two edges with weight 7. At this point the algorithm is going to consider edges with weight 9. So let's suppose it consider the edge (v,w) circled in purple.
<img src="CS6515_images/GA1-014.png" width="500;"/></p>
<p>We assume by induction that X is correct so far. By correct we mean $X \subset T$ where T is an MST. We haven't found the MST yet, but we assume it's existence. think of X as the partial construction of an MST T. Suppose we add the edge (v,w) to X, then we must ensure that $(v,w) \cup X$ is still part of an MCT. Claim $X \cup e \subset T'$ where T' is an MST. T' may be different than T, but this doesn't really matter. All that matters is that the union doesn't violate the properties of an MST. So how do we know that it's inclusion is still an MST?</p>
<p>Take a closer look at the graph illustration. Before adding v,w they are in different components. This means that adding them will bridge their components without creating any cycles. Had they been in the same component adding this edge would have created a second path between them, which would indicate a cycle.</p>
<p>Now, the key property is that if we consider one of these components, let's say the component containing V and we let S be the set of vertices and the component containing V. And $\bar{S}$ is the complement of this set (the rest of these vertices). Then the key property is that our current subset X, has no edges which go from S to S_bar.</p>
<p>If there was an edge that goes from S to S_bar, then that vertex in S_bar which is connected to S would be added into S. So, it would be in the component containing V. This component is a maximal set. So, we know that nobody in S_bar is connected to anybody in S. Therefore, there are no edges of X going from S to the rest of the graph. That's the definition of a component. So X has no edges that go between S and S_bar. So, X doesn't cross between S and the rest of the graph, S_bar But the edge e=(v,w) does. e is also a minimum weight between S and S_bar.</p>
<p>So what we must prove is that an edge, like (v,w), that crosses between two MST components (such as S, and S_bar) can be added to an MST subset and remain an MST. This is an important property known as the Cut property</p>
<h3 id="Graph-Cut-&amp;-Min-Cut-Property">Graph Cut &amp; Min Cut Property<a class="anchor-link" href="#Graph-Cut-&amp;-Min-Cut-Property">¶</a></h3><p><strong>Definition: Cut</strong></p>
<p>For an undirected graph G=(V,E) with partition $V=S \cup \bar{S}$<br/>
Then cut($S,\bar{S}$) = {$(v,w)\in E : v \in S, w \in \bar{S}$} (look closely to see this refers to edges that cut across the two distinct sets). Later on we'll see other optimization problems that make use of this idea. Like the min-cut, and max-cut problems</p>
<p><img src="CS6515_images/GA1-015.png" width="500;"/></p>
<p>We are now ready to state the cut property, this will help us to finalize the correctness of our MST Algorithms.</p>
<p><strong>Lemma:</strong></p>
<p>For undirected graph G=(V,E)</p>
<ul>
<li>Take $X \subset E$ where $X \subset T$ for an MST T</li>
<li>Take $S \subset V$ where no edge of X is in the cut($S,\bar{S}$)<ul>
<li>we get to Choose $S,\bar{S}$</li>
</ul>
</li>
<li>Look at all edges of G in cut($S,\bar{S}$)             <ul>
<li>of these take a min weight edge, say $e^*$ is a min weight edge in cut($S,\bar{S}$)             </li>
</ul>
</li>
<li>Then $X \cup e^* \subset T'$ where T' is a MST
<img src="CS6515_images/GA1-016.png" width="500;"/></li>
</ul>
<p>Now how did this apply for Kruskal's algorithm? For Kruskal's algorithm we said that the edge $e^*$ was between vertices v and w. We added this edge $e^*$, if the component containing v, one of the endpoints of $e^*$ was not the same as a component containing w. So v and w were not connected in the current subgraph.</p>
<p>So what was the set $S$ in this example? The set $S$ in this example was the component containing v, and $\bar{S}$ is the  component containing w. Therefore since the component is a maximal set of connective vertices we know that there is no edge in the partial solution which crosses from $S$ to $\bar{S}$. And then since we consider the edges in increasing order, in sorted order, then we know that e-star must be the minimum weight edge crossing from this component to outside. So $e^*$ or that Kruskal's algorithm considers is satisfies the hypothesis of the Lemma. So if we add an $e^*$ to our partial solution we'll still be on our way to an MST.</p>
<p>So this proves correctness of Kruskal's algorithm. Kruskal's algorithm uses a particular type of set s, but this is true in general for any cut s.</p>
<p>Now it's important to understand the statement of the cut property and also to understand the proof. The statement, the main idea is that any edge which is minimum weight across a cut is going to be part of some MST. Why? Because if you give me a tree T which does not contain this minimum weight edge across the cut then I can add this edge into the current tree and I can construct a new tree which is of smaller weight.</p>
<p>Actually, to be precise, I'm not necessarily going to construct a new tree with a smaller weight, but if you give me a tree T which does not contain this edge e star, or construct a new tree, T', where the weight of T' is at most the weight of T. So the weight doesn't increase. And that's going to be the intuition for the proof.</p>
<p>We're going to take this tree T,  we don't know this tree T but we know there exists an MST T which contains X, our partial solution so far. And now we're going to add this edge e star into X. If this MST T contains X unit e star then we're all done. Now there's no reason why this edge e star has to be in this tree T. So what happens if edge e star is not in the tree T? Well then we're going to construct a new tree, T', where the weight of T' is at most the weight of T, and in this tree T prime will be the edge e star and also X, and we'll construct this tree T prime by modifying the tree T slightly by adding in the edge e star and then removing an edge, in order to construct a new tree. So let's go ahead and dive into the proof of the cut property.</p>
<p><strong>Proof Outline</strong><br/>
Fix G, X, T, and S</p>
<ul>
<li>we assume $X \subset T$, where T is an MST, we don't know T yet but we know it must exist</li>
<li>at this point no edge of X may cross $S$ and $\bar{S}$<ul>
<li>However, T may have edges that cross between $S$ and $\bar{S}$</li>
<li>T is an MST afterall and must connect $S$ and $\bar{S}$</li>
</ul>
</li>
<li>now we choose an $e^*$ with a minimum weight, that crosses $S$ and $\bar{S}$<ul>
<li>ie $w(e) \le w(e_1) \le \cdots \le w(e_n)$</li>
</ul>
</li>
</ul>
<p><strong>Goal:</strong> Construct MST T' where $X \cup e^* \subset T$</p>
<p>How? Well we will do this by constructing T'. Well we have this tree T. T is an M S T and T contains the partial solution x. Now the problem is that this tree T might not contain this edge e star. Well we try to modify this tree T to construct a new tree, T', and we'll show that this new tree T' is a minimal spanning tree. So the weight of T' is at most the weight of T, actually the weights must be equal, and this new tree T' is modified a little bit from T so that it contains $e^*$ in addition to containing the partial solution x.</p>
<p>Let's now take a look at how to construct T'. We know that there is an MST T, and thus $e^*$ is a part of T or it is not.</p>
<p><strong>Case A:</strong> $e^*$ is a part of T. In this case T' = T</p>
<p><strong>Case B:</strong> $e^*$ is not part of T.</p>
<ul>
<li>begin by looking at $T \cup e^*$, where $e^* = (a,b)$ </li>
<li>recall that T is a tree, so adding an edge creates a cycle, let's call it C </li>
<li>but then there are two paths between a and b, so we may drop an edge in order to create an MST</li>
<li>of course, since we are creating a minimum spanning tree, the edge we drop must be at least that of $e^*$</li>
<li>but any edge that crosses $S$ and $\bar{S}$ will be at least that of $e^*$, since it was chosen to be the minimum cut        </li>
<li>so we can drop any of the other edges ... perhaps we "cut" them out?</li>
<li>so let's drop an arbitrary edge e', that cuts across $S$ and $\bar{S}$ </li>
<li>finally we have $T' = T \cup e^* - e'$</li>
</ul>
<p>Well, we've constructed this T′ and now we have to prove that T′ is in fact a tree and that is a minimum weight. Therefore, it's a minimum spanning tree.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="P1-Proof-T'-is-a-tree">P1-Proof T' is a tree<a class="anchor-link" href="#P1-Proof-T'-is-a-tree">¶</a></h3><p>First we prove that T prime is a tree. How do we construct T prime once again? We took the tree T and we added it in this edge $e^*$ that created a cycle. Now, along that cycle we removed any edge of that cycle. In this case we removed, let's say e'.</p>
<p>What we're going to show now is that we can remove any edge of this cycle and the resulting graph T' will be a tree. Now, how are we going to show that the T' is a tree? Well, we're going to show that T' is connected and has exactly n-1 edges.</p>
<p>Recall from the beginning of the lecture, we said that if a sub graph has exactly n-1 edges and is connected then it must be a tree. Now the fact that it has exactly n-1 edges is obvious. Why? Because T is a tree so it has exactly n-1 edges, we added one edge in and removed one edge. So we still have exactly n-1 edges. So what remains? We just have to show that it's connected. So take any pair of vertices y and z and let's show that there's a path between y and z in this sub graph T'.</p>
<p>In T there is a path between y and z of course because T is connected. The path goes along this edge e prime. Now what happens 
in T'? T' doesn't contain the edge e' ($T' = T \cup e^* - e'$). So this old path between y and z no longer exists in T prime.  So let's let P denote the path $y \rightsquigarrow z \in T$, the original tree. And now we have to construct a path in T prime which goes from y to z. Now, recall that when we looked at $T \cup e^*$ what do we get? We got a cycle, these blue edges are the cycle in $T \cup e^*$. So let C denote the cycle in $T \cup e^*$. Let's denote the end points of e prime as c and d. Now e prime is a path from c  to d in T. Now this cycle C has two paths between c and d, e' is one of those paths, let's take the other path.</p>
<p><img src="CS6515_images/GA1-017.png" width="500;"/></p>
<p>So let's look at this cycle C and take out the edge e prime, what do we have? We have a path and this path goes from c to d and all these edges in this path, exist in T prime because T prime contains all these edges except for e prime but we're not using e prime, we took e prime out. So we took the cycle, dropped off the edge e prime and this gives us a path from c to d in T prime. So, how do we get from y to z in T prime? Well, we're going to follow the original path P for whenever we hit this edge e prime, we replace this edge e prime by this new path P'. Now, to show that y and z are connected in T'. So we want to show that there is a path from y to z in T prime. Well, we're going to use the original path in T and then whenever we hit the edge e prime, we're going to replace the edge e prime by this path P prime because the edge e prime does not exist in T prime but the path P prime does exist. So we're going to go along this path and now the original path uses this edge e prime but it doesn't exist now so we use the rest of the cycle. It gets over to d and then from d we follow the rest of the path P. So,  actually, in this example we're backtracking. So this doesn't actually give us a path, it gives us a walk. But it shows that y and z are connected in T prime and then, of course, we can truncate this walk to make a path.</p>
<p>Our whole point is to show that y and z are connected and that's all we need to show. We've shown that's the case for any y and z therefore, T' is connected and since it has exactly n-1 edges then T' is in fact a tree.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="P2-Proof-T-is-an-MST">P2-Proof T is an MST<a class="anchor-link" href="#P2-Proof-T-is-an-MST">¶</a></h3><p>Now we want to show that it's a minimum spanning tree. How do we show it's of minimum weight? Well we know that T is an MST,  so we know that the weight of T is a minimum over all trees. So to show that T' is a minimum spanning tree, we have to show that the weight of T' equals the weight of T. What do we know about the weight of $e^*$ compared to that of e prime? Well, we know that the weight of $e^*$ is minimum over all edges cross the cut, e' also crosses the cut from $S$ to $\bar{S}$ therefore, $w(e^*) \le w(e')$.</p>
<p>Now what is the weight of T'? It's $w(T') = w(T) + w(e^*) - w(e')$. With $w(e^*)$ at most the $w(e')$ so this part is almost zero. So this whole thing is at most the weight of T, $w(T') \le w(T)$. So we've shown that w(T') is at most w(T), and because T is minimum weight, therefore T' is also a minimum weight and in fact, the weight of T' must equal the weight of T. So the weight of $e^*$ must equal the weight of e'. Otherwise we retrain the new tree T prime which is strictly smaller weight than t,
 which would contradict the fact that T is an MST. So all these edges of T which cross this cut, must be of exactly the same weight as e star otherwise, we can obtain a new tree which is of smaller weight. That completes the proof of the cut property.</p>
<p>Now the key idea that I want to stress, is that I can take a tree T and I add an edge into that tree so I take T union e star that creates a cycle. Now I can remove any edge of that cycle and I get a new tree T prime. Now that's the idea that I wanted to get from the proof of the cut property. The other idea I want you to get, is the idea from the statement of the cut property. From the statement of the cut property, I want you to get the idea that a minimum weight edge across the cut is part of a MST. Those are the two key ideas I want you to get from the statement of the cut property and the proof idea of the cut property.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Prim's-Algorithm">Prim's Algorithm<a class="anchor-link" href="#Prim's-Algorithm">¶</a></h3><p>An alternative to kruskal's algorithm is Prim's, in which the intermediate set of edges X always forms a subtree. In contrast to kruskal, Prim is focused on the vertices rather than the edges. It begins at a root vertex r, looks for the lightest edge starting at r, adds it, then takes the end point as the new root. It proceeds in this manner to by growing a single tree. Kruskal's approach grows multiple trees, a forest, and by the end converges to an MST</p>
<pre><code>Prim(G):
Input   Undirected graph G=(V,E) with weights w(e)

Set X = null           # edges selected so far
while |X| != |V| - 1
    pick a set S in V for which X has no edges between S and V-S
    let e in E be the minimum weight edge between S and V-S
    add e to X</code></pre>
<p>Use the cut property to prove Prim's algorithm, This is an excercise left to the reader.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Kruskal-v-Prim">Kruskal v Prim<a class="anchor-link" href="#Kruskal-v-Prim">¶</a></h3><table>
<thead><tr>
<th style="text-align:left">Num</th>
<th style="text-align:left">Prim’s Algorithm</th>
<th style="text-align:left">Kruskal’s Algorithm</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">This algorithm begins to construct the shortest spanning tree from any vertex in the graph.</td>
<td style="text-align:left">This algorithm begins to construct the shortest spanning tree from the vertex having the lowest weight in the graph.</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">To obtain the minimum distance, it traverses one node more than one time.</td>
<td style="text-align:left">It crosses one node only one time.</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">The time complexity of Prim’s algorithm is O(E log V)</td>
<td style="text-align:left">The time complexity of Kruskal’s algorithm is O(E log V).</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">In Prim’s algorithm, all the graph elements must be connected.</td>
<td style="text-align:left">Kruskal’s algorithm may have disconnected graphs.</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">When it comes to dense graphs, the Prim’s algorithm runs faster.</td>
<td style="text-align:left">When it comes to sparse graphs, Kruskal’s algorithm runs faster.</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">It prefers list data structure.</td>
<td style="text-align:left">It prefers the heap data structure.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="MF0:-Max-Flow-Problem">MF0: Max Flow Problem<a class="anchor-link" href="#MF0:-Max-Flow-Problem">¶</a></h2><p>We will now turn our attention to a well known problem known as the max flow problem. It's a quite useful problem. We'll first look at basic algorithm for the problem, known as a Ford Fulkerson Algorithm, and then we'll prove correctness of the Ford Fulkerson Algorithm. And along the way, we'll get the classic result known as the Max-flow- min-cut theorem. Next, we'll look at the nice application of the Max-flow problem to computer vision known as the Image Segmentation problem. We'll also look at a more general version of the problem,  Max-flow with additional demand constraints,  and we'll see how to reduce this more general problem to the original Max-flow problem. Finally, we'll see a faster Algorithm for the Max-flow problem, known as the Edmonds-Karp Algorithm.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="MF1:-Ford-Fulkerson-Algorithm">MF1: Ford-Fulkerson Algorithm<a class="anchor-link" href="#MF1:-Ford-Fulkerson-Algorithm">¶</a></h2><p>Overview of topics</p>
<ul>
<li>Ford-Fulkerson algo</li>
<li>Edmonds-Karp algo</li>
<li>Max-Flow = Min-Cut Theorem</li>
<li>Application <ul>
<li>Image segmentation</li>
</ul>
</li>
</ul>
<p>Problem Setup: Sending a supply from a vertex s to a destination t. This supply could be internet traffic, electricity, or it may be products. In all three examples there is an s and t, which represent the starting and end points. There is also a medium with a capacity. An electrical wire has a maximum capcity, overload it and sparks fly. A sewage pipe can only carry so much before it backs up, the road used by a delivery truck has a max flow, go over and you get a gridlock.</p>
<p>We can model each of these situations with a network graph like the following, with a designated start s, and end t. We also add some capacity weights to better reflect our constraint. Clearly capacity is always positive .... would a black hole have negative capacity? Food for thought.
<img src="CS6515_images/MF1-001.png" width="400;"/></p>
<p>Formally: A <strong>Flow Network</strong> consists of</p>
<ul>
<li>a directed graph G=(V,E), </li>
<li>source and sink vertices, we will call these s,t </li>
<li>For each edge $e \in E$, a capacity $c_e &gt; 0$</li>
</ul>
<p>There seems to be some slight differences in nomenclature around the definition of a flow network. The book Into to algorithms aka CLRS add a constraint that if $(u,v) \in E$ then we cannot also have $(v,u) \in E$. Wikipedia on the other hand doesn't make this distinction</p>
<p>Our goal</p>
<ul>
<li>Let $f_e$ be the flow along edge e</li>
<li>then maximize the flow that travels from s to t</li>
</ul>
<p>Now we formalize the max flow problem</p>
<p>Given a flow network,</p>
<ul>
<li>a directed graph G=(V,E), </li>
<li>start and end vertices $s,t \in V$</li>
<li>and capacities $c_e \gt 0$ for $e \in E$</li>
</ul>
<p>Goal</p>
<ul>
<li>Find flows $f_e$ for $e \in E$ </li>
<li>under the constraints<ul>
<li>Capacity constraint, the edge flow cannot exceed it's capacity<ul>
<li>$\forall e \in E, 0 \le f_e \le c_e$ </li>
</ul>
</li>
<li>Conservation of flow, each node must have an out flow equal to it's inflow<ul>
<li>$\forall v \in V - \{ s \cup t \}$ flow-in to v = flow out of v</li>
<li>$\sum f_{wv}$ over $\overset{\rightarrow}{wv} \in E$ </li>
<li>is equal to $\sum f_{vz}$ over $\overset{\rightarrow}{vz} \in E$</li>
</ul>
</li>
</ul>
</li>
<li>Find a valid flow of maximum size        <ul>
<li>where size of flow is the total flow sent        </li>
<li>size(f) = flow out of s = flow-in to t</li>
</ul>
</li>
</ul>
<p>Consider the following flow network, the numbers in red denote the capacity of that edge. 
<img src="CS6515_images/MF1-002.png" width="500;"/></p>
<p>No bones about it ... this is a tedious and error prone computation to try by hand. To make matters worse there is no 1 right answer in this case. There are three here and they all end up with the output of 5 and 7 to t. This gives a size(f) of 12.</p>
<p><img src="CS6515_images/MF1-003.png" width="500;"/></p>
<p>Notice also that there is a cycle in this graph, consider c -&gt; f -&gt; e -&gt; d -&gt; c. Recall that in some situations cycles can cause problems. When looking at a flow problem like this it makes no sense to use the cycle for the simple reason that by passing it will produce the same output. It can't gain flow, recall the conservation of flow constraint. Gaining would violate this. Anyhoo, the point is that cycles are ok, and in fact help simplify the problem in some cases. However, the problem is well defined regardless of the existance of cycles.</p>
<p><strong>Anti-Parallel Edges</strong><br/>
Roughly speaking Anti-parallel edges are edges in the opposite direction between the same two vertices. In general this sort of event is not allowed in a flow network. Generally a flow network direction is like a one way street.</p>
<p>They deserve a special call out as handling them, replacing, becomes necassary in many cases. In this situation we will want to insert a vertex at some point along one of the edges. However, we must retain the characteristic of the original network intact so both the new edges will have the same capacity as the old edge.</p>
<p>In this image we can see a simple flow network with anti-parallel edges on the left. The network on the right is after they've been handled using the process above. 
<img src="CS6515_images/MF1-004.png" width="500;"/></p>
<p><strong>Toy Example</strong><br/>
Consider the following toy example which we will use to help visualize our algorithms.<br/>
<img src="CS6515_images/MF1-005.png" width="500;"/></p>
<p>We will work through an iteration of a simple algorithm</p>
<ol>
<li><p>we begin with three graphs, our input, our flow initialized to 0, and our availability
<img src="CS6515_images/MF1-006.png" width="500;"/></p>
</li>
<li><p>our next step is to find an path P from s to t with available flow. So we choose one, highlighted in green in our availability graph in the middle.</p>
</li>
</ol>
<ul>
<li>find st-path p with available capacity 
<img src="CS6515_images/MF1-007.png" width="500;"/></li>
</ul>
<ol>
<li>We will also need to calculate the capacity along this edge. </li>
</ol>
<ul>
<li>Let $c(P) = \underset{e \in p}{min} (c_e - f_e)$</li>
<li>which does nothing to our numbers but it gives us the max flow along the path p</li>
</ul>
<ol>
<li>Augment our flow by c(p) along p
<img src="CS6515_images/MF1-008.png" width="500;"/></li>
</ol>
<p>now all we need to do is repeat by going back to 2. But this time we will need to update our available capacity in order to reflect the augmentation in step 4. 
<img src="CS6515_images/MF1-009.png" width="500;"/></p>
<p>Of course this being a toy example we guessed the path on the first try.</p>
<p>Let's summarize our approach</p>
<ul>
<li>1-Start with $f_e = 0, \forall e \in E $</li>
<li>2-Find st-path p with available capacity</li>
<li>3-Let $c(P) = \underset{e \in p}{min} (c_e - f_e)$</li>
<li>4-Augment f by c(P) along P</li>
<li>5-Go back to 2 and repeat <ul>
<li>until there is no longer anymore st-paths with available capacity</li>
</ul>
</li>
</ul>
<p>Let's consider the last, and final, step from our example above.<br/>
<img src="CS6515_images/MF1-009.png" width="500;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Residual-Networks">Residual Networks<a class="anchor-link" href="#Residual-Networks">¶</a></h3><p>You'll notice that we still have some capacity along two of the edges. In particular along (s,b) we have 7 and (a,t) also has 7 available. So we want to augment our flow, but to do so we would need an path s -&gt; b -&gt; a -&gt; t. we have a path from s -&gt; b and we have an edge from a -&gt; t. But we're missing an edge from b -&gt; a.</p>
<p>This begs the question what does it mean to augment a path? Even more so when there is no such edge. In this case the available capacities illustrated by the middle graph is not properly representing everything that's possible given the flow graph, far right. In particular because there is no edge from b-&gt;a.</p>
<p>So we will augment it by adding an edge from b-&gt;a, with available capacity 10, similar to the edge a-&gt;b. This new edge is limited not by the capacity in the input graph but by the flow graph at the far right. The flow along this new edge would offset the flow along a-&gt;b. This is now officially referred to as the residual network.</p>
<p><img src="CS6515_images/MF1-010.png" width="500;"/></p>
<p>Now we update our flow and look at what happens! It's size has increased from 10, to 17! Pretty weird, eh?
<img src="CS6515_images/MF1-011.png" width="500;"/></p>
<p>Let's now give a more formal definition of a residual network,</p>
<p>Let $G^f = (V,E^f)$ denote a residual flow network</p>
<ul>
<li>for a flow network $G=(V,E)$ with $c_e$ for $e \in E$</li>
<li>and flow $f_e$ for $e \in E$</li>
</ul>
<p>Constructing $G^f = (V,E^f)$ is somewhat straight forward</p>
<ul>
<li>if $\overset{\rightarrow}{vw} \in E$ and $f_{vw} \lt c_{vw}$<ul>
<li>then add $\overset{\rightarrow}{vw}$ to $G^f$ with capacity $c_{vw} - f_{vw}$</li>
</ul>
</li>
<li>if $\overset{\rightarrow}{vw} \in E$ and $f_{vw} \gt 0$<ul>
<li>then add $\overset{\rightarrow}{ww}$ to $G^f$ with capacity $f_{vw}$</li>
<li>this is a backward edge like the example above</li>
</ul>
</li>
</ul>
<p>This is pretty wordy so here's a simple mathematical case function that says the same thing</p>
<p><strong>Residual Capacity:</strong><br/>
Given a flow $f$ over $G$<br/>
$C_f(u,v) = 
\begin{cases}
C(u,v) - f(u,v) &amp; if (u,v) \in E   \\
f(u,v) &amp; if (v,u) \in E   \\
0 &amp; otherwise   \\
\end{cases}$</p>
<h3 id="Ford-Fulkerson-Algo">Ford Fulkerson Algo<a class="anchor-link" href="#Ford-Fulkerson-Algo">¶</a></h3>
<pre><code>1. set f_e = 0 for all e in E

2. Build the residual network G^f for the current flow f

3. Check for st-path P in G^f
    - if no st-path then output(f)
    - (NB this can be done using either DFS or BFS)

4. Given P, 
    let c(P) = min capacity along P in G^f

5. Augment f by c(P) units along P
    - Increase the flow of every edge in this path by the same amount. 
    - The amount is the largest amount that will not cause one of the capacities to be exceeded.

6. Repeat until no such st-path exists</code></pre>
<p><strong>Correctness</strong>: Follows from the max-flow = min-cut theorem, to be shown &amp; proven in the next section</p>
<p><strong>Running Time</strong>: Assume all capacities are integers, then the flow will augment/increase by $\ge 1$ unit per round of ford-fulkerson. So if C=size of max flow then we should have a worst case of C rounds. So now we are left to determine the runtime of the algo body, steps 2 to 5 inclusive.</p>
<p>Step 1: Building the residual network takes O(n) times as the path P is composed of at most n-1 edges.<br/>
Step 2: Requires running DFS or BFS, both of whom require O($|V|+|E|$) which is bounded by O($|E|$)<br/>
Step 5: Also requires O($|V|$)</p>
<p>Step 2 dominates the runtimes, what may not be so obvious here is that the number of rounds is C, where C is the max flow. Why? Because the algo will keep incrementing the flow until it hits this capacity which is bounded by the max flow. Thus putting these together yields a total runtime of O(C*|E|) where C is the size of the max flow. recall that such a runtime is described as pseudo-polynomial.</p>
<p>This is a bit problematic though. First it assumes integer capacities, which may not always be true. It also says that the run time depends on the output, the max flow, which in turn depends on the size of the capacities of the input.</p>
<p>This means that the runtime is what's often called pseudo-polynomial. In the next section we will discuss the Edmonds-Karp algorithm. It is very similar to Ford-Fulkerson, except rather than looking at all paths it looks for the shortest path. It's run time does not depend on the max flow and is much more stable: O($m^2 n$). It also relaxes the integer capacity requirement.</p>
<p>In 2013 Orlin came up with an algorithm that runs in just O($m n$) time, which is currently the top of the class. This however is beyond the scope of this class.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="MF2-Max-Flow-Min-Cut">MF2 Max-Flow Min-Cut<a class="anchor-link" href="#MF2-Max-Flow-Min-Cut">¶</a></h2><p>Goal 1: Proof of the Max-Flow=Min-Cut theorem</p>
<ul>
<li>plus application to image segmentation        </li>
</ul>
<p>Goal 2: Ford-Fulkerson correctness proof, alongside Edmonds-Karp</p>
<p>Recall the ford-fulkerson algorithm:<br/>
INPUT: Flow network, directed G=(V,E) with source and sink ($s,t \in V$) &amp; capcities $c_e &gt; 0$<br/>
OUTPUT: flow $f^*$ of max size, ($size(f) = f_{out}(s) = f_{in}(t)$)</p>
<p>When does ford-fulkerson stop running? When there are no remaining augmenting path in residual graph of G, $G^{f^*}$</p>
<p><strong>Lemma</strong> For a flow $f^*$ if there is no augmenting path in $G^{f^*}$ then $f^*$ is a max-flow.</p>
<p>Consider: Given a flow network and a flow f, what is the time required to check if f is a max flow?<br/>
Think about what it would take to perform this check. First you'll need to build a residual graph, then you'll need to check for an st path with available capacity. In the worst case this second step would require O(|E|) time as each edge needs checking.</p>
<h3 id="MaxFlow---MinCut-Theorem">MaxFlow - MinCut Theorem<a class="anchor-link" href="#MaxFlow---MinCut-Theorem">¶</a></h3><p><strong>First</strong> we need to define what a MinCut is. A cut is a partition of V into two sets ($V=L \cup R$). Here L is for left and R is for right. Further define an st-cut as a cut where $s \in L$ and $t \in R$.</p>
<p>We will also need to define the capacity of the cut, ie the capacity from L to R,:
$$\large capacity(L,R) = \sum_{\overset{\rightarrow}{vw} \in E} c_{vw}$$
Note that $v \in L$ and $w \in R$. We are NOT concerned about edges that go from R to V. Do you know why? It's because the flow goes from the source in L to the sink in R. edges that flow from R to L wouldn't benefit us unless there was a cycle, and you'll recall that a flow network may contain cycles, but it makes no sense to use them due to the conservation of flow/energy.</p>
<p>Note</p>
<ul>
<li>a cut is any partition of vertices, it doesn't care about the edges</li>
<li>a cut not always a simple linear partition. It may be, but it doesn't have to be. </li>
<li>a cut may result in disconnected vertices</li>
<li>see image below</li>
</ul>
<p><img src="CS6515_images/MF2-001.png" width="500;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Min-ST-cut-Problem-Formulation">Min ST-cut Problem Formulation<a class="anchor-link" href="#Min-ST-cut-Problem-Formulation">¶</a></h3><p>INPUT: a Flow network, directed graph with source and end vertex (s,t), a positive capacities for all edges<br/>
OUTPUT: st-cut(L,R) with minimum capacity</p>
<p>Let's consider our flow network from before, here is one possible st-cut. We can sum up the edges leaving L to see that the st-cut has a capacity of 27=(8+2+2+5+3+7). Seems a bit high no?
<img src="CS6515_images/MF2-002.png" width="500;"/></p>
<p>Looking at the flow network graph we know that we can do better than 27. I'm sure you can see where this is going. Consider the following st-cut, with a capacity of 12, which just so happens to be the max flow: 
<img src="CS6515_images/MF2-003.png" width="500;"/></p>
<p>Do you think it would be possible to go lower than this? I hope not because what we've just illustrated is the MaxFlow MinCut theorem.</p>
<p><strong>$\color{red}{\text{MaxFlow-MinCut Theorem}}$</strong> For any flow network the size of the max flow is the minimum capacity of an st-cut.</p>
<p>Sketch of proof: To prove equality we prove (1) max flow is at most the min st-cut, and (2) max flow is at least the min st-cut. Put together these will prove equality.</p>
<p>To prove (1) We will see that for any flow f and any st-cut(L,R), $size(f) \le capacity(L,R)$. Of course the max flow is one of these flows and similarly the capacity(L,R) could be the min st-cut capacity, so our result follows directly from this. In fact so does (2). The general case proves that the max flow is "at most"</p>
<p>Let's recycle our example from before to develop our intuition
<img src="CS6515_images/MF2-002.png" width="500;"/></p>
<p>The flow must go from s to t, and s is in L. Further, we can say that the flow leaving L is at most the sum of the capacities of the edges from L to R. The flow cannot escape L any other way! The total capacities of these edges is 27, so the total possible flow is 27, so the flow is bounded by 27.</p>
<p>Formally, we claim that $size(f) = f_{out}(L) - f_{in}(L)$ (this is a NOT a typo)</p>
<ul>
<li>$\large f_{out}(L) = \sum_{\overset{\rightarrow}{vw} \in E} f_{vw}$<ul>
<li>where $\overset{\rightarrow}{vw} \in E$ is the edges leaving L only</li>
<li>this is the flow out of L</li>
</ul>
</li>
<li>$\large f_{in}(L) = \sum_{\overset{\rightarrow}{wv} \in E} f_{wv}$<ul>
<li>where $\overset{\rightarrow}{wv} \in E$ is the edges into L only</li>
<li>This is the flow into L</li>
</ul>
</li>
<li>Then we add a couple of terms to help us     <ul>
<li>$\sum_{\overset{\rightarrow}{vw} \in E} f_{vw}$ for $v,w \in L$</li>
<li>$\sum_{\overset{\rightarrow}{wv} \in E} f_{vw}$ for $w,v \in L$</li>
<li>these are edges that remain in L, neither an inflow nor an outflow</li>
</ul>
</li>
<li>Consider the positive terms(1&amp;3), ie the sum of flows from L to R + sum of flows that stay in L<ul>
<li>together the form the total outflow of all vertices v in L $\sum f_{out} \; v \in L$</li>
</ul>
</li>
<li>similarly adding the negative terms(2&amp;4)    <ul>
<li>yields the total inflow into L</li>
</ul>
</li>
<li>recall that the source vertex, s, is in L, it has an inflow of 0, so we can safely remove it<ul>
<li>but we need to keep it's outflow</li>
<li>so we take it outside</li>
</ul>
</li>
<li>this results in the summation of $f_{out} - f_{in}$ over the same vertices<ul>
<li>which we know is 0 due to the conservation of energy</li>
</ul>
</li>
<li>This leaves us with $f_{out}(s)$ which we know is the size of the flow </li>
<li>This ends the proof of  $size(f) = f_{out}(L) - f_{in}(L)$
<img src="CS6515_images/MF2-004.png" width="500;"/></li>
</ul>
<p>Now we want to demonstrate that $size(f) \le capacity(L,R)$<br/>
From above we observe that $size(f) = f_{out}(L) - f_{in}(L)$<br/>
This implies that $f_{out}(L) - f_{in}(L) \le f_{out}(L)$<br/>
But the flow out of L can't exceed the capacity of the cut(L,R)<br/>
hence $f_{out}(L) - f_{in}(L) \le f_{out}(L) \le capacity(L,R)$</p>
<p>This proves that $size(f) \le capacity(L,R)$ and thus max flow $\le$ min st-cut capacity</p>
<p>Now we must prove max flow $\ge$ min st-cut capacity as well. together they prove equality.</p>
<p>Take the flow $f^*$ provided from ford-fulkerson. We know for a fact that $f^*$ cannot have any st-paths in the residual graph $G^{f^*}$. So we'll construct an (L,R) where $size(f^*) = capacity(L,R)$. This implies the goal of our proof, since</p>
$$\underset{f}{max} size(f) \ge size(f^*) = capacity(L,R) \ge \underset{L,R}{max} capacity(L,R)$$<p><strong>Proof</strong></p>
<ul>
<li>take a flow $f^*$ with no st-path in the residual $G^{f^*}$<ul>
<li>t/f t is not reachable from s in $G^{f^*}$</li>
<li>t/f they can be seperated </li>
</ul>
</li>
<li>so let L = vertices reachable from s in $G^{f^*}$<ul>
<li>clearly t cannot be in L </li>
<li>let R = V - L, be the remaining vertices, t will be in R, but s will not be    </li>
</ul>
</li>
<li>we now have now partitioned V into an st-cut!</li>
<li>Now we need to prove that the capacity of this cut is equal to the $size(f^*)$</li>
</ul>
<h3 id="Properties">Properties<a class="anchor-link" href="#Properties">¶</a></h3><p><img src="CS6515_images/MF2-005.png" width="500;"/></p>
<p>But first we will need some properties of cuts so that we may conclude our proof. Consider the above image: At left we have a flow network with capacities in red, and a max flow in green. On the right we have a residual graph $G^{f^*}$</p>
<p>Consider some observations:</p>
<ul>
<li>edge (e-&gt;d) in the original graph<ul>
<li>has positive flow, so the inverse edge (d-&gt;e) appears in the residual </li>
<li>has spare capacity, so the original edge (e-&gt;d) appears in the residual as well</li>
</ul>
</li>
<li>edge (f-&gt;e)    <ul>
<li>has no flow in the original, so there is no inverse</li>
<li>has spare capacity so the original appears in residual</li>
</ul>
</li>
<li>edge(c-&gt;f)    <ul>
<li>has no spare capacity thus the residual only has an inverse</li>
</ul>
</li>
</ul>
<p>Recall that we defined L as the set of vertices reachable from s in $G^{f^*}$, we've circled these in purple. The reader is encouraged to verify this. The remaining vertices are defined as R.</p>
<p>Now let's consider the edges which have potential outflows: (d,t),(d,c),(s,c) in the original graph. You'll notice that none of these edges appear in the residual graph. In fact their inverse appears, as they have no spare capacity. Furthermore, they cannot appear. If they did then there would be a path to t violating our assumption $f^*$ has no st-path in $G^{f^*}$. What this tells us is that the flow along these edges must be fully capacitated, ie saturated. hence the flow out of L is simply the capacity of L -&gt; R
$$\forall \overset{\rightarrow}{vw} \in E, v\in L, w\in R $$<br/>
$$ f_{vw}^* = c_{vw} \&amp; f^{*out}(L) = capacity(L,R) $$</p>
<p>Now let's consider the edges going from R to L, in our original graph<br/>
$$\forall \overset{\rightarrow}{zy} \in E, z\in R, y\in L $$<br/>
$$ f_{zy}^* = 0 \&amp; f^{*out}(L) = 0 $$</p>
<p>We can put these together to get<br/>
$$ size(f^*) = f^{*out}(L) - f^{*in}(L) = capacity(L,R) - 0 $$
which is what we wanted</p>
<p>so Let's review</p>
<ul>
<li>We wanted to show max size(f) $\ge$ min cap(L,R)</li>
<li>we took $f^*$ from ford fulkerson algo, <ul>
<li>which has no st-path in  $G^{f^*}$</li>
<li>which also means there is no augmenting path from s to t    </li>
</ul>
</li>
<li>we used this to construct (L,R) <ul>
<li>where L is the vertices reachable from s, and cannot reach t, which is in R</li>
<li>making it a valid st-cut</li>
</ul>
</li>
<li>we proved that the $size(f^*) = cap(L,R)$</li>
</ul>
<p>What this also does is prove</p>
<ul>
<li>our earlier lemma: For a flow $f^*$ if there is no augmenting path in $G^{f^*}$ then $f^*$ is a max-flow.</li>
<li>the correctness of ford-fulkerson algorithm</li>
</ul>
<p>and it shows us how to create a min st-cut.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="MF4-Edmonds-Karp-Algorithm">MF4 Edmonds-Karp Algorithm<a class="anchor-link" href="#MF4-Edmonds-Karp-Algorithm">¶</a></h2><p>Previously we examined the Ford-Fulkerson algorithm and in this section we turn our attention to the Edmonds-karp algorithm.</p>
<p>Let's take a look at the key take-aways from each algorithm</p>
<ul>
<li>Ford Fulkerson<ul>
<li>finds augmenting paths using eith dfs or bfs</li>
<li>runtime: O(mC) where C = size of the max flow</li>
<li>assumes integers capacities</li>
</ul>
</li>
<li>Edmonds-Karp<ul>
<li>Finds augmenting paths using BFS only</li>
<li>runtime: $O(m^2 n)$</li>
<li>assumes nothing about capacities</li>
</ul>
</li>
</ul>
<p>In some ways EK is an implementation of FF, but with a stronger guarantee of runtime and relaxed assumptions. As a result fo this their algorithms are highly similar.</p>
<p>Recall The ford fulkerson algo</p>
<p><strong>Input</strong> Flow network G=(V,E) with integer capacities c(e) &gt; 0</p>
<ul>
<li><ol>
<li>Initialize $f_e = 0; \forall e \in E$ </li>
</ol>
</li>
<li><ol>
<li>Build residual network $G^f$</li>
</ol>
</li>
<li><ol>
<li>Check for st-path P in $G^f$ (use DFS or BFS)<ul>
<li>if no path: return(f)</li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>Let c(p) = min capacity along P in $G^f$</li>
</ol>
</li>
<li><ol>
<li>Augment f by c(P) units along P</li>
</ol>
</li>
<li><ol>
<li>Repeat (goto 2)<ul>
<li>until no st-path in $G^f$</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="Edmonds-Karp">Edmonds Karp<a class="anchor-link" href="#Edmonds-Karp">¶</a></h3><p><strong>$\color{red}{\text{Edmonds Karp}}$</strong> We've bold the tweaks we've made</p>
<p><strong>Input</strong> Flow network G=(V,E) with capacities c(e) &gt; 0 <strong>Integers REMOVED</strong></p>
<ul>
<li><ol>
<li>Initialize $f_e = 0; \forall e \in E$ </li>
</ol>
</li>
<li><ol>
<li>Build residual network $G^f$</li>
</ol>
</li>
<li><ol>
<li>Check for st-path P in $G^f$ (use BFS) <strong>DFS REMOVED</strong> <ul>
<li>if no path: return(f)</li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>Let c(p) = min capacity along P in $G^f$</li>
</ol>
</li>
<li><ol>
<li>Augment f by c(P) units along P</li>
</ol>
</li>
<li><ol>
<li>Repeat (goto 2)<ul>
<li>until no st-path in $G^f$</li>
</ul>
</li>
</ol>
</li>
</ul>
<p>Observations</p>
<ul>
<li>at least one edge will change in each round/iteration. </li>
<li>We will keep incrementing/augmenting until full capacity is reached along one of the edges</li>
<li>once full capacity is reached along an edge it will be removed from the residual graph in the next round<ul>
<li>others may be removed as well </li>
<li>but at least 1 edge is removed in each stage</li>
</ul>
</li>
</ul>
<p>We want to prove that the running time is $O(m^2 n)$, To prove this we show that the number of round is $\le mn$, which is also the number of augmentations. In each round we call on BFS which you may recall has a linear runtime of O(m+n). So if we assume the number of edges, m, is at least the number of vertices, n, we can simplify BFS to O(m). Thus making the total run time $O(m^2 n)$</p>
<p>Recall that we stated that in every round the residual graph changes, and in particular at least 1 edge is deleted. The edge that is deleted corresponds to the one with the minimum capacity along the augmenting path. In each round some edge(s) are deleted, and some may even be (re)inserted. This leads us to the following lemma</p>
<p><strong>Lemma</strong>: For every edge e, e is deleted and reinserted later $\le n/2$ times, ie "at most" n/2 times</p>
<p>Since every edge is deleted at most n/2 times, and we have m edges, we can say that there is at most m*(n/2) total rounds. This proves the number of rounds is at most m*n, and the runtime of $O(m^2 n)$ follows. Now we need to prove our lemma.</p>
<p>First we'll need a quick review of some basic properties of BFS</p>
<ul>
<li>Input: Directed graph G=(V,E), start vertex $s \in V$, weights ignored</li>
<li>outputs: dist(v) = min number of edges from s to v, for all $v \in V$</li>
<li>another way to think of the dist array is as a set of levels, see illustration below<ul>
<li>each new frontier creates a new level of sorts, </li>
<li>where a new level is roughly defined as the min number of edges to that vertice</li>
</ul>
</li>
<li>let level(v) = dist(v)<ul>
<li>the level cannot go up by more than 1 at each edge</li>
<li>similarly the level cannot stay the same nor can it go down along an edge<ul>
<li>going down would imply that there is a shorter path      </li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="CS6515_images/MF4-001.png" width="500;"/></p>
<p>The path from s to t that is found by BFS, is going to be the path where the level does up by 1 at every edge. This will be important to the proof of the lemma as we will look at how the level(z) changes as the residual graph $G^f$ changes.</p>
<p>How does level(z) change as $G^f$ changes? Consider removing the edge a-&gt;d in the graph above. Upon removal the level of d would become 3, as it is no longer reachable in 2 steps. You might be wondering if inserting would decrease the level, you would be correct in this, except this will never happen. What we will see is that for every $z \in V$, level(z) does not decrease. It may increase or remain the same, but it will never decrease. To better understand why we will need to consider which edges and inserted and which are removed.</p>
<p>Consider an edge in the original flow $\overset{\rightarrow}{vw} \in E$<br/>
There are 4 possible cases to consider</p>
<p>When would we add (vw) to the residual graph $G^f$?</p>
<ul>
<li>if the flow was full, and then reduced, meaning there is left over capacity<ul>
<li>a reduction would occur when the back edge was in the augmenting path, ie $(wv) \in P$</li>
</ul>
</li>
</ul>
<p>When would we remove (vw) from the residual graph $G^f$?</p>
<ul>
<li>this would occur when the flow is full, ie the flow reaches it's capacity<ul>
<li>meaning we augmented the path along the forward edge vw, ie $(vw) \in P$</li>
</ul>
</li>
<li>so in order to remove vw from $G^f$ then the edge vw must be on the augmenting path, because we increased the flow</li>
</ul>
<p>When would we add the back edge (wv) to the residual graph $G^f$?</p>
<ul>
<li>This would happen when the flow was empty and we increased the flow along the forward edge, ie $(vw) \in P$</li>
<li>in order to increase the flow it must have been on the augmenting path</li>
</ul>
<p>When would we remove the back edge (wv) from the residual graph $G^f$?</p>
<ul>
<li>this would happen if the flow was positive and is now empty<ul>
<li>this would imply that the flow was decreased along the forward edge from v-&gt;w</li>
<li>to decrease the flow along the forward edge, we send flow along the backwards edge</li>
<li>meaning the backedge must be on the augmenting path P, $(wv) \in P$</li>
</ul>
</li>
</ul>
<p>We can see a pattern in the above cases</p>
<ul>
<li>adding an edge happens when the edge in the opposite edge is on the augmenting path<ul>
<li>if add (yz) to $G^f$ then $(zy) \in P$</li>
</ul>
</li>
<li>removing an edge     <ul>
<li>if remove (yz) from $G^f$ then $(yz) \in P$</li>
</ul>
</li>
</ul>
<p>Now that we know how and when edges are added and removed, we can prove our lemma from earlier:<br/>
<strong>Lemma/Claim</strong> For every $z \in V$, level(z) does not decrease</p>
<p>A level might decrease if we add an edge (yz) that gives a shorter path. We ignore the case for removing an edge as this cannot decrease the number the path. Suppose level(z) = i and we add (yz) to $G^f$, that would mean that the reverse edge is along the augmenting path, ie $(zy)\in P$. We can also say that (zy) is on a BFS path. Accordingly BFS would say that level(y)=level(z)+1=i+1. the level(z) has not decreased despite adding an edge from a higher to lower level. which completes the proof of the lemma/claim.</p>
<p><strong>Delete/Add Effect</strong><br/>
Take level(v) = i</p>
<p>Suppose we delete $\overset{\rightarrow}{vw}$ from $G^f$</p>
<ul>
<li>then $\overset{\rightarrow}{vw} \in P$ <ul>
<li>which implies level(w) = level(v) + 1 $\ge$ i + 1</li>
</ul>
</li>
</ul>
<p>Suppose later in the algorithm we add it back, $\overset{\rightarrow}{vw}$ into $G^f$</p>
<ul>
<li>then $\overset{\rightarrow}{wv} \in P$ <ul>
<li>which implies level(v) = level(w) + 1 $\ge$ i + 2</li>
</ul>
</li>
</ul>
<p>Now look at what has happened. v began at level i. After being removed and reinserted back it has increased 2 levels.</p>
<p>For the final summary think of the min and max levels, This is of course 0 and n respectively. If n is the max and each delete+insert costs 2 units then we will be limited to n/2 (delete+insert) operations. Since there is m edges in the graph, this proves the main result that there is at most nm rounds in the edmond karp algorithm</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="MF5:-Max-Flow-Generalized">MF5: Max-Flow Generalized<a class="anchor-link" href="#MF5:-Max-Flow-Generalized">¶</a></h2><h3 id="Demand-Constraints">Demand Constraints<a class="anchor-link" href="#Demand-Constraints">¶</a></h3><p>Input: Flow network, directed graph G(V,E), $s,t \in V$, c(e) &gt; 0 for all $e \in E$. Additionally we are given the demand constraint $d(e) \ge 0$ for $e \in E$<br/>
Goal: To find a feasible flow, where feasible is defined as: for $e \in E, \; d(e) \le f(e) \le c(e)$. Basically we need a valid flow, just as before, but we must also meet the demand expectations</p>
<p>This problem leads to two questions: first, is there a feasible flow? Second, given the feasibility flow, what is the max flow?<br/>
<strong>Example</strong><br/>
Let's re-use our example from previous sections. Numbers in green represent the demand and those in red represents the capacity. Is there a feasible flow for this example?
<img src="CS6515_images/MF5-001.png" width="500;"/></p>
<p>There is a feasible flow and it is tedious to determine
<img src="CS6515_images/MF5-002.png" width="500;"/></p>
<p>Algorithmically this is done by reducing the feasible flow problem to one of the max-flow. It's important to note that the feasible flow need NOT be a max flow, it just has to be feasible. Reduction implies that we will be able to make use of our max-flow algorithms without any alteration. We can modify the inputs of course, just as we did earlier to re-use DFS to find our strongly connected components. An added bonus to this approach is knowing the runtime of our algorithm, as well as their correctness.</p>
<p><img src="CS6515_images/MF5-003.png" width="500;"/></p>
<p>Above is a rough sketch of our approach</p>
<ul>
<li>We will use some algo g to construct G',c' to fit the inputs expected by our max-flow algo</li>
<li>We will get back a flow f' which we must translate back, using some algo h, into our original inputs</li>
<li>Our goal then is to find g &amp; h</li>
</ul>
<p><strong>Part 1</strong> Let's begin by looking at how to create G' and c', from the original inputs G,c, and d. Somehow we need to encode, or incorporate, the demand d into G and c to get G' and c'.</p>
<p>As is our usual approach to such problems let's sketch a simple graph to help us visualize the situation. In the image below our simple graph is on the left. The first number represents the demand, the second number is the capacity.<br/>
<img src="CS6515_images/MF5-004.png" width="500;"/></p>
<p>On the right side of the image we have a working prototype of G', we've taken the liberty of adding two new vertices s' and t'. It will become apprarent later why we've added these. For now it suffices to know these represent the start and end points of G'. Also notice that G' contains all the same vertices, and edges found in the original graph G.</p>
<p>What we will do is change the capacities so as to reflect the demands of the original problem. What we want to capture is that there is non-negative flow along an edge, f'(e) &gt; 0, if and only if we can construct a flow in the original graph G that meets, or exceeds, the demand required (1). What we will also want is that the flow in G' satisfies this new capacity constraint, c' &gt; f', iff the flow f satisfies the capacity constraint in the original network, c &gt; f (2).</p>
<p>(1) $f'(e) \ge 0 \iff f(e) \ge d(e) $ (A flow of 0 in G' should correspond to a flow of d in G)<br/>
(2) $c'(e) \ge f'(e) \iff c(e) \ge f(e)$<br/>
(3) $c'(e) = c(e) - d(e) $</p>
<p>In a manner these reflect a shifting of capacities, so we can formulate this as (3). Now consider how this reflects in the new graph. (sa) had a capacity of 10 and a demand of 3, t/f in G' we set (sa)' to capacity of 7. we've subtracted the demand of 3.</p>
<ul>
<li>ie $\forall e \in E$ add e to G' with c'(e) = c(e) - d(e)</li>
</ul>
<p>This results in the following Graph 
<img src="CS6515_images/MF5-005.png" width="500;"/></p>
<p>Our next step is to add some additional edges to G' to incorporate the start and end vertices, s' &amp; t'. This is a bit trickier to understand. We want a valid flow in G' to correspond to a feasible flow in the original G. Suppose that the flow is 0 in G'. For example f'(sa)=f'(ab)=f'(at) = 0. Then what does this say about those edges in G? According to (1) above this means that  each of these edges in G have a flow equal to their demand. But this could result in an invalid flow. In our example we would end up with a flow of 3 into a, and 7 out of a. This violates the conservation of flow and is t/f invalid. So we will use our new start and end vertices to remedy this situation. To do this we add an edge from s' to each vertice in G</p>
<ul>
<li>ie $\forall v \in V$ add s'-&gt;v to G' with c'(s',v) = d_into(v)<ul>
<li>Note that d_into(v) is the total demand for the vertex, it's not the demand of an edge.</li>
</ul>
</li>
<li>This gives us the ability to offset the flow, in order that we may conserve the flow</li>
</ul>
<p><img src="CS6515_images/MF5-006.png" width="500;"/></p>
<p>In a similar fashion we will also add an edge from each vertex to t'. Here we take the capacity to be the output demand of the vertex.</p>
<ul>
<li>ie $\forall v \in V$ add v-&gt;t' to G' with c'(v,t') = d_outof(v)</li>
</ul>
<p><img src="CS6515_images/MF5-007.png" width="500;"/></p>
<p>We're almost ready but there's a rather odd problem that is not so obvious. In G' s &amp; t are no longer the start and end vertices. By adding a new start and end we've changed them into internal vertices, meaning they are now subject to the conservation of flow requirement. recall that s,t were excluded from this requirement because it wouldn't make sense before. A start vertex cannot have an inflow, nor can and end vertex have an outflow. But now they can. No worries, we will add a back edge from t to s to remedy this situation. Furthermore, there are no constraints on this edge so we will assign it a capacity of $\infty$</p>
<p>Now finally we have our final graph.
<img src="CS6515_images/MF5-008.png" width="500;"/></p>
<p>So what is the max-flow of this new graph G'? What we can see that is that it will be bounded by the total capacities out of s' and the total into t'. Let's consider this to see where it takes us</p>
<p>first observe that $D = \sum_{e \in E} d(e) = \sum_{v \in V} d\_into(v) = \sum_{v \in V} d\_outof(v)$</p>
<ul>
<li>summing the demand along each edge is the same as summing the demand into a vertex, or summing out of each vertex</li>
<li>consequently<ul>
<li>$c'_{outof}(s') = \sum_{v \in V} d\_into(v) = D$</li>
<li>$c'_{into}(t') = \sum_{v \in V} d\_outof(v) = D$</li>
</ul>
</li>
</ul>
<p>This tells us quite a bit about the size of the flow in G', in fact for flow f' in G', size(f') $\le$ D. This is what is called saturating: F' is saturating if size(f') = D. And this means that the flow f' is of max size. This leads us to our next lemma.</p>
<h3 id="Feasible-iff-Saturating">Feasible iff Saturating<a class="anchor-link" href="#Feasible-iff-Saturating">¶</a></h3><p><strong>LEMMA</strong> G has a feasible flow $\iff$ G' has a saturating flow</p>
<p>This provides us with the direction to solving our original problem, which is to find a feasible max flow. We will create our G' as outlined above, we run max-flow on G', then we check if the the size of the max-flow is equal to D or not. if it is equal then it is saturating and by the lemma G has a feasible flow. In fact we will take f', aka the saturating flow, and construct a feasible flow for the original G from it.</p>
<p><strong>Proof of Lemma</strong><br/>
<strong>Part 1:</strong> saturating $\implies$ feasible</p>
<p>We will proceed by constructing a feasible flow f in G from the saturating flow f' in G'. recall that in the construction of G' we shifted the capacities of each edge by their demand d(e). So let's apply a similar logic to the flow. ie f'(e) = f(e) - d(e). Now we move the demand to the left side to get f'(e) + d(e) = f(e).</p>
<p>First let's check to see if f is feasible. It's trivial to see that if $f' \ge 0$ then $f \ge d$, t/f f is feasible. This proves feasiblility but let's consider the 2nd condition as well: $c \ge f$. Recall that this implies that</p>
<ul>
<li>$f(e) \le c'(e) + d(e)$</li>
<li>$f(e) \le (c(e) - d(e)) + d(e)$ sub the original eq of c'</li>
<li>$f(e) \le c(e)$ which is what we would expect   </li>
</ul>
<p>Not all that's left is to check that all inflows are equal to the outflows, ie the validity of the flow! To do this we need to prove that for any $v \in V$-{s,t}, $f_{in}(v) = f_{out}(v)$. We know that $f'_{in}(v) = f'_{out}(v)$ holds as it's the output of a max-flow algo. Let's look at $f'_{in}(v)$.</p>
<p>$\begin{split}
f'_{in}(v) &amp; = f'( \overset{\rightarrow}{s'v}) + \sum_{u \in V} f'(u-&gt;v) \\
&amp; = d_{in}(v) + \sum_{u \in V} (f(u-&gt;v) - d(u-&gt;v)) &amp; \text{ rewrite f' in terms of f } \\
&amp; = \sum_{u \in V} f(u-&gt;v) &amp; \text{ cancel out the demands as they both represent into v } \\
&amp; = f_{in}(v) &amp; \text{ summation of flows into v is just the inflow f\_in }
\end{split}$</p>
<p>What this tells us is that the flow into v in G' is the same as the flow into v in G. A similar arguement can be made for the outflow of v in both G' and G. Together they give us the original statement that we wanted to prove, namely $f_{in}(v) = f_{out}(v)$. t/f f is a valid flow as required.</p>
<p>In addition to proving the implication we've also shown how to construct the feasible flow</p>
<ul>
<li>Simply set f(e) = f'(e) + d(e)</li>
</ul>
<p>Now let's move to proving the inverse implication</p>
<p><strong>Part 2:</strong> feasible $\implies$ saturating<br/>
In a manner similar to previous we will assume a feasible flow f in G, and use it to construct a saturating flow f' in G'.</p>
<p>We begin by defining the flow f'(e) as a shifting of the original flow f(e) by d(e) units,</p>
<ul>
<li>for the original edges <ul>
<li>for all $e \in E$(the original G), let f'(e) = f(e) - d(e) </li>
</ul>
</li>
<li>but we also added edges that should be taken into account</li>
<li>for $v \in V$<ul>
<li>let f'(s'-&gt;v)  = $d_{into}(v)$ in order for it to be fully saturated </li>
<li>let f'(v-&gt;t')  = $d_{outof}(v)$</li>
<li>let f'(t'-&gt;s') = size(f)</li>
</ul>
</li>
</ul>
<p>Now we have constructed flow so we need only show that it is saturated and is a valid flow.</p>
<p>Since f is assumed to be feasible we know that</p>
<ul>
<li>(1) f(e) $\ge$ d(e) $\implies$ f'(e) $\ge$ 0</li>
<li>(2) f(e) $\le$ c(e) $\implies$ f'(e) $\le$ c(e) - d(e) = c'(e)</li>
</ul>
<p>Now we check validity, $f'_{in}(v) = f'_{out}(v)$.<br/>
We know that $f_{in}(v) = f_{out}(v)$, Let's see if/how can use this. Recall that $f'_{in}(v)$ is the total demand from s' into v, $d_{into}(v)$. Plus the sum of all the internal vertices in V, which we can write in terms of the original G as $\sum_{u \in V} (f(uv) - d(uv))$</p>
<ul>
<li>ie $f'_{in}(v) = d_{into}(v) + \sum_{u \in V} (f(uv) - d(uv))$</li>
<li>we cancel out the demand terms as they are the same and will offset each other, leaving us with</li>
<li>$f'_{in}(v) = \sum_{u \in V} f(uv) = f_{in}(v)$</li>
<li>similarly we can show that </li>
<li>$f'_{out}(v) = \sum_{u \in V} f(vu) = f_{out}(v)$</li>
</ul>
<p>Putting these together yields $f'_{in}(v) = f'_{out}(v)$ which is our goal. hence f' is a valid flow</p>
<p>We've now proven that G has a feasible flow if and only if our constructed G' has a saturating flow.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>So to find a feasible flow in G</p>
<ul>
<li>we construct G' as described above</li>
<li>we run a max-flow algo on G' (either edmonds-karp or ford-fulkerson will work so pick your fav)</li>
<li>check to see if the max-flow returned is equal to D, where D is the total capacity</li>
<li>if it is equal to D <ul>
<li>then it is a saturating flow </li>
<li>hence it can be transformed back to a feasible flow f for graph G.</li>
</ul>
</li>
</ul>
<p>The only thing that remains is to determine how to find a feasible flow that is also a max flow. Recall how our max-flow algorithms worked. Both FF and EK start with a zero-flow, build a residual graph, and iteratively find augmenting st-paths and increment along such paths, and rebuild the residual graph. They both terminate when they can no longer find augmenting st-paths. This guarantees that the flow constructed is a max-flow. We will take a similar pattern/approach to the max feasible flow.</p>
<p>Given feasible flow f</p>
<ul>
<li>Build a residual graph $G^f$ (we will try to augment this)</li>
<li>find st-path <ul>
<li>if found then augment and repeat</li>
<li>else flow is at a max</li>
</ul>
</li>
</ul>
<p>However, this residual graph is not exactly the same as before.</p>
<p>Given a flow $f$ over $G$<br/>
$c_f(\overset{\rightarrow}{vw}) = 
\begin{cases}
C(\overset{\rightarrow}{vw}) - f(\overset{\rightarrow}{vw}) &amp; if (\overset{\rightarrow}{vw}) \in E   \\
f(\overset{\rightarrow}{wv}) - d(\overset{\rightarrow}{wv}) &amp; if (\overset{\rightarrow}{wv}) \in E   \\
0 &amp; otherwise   \\
\end{cases}$</p>
<ul>
<li>Case 1 represents forward edges, thus this is the left over capacity from the flow</li>
<li>Case 2 represents backwards edges, this is capped by the demand constraint, it cannot decrease less than that <ul>
<li>basically back edges in this type of residual graph are limited to how much the flow exceeds the demands</li>
<li>where as in the previous cases the only limit was 0</li>
</ul>
</li>
<li>Case 3 all edges not in 1 or 2</li>
</ul>
<h3 id="Example">Example<a class="anchor-link" href="#Example">¶</a></h3><p>Here is our example from earlier, along with it's graph G'. Your goal is to specify the capacities along each edge in G'
<img src="CS6515_images/MF5-009.png" style="float:left" width="500;"/></p>
<p>Solutions are in the appendix</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="MF3:-Image-Segmentation">MF3: Image Segmentation<a class="anchor-link" href="#MF3:-Image-Segmentation">¶</a></h2><p>An application of max-flow to Image segmentation. What we want to do is divide an image into it's foreground and background.</p>
<p>In order to formulate this problem into a graph setting we will think of an image as a graph. Each pixel corresponding to the vertices of the graph. The edges will simply be the neighbouring pixels. So each pixel has at most 8 edges that connect it to the 8 pixels in it's immediate neighborhood. This will be an undirected graph as each neighbour pixel is also connected to the originating pixel as well.</p>
<p>We will also add a couple of parameters to help solve this problem</p>
<p>For all i in V (all pixels in our image)</p>
<ul>
<li>Let $f_i \ge 0$ be the weight or likelihood that pixel i is part of the foreground</li>
<li>Similarly let $b_i \ge 0$ be the weight or likelihood that pixel i is part of the background</li>
</ul>
<p>for each $(i,j) \in E$</p>
<ul>
<li>Let $P_{ij} \ge 0$ be the seperation cost, the cost of seperating pixels i &amp; j into different objects</li>
</ul>
<p>Here's an illustration on a toy 3x3 image composed of only black and white pixels
<img src="CS6515_images/MF3-001.png" width="500;"/></p>
<p>We will partition the image vertices, V, into F &amp; B, just as before $V = F \cup B$. Perhaps you can see where this is going? Clearly a pixel cannot belong to both F &amp; B so this defines a cut. so we can compute the weight/capacity of this cut just as before.</p>
$$w(F,B) = \sum_{i \in F} f_i + \sum_{j \in B} b_j - \sum_{i \in F, j \in B} P_{ij} - \sum_{j \in F, i \in B} P_{ij} $$<p>This defines a weight for an assignment of pixels to foreground and background. Now our image segmentation becomes just an optimization problem! Our goal is to maximize the weight w(F,B), we do this by looking over the possible partitions. Unfortunately though our min-cut algorithm requires an st-cut and returns a minimum. So a little extra work is required. Also recall that all the terms in a min-cut problems must be positive, so we need to convert the subtraction of the penalty into a positive term somehow.</p>
<p>So let's reformulate our problem just a bit 
<img src="CS6515_images/MF3-002.png" width="500;"/></p>
<p>What this means is that</p>
<ul>
<li>for $L = \sum_{i \in V} (f_i + b_i)$</li>
<li>$w(F,B)=\sum_{i \in F} f_i + \sum_{j \in B} b_j - \sum_{i \in F, j \in B} P_{ij}$<ul>
<li>this is the same as before</li>
</ul>
</li>
<li>$w'(F,B)=\sum_{i \in F} b_i + \sum_{j \in B} f_j + \sum_{i \in F, j \in B} P_{ij}$    <ul>
<li>this one takes some explanation</li>
<li>sum of background likelihoods of pixels in the foreground</li>
<li>plus sum of foreground likelihoods of pixels in the background</li>
<li>plus sum of penalties</li>
</ul>
</li>
<li>Now we can relate w(F,B) and w'(F,B)<ul>
<li>w(F,B) = L - w'(F,B)</li>
<li>thus maximizing w(F,B) is the result of minimizing w'(F,B)</li>
</ul>
</li>
</ul>
<p>Now we have a minimization problem, and all the terms of w'(F,B) are positive. Now we can use our max-flow solution using min st-cuts.</p>
<p>Let's review our new formulation:</p>
<p>Given an undirected graph G=(V,E) with weights:</p>
<ul>
<li>$f_i,b_i \ge 0, \forall i \in V$</li>
<li>$P_{ij} \ge 0, \forall (i,j) \in E $</li>
</ul>
<p>Find the partition $V=F \cup B$ which minimizes w'(F,B)</p>
<p>What is still missing is our flow network, a directed graph. This is rather easy to add though. We will make all our edges bidirectional. Our original construction was to create a edge with no direction. Now for every edge $e \in E$ we create E' composed of $\overset{\rightarrow}{ij} \in E$ and $\overset{\rightarrow}{ji} \in E$. each of these will also get the seperation penalty $P_{ij}$</p>
<p>What we do next is add a source and sink vertices, s and t,</p>
<ul>
<li>foregrounds: $for i \in V$ add edge s -&gt; i of capacity $f_i$</li>
<li>similarly </li>
<li>backgrounds: $for i \in V$ add edge i -&gt; t of capacity $b_i$</li>
</ul>
<p>Here's what our final graph looks like<br/>
<img src="CS6515_images/MF3-003.png" width="500;"/></p>
<p>Now we have a flow network, and we have source &amp; sink vertices. We can now run Ford-Fulkerson and get a Max-flow. Once we do we can compute the max-flow size to determine the min st-cut.</p>
<p>Let's take a moment to understand st-cuts in the context of this problem.</p>
<p>What is the capacity(F,B)? Recall that this is the sum of capacities of edges going from F to B. What are these edges?<br/>
Suppose we define a partition like so<br/>
<img src="CS6515_images/MF3-004.png" width="500;"/></p>
<p>Which edges cross from F to B?</p>
<ul>
<li>for $i \in F$, get edges i -&gt; t with cap $b_i$, since all pixels got an edge to t</li>
<li>for $j \in B$, get edges s -&gt; j with cap $f_i$, since the source s got an edge to all pixels</li>
<li>we also get all the edges from pixels inside F to pixels inside B, but only those whose direction is outwards</li>
<li>for $(i,j) \in E: i \in F, j \in B$ get i -&gt; j with capacity $P_{ij}$</li>
</ul>
<p>Summing all of these yields capacity(F,B) which is w'(F,B)</p>
<p>Let's summarize what we've done to arrive at a solution</p>
<ul>
<li>We began with the inputs G,f,b,p, where G was undirected</li>
<li>our first step was define our flow network (G',c), where G' is directed with s,t in V</li>
<li>we ran our max-flow to get $f^*$<ul>
<li>which by the maxFlow-minCut theorem: size($f^*$) = capacity of the min st-cut</li>
</ul>
</li>
<li>we demonstrated that capacity of the min st-cut = $\underset{F,B}{min} w'(F,B)$</li>
<li>we also demonstrated that $\underset{F,B}{max} w(F,B)$ is achieved on the same partition as $\underset{F,B}{min} w'(F,B)$    </li>
</ul>
<p>In conclusion we've found the partition F,B that maximizes the equation w(F,B) = L - w'(F,B).</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="NP0:-NP-Completeness">NP0: NP Completeness<a class="anchor-link" href="#NP0:-NP-Completeness">¶</a></h2><p>This section deals with the classification of algorithms based on efficient runtimes. The run time to compute a solution, as well as the run time to verify a solution. Although this may seem like a simple division it turns out that in many cases it's difficult if not impossible to determine the runtime. In such cases determining if an efficient can exist is an even harder ask than it seems.</p>
<p>First some definitions:</p>
<ul>
<li>Efficiency: for an algorithm to be efficient this means that at a minimum the runtime is bounded by a polynomial function on the input size. ie there exists a c for which the runtime is $O(n^c)$.</li>
<li>P in this section refers to polynomial<ul>
<li>this is the set of search/decision problems that can be solved in polynomial time</li>
</ul>
</li>
<li>NP in this section refers to nondeterministic polynomial<ul>
<li>this is the class, or set, of all search/decision problems where the solution can be verified in polynomial time</li>
<li>some literature uses decision rather than search, this is a rather minor insignificant distinction</li>
<li>NP includes P for computable problems, such as those we've looked at up until now  </li>
</ul>
</li>
</ul>
<p>An alternative way of thinking of a nondeterministic problem: It's one where your allowed to just guess at each step, in order to arrive at an accepting state.</p>
<p>Is $P \ne NP$ or is $P = NP$?</p>
<p>Unfortunately, this is an outstanding question that the best mathematicians have yet to solve!!</p>
<p>The topic of P and NP can be difficult to understand, so hopefully these examples help.</p>
<p>Consider the game of chess. Suppose that I presented to you a game with a well defined placement of pieces and told you the next best move is Y. How would you verify the correctness of Y? There are just so many different 2nd 3rd etc etc moves that computing all possible outcomes is challenging. This is an solution that cannot, in general, be verified in polynomial time. It's outside NP.</p>
<p>Consider a Rubrik's cube, with only 6 sides and 9 squares per side, it is within our grasp to prove that given a rubik's cube in any state, it is solvable in polynomial time. This is considered to be an NP-Complete problem, for a 3x9 cube. However it has also been shown that for a more general NxNxN...xN cube this becomes intractable.</p>
<p>In the following sections we will look at the following topics</p>
<ul>
<li>NP &amp; NP-complete reductions</li>
<li>NP-Completeness (3SAT, Graph Problems, Knapsack)</li>
<li>What does NP-completeness mean?           </li>
<li>What's $P = NP$ and $P \ne NP$ mean?            </li>
<li>How do we show that a problem is intractable, meaning it is unlikely to be solvable in an efficient manner.     </li>
</ul>
<p><strong>More Definitions</strong><br/>
Search Problem:</p>
<p>Can be loosely defined as a problem where the solution can be verified efficiently</p>
<pre><code>- given a solution you can verify the solution in polynomial time
- time to generate a solution is irrelevant
- NP = class of all search problems
- P = class of all search problems that can be solved in polynomial time
</code></pre>
<p>It should be pretty clear that P is a subset of NP, $P \subset NP$</p>
<p>More formally a Search problem is defined as</p>
<ul>
<li>First it has to be of the form<ul>
<li>given an instance I</li>
<li>find a solution S for I if one exists</li>
<li>output No if I has no solution</li>
</ul>
</li>
<li><p>Secondly it has to meet the following requirement</p>
<ul>
<li>If given an instance I and a solution S</li>
<li>then S can be verified to be a solution to I in polynomial-time ( polynomial in |I| )</li>
</ul>
</li>
<li><p>Note that S need not be a correct solution, it could be wrong, it still must be verifiable in polynomial time</p>
</li>
</ul>
<p>$P = NP$ means that it is as difficult to solve a problem as it is to verify the solution. I feel that it is important to point out that this is talking about problem in the general tense. ie a Class of problem. Not any particular instance or example of a problem. For most of the problems considered so far solution verification is considerably easier than solution discovery. But equality would also imply the reverse. So if a solution can be verified in polynomial time then it can be solved in polynomial time, and this turns out to be impossible to prove. This has lead many to say that $P \ne NP$. Which allows for P implies NP, and doesn't require NP implies P. Unfortunately this is not all that easier to prove either.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="NP1:-Intro-and-Examples">NP1: Intro and Examples<a class="anchor-link" href="#NP1:-Intro-and-Examples">¶</a></h2><p>Most proofs of being NP tend towards being short and crisp, and quite trivial to be frank.</p>
<h3 id="SAT-Problem">SAT Problem<a class="anchor-link" href="#SAT-Problem">¶</a></h3><p>Recall that we first introduced the SAT problem back in section GR2, where we looked at the 2-SAT problem.</p>
<ul>
<li>Input: Boolean formula f in CNF form, with n variables and m clauses</li>
<li>Output: an assignment of variables if one exists, else No otherwise<ul>
<li>where assignement =&gt; a True/False value for each of the n variables s.t. the formula f evaluates to True</li>
</ul>
</li>
</ul>
<p>Here's another example<br/>
$f = (x_3 \vee \bar{x_2} \vee \bar{x_1})\wedge(x_1)\wedge(x_2 \vee \bar{x_3})\wedge(\bar{x_1} \vee \bar{x_3})$<br/>
Rest assured this formula has a solution and we'll leave it as an exercise to find a solution to this</p>
<p>Suppose we provided you with a solution to this. How long would take to verify the correctness of the solution? Pretty quick I would hope for such an easy problem instance.</p>
<p>Let's generalize this type of problem. Suppose we provided you a formula f and an assignment of values to variables x_1,...,x_n. What would be the runtime of verification? For m clauses this would have a worst case of O(nm). O(n) for verifying that a clause is satisfied, then multiplied by m clauses. Since n x m is polynomial in the input size, this proves that SAT is in NP.</p>
<h3 id="K-Colorings-Problem">K-Colorings Problem<a class="anchor-link" href="#K-Colorings-Problem">¶</a></h3><ul>
<li>Input: undirected G=(V,E), and integer k &gt; 0</li>
<li>Output: Assign each vertex a color in {1,2,...,k} <ul>
<li>s.t. no edge has the same color at it's endpoints, adjacent vertices are of different colors   </li>
<li>No, if the proper coloring doesn't exist</li>
</ul>
</li>
</ul>
<p>What would it take to verify that for a given G and a coloring that the assignment is valid? Pretty easy when you consider that this boils down to an edge having ends of different colours. So we iterate through the m-edges and check colour(v) $\ne$ colour(w). This should take no more than O(m). Which of course means that K-Colouring $\in$ NP.</p>
<h3 id="MST-Problem">MST Problem<a class="anchor-link" href="#MST-Problem">¶</a></h3><ul>
<li>Input: G=(V,E) with positive edge lengths</li>
<li>Output: Tree T with minimum weight</li>
</ul>
<p>Is MST$\in$NP? what about P, is MST$\in$P?</p>
<p>First notice that a solution is guaranteed, unlike in our previous example problems which could return No if a solution didn't exist. So we will always have a tree.</p>
<p>Second, suppose we're given the graph G and the tree T. First we can run DFS/BFS to verify that T is in fact a tree. Recall that these run in O(|V|+|E|). Second we can run Kruskal or Prim to verify that T is of min weight. The weight of the Tree T given to us must be of the same weight as T' that is returned by prim/kruskal. It doesn't have to be the same tree with the same edges. Only the weight matters. Further, these both run in linear time so we can conclude that verifying MST can be done in Polynomial time. This concludes the proof of MST$\in$NP.</p>
<p>Now we move on to proving that MST$\in$P. This of course is trivial as we've already proven that both Kruskal and Prim can compute/determine a min tree in O(m log n). Thus a solution can be found in polynomial time and thus MST$\in$P.</p>
<h3 id="Knapsack-Problem">Knapsack Problem<a class="anchor-link" href="#Knapsack-Problem">¶</a></h3><ul>
<li>Inputs: capacity B, and n objects with integer weights $w_1,...,w_n$ and n integer values $v_1,...,v_n$</li>
<li>Outputs: subset S of objects with a total weight$\le$B, and max total value</li>
<li>Note: recall that there are two variants of this problem, with and without repetition</li>
</ul>
<p>Is KP$\in$NP? what about P, is KP$\in$P?</p>
<p>It's in neither! recall that the runtime of KP is O(nB) and we talked about why this is not efficient. It turns out that O(nB) is actually exponential.</p>
<p>To prove in NP we need to prove that the capacity is less than B and that the values are at a max. Proving the capacity statement is rather trivial. Proving the max value is a bit trickier. For this problem we don't have a graph representation. All we have is a DP method that takes exponential time. So we have no way to verify the solution in polynomial time.</p>
<p>You might be thinking well if KP$\in$NP is false then KP$\not\in$NP is true, but this wrong. Frankly we don't know if there exists an algorithm to check the solution in polynomial time, but we also cannot rule it out. So we're like Shrodeingers cat, we can say which is true. Similarly there may exist an algo that finds the max subset in polynomial time, but until the day it's announced to the world neither KP$\in$P nor KP$\not\in$P are true.</p>
<p>Let's consider a small adjustment to the above. What if instead of maximizing the value we wanted to achieve a value V.</p>
<h3 id="Knapsack-Search-Modified-Knapsack">Knapsack-Search Modified Knapsack<a class="anchor-link" href="#Knapsack-Search-Modified-Knapsack">¶</a></h3><ul>
<li>Inputs: capacity B, and n objects with integer weights $w_1,...,w_n$ and n integer values $v_1,...,v_n$<ul>
<li>and a goal g</li>
</ul>
</li>
<li>Output: subset S of objects with <ul>
<li>total weight $\sum_{i \in S} w_i \le$B, </li>
<li>and total value $\sum_{i \in S} v_i \ge g$ (previously we tried to maximize sum)</li>
<li>No if no such subset S exists</li>
</ul>
</li>
</ul>
<p>Suppose for a moment we could solve this in polynomial time. If this were true then we could theoretically run a binary search on g to find the maximum. This begs the questions how many rounds of binary search would we need to perform to converge to an optimal value? Well the maximum possible of V is just the sum $V = \sum v_i$. So this implies a runtime of O(log V). So is this in NP? Yes! we can prove that now.</p>
<p>Let's assume that we're given the weights and values, a capcity B, a goal g, and a Solution S. To check this solution we need to check the conditions: (1) total weight $\sum_{i \in S} w_i \le$B, and (2) total value $\sum_{i \in S} v_i \ge g$. We're just summing up at most order numbers which can be done/checked in O(n) time.  We can check a proposed solution in polynomial time and thus this version of knapsack is in NP.</p>
<p>You may ask, "Does this really take O(n) time to some of these numbers?"</p>
<p>Well, if you really look at it in terms of the magnitude of these numbers, we had capital V as the sum of all values. That's led capital W to note the sum of all weights. Certainly to add up to these numbers, the time required is at most the number of bits. The number of bits in each of these numbers is at most log of capital W. So the time required to compute this sum is the most order n times log of capital W. To compute this sum is the most O(n log W). Since the input side is log W and log V, this is still polynomial in the input size.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Intuition">Intuition<a class="anchor-link" href="#Intuition">¶</a></h3><p><img src="CS6515_images/NP-001.png" width="500;"/></p>
<p>Now that we've looked at several we should be good to go a bit deeper.</p>
<p>The donut area problems are generally called NP-Complete. These are problems that can be verified in polynomial time, but they cannot be solved in polynomial time. They are also known as the hardest problems in NP, you think of these as the outermost layer of the donut.</p>
<p>Consider:</p>
<ul>
<li>if $P \ne NP$ then all NP-Complete problems are not in P</li>
<li>the complement / or negation of this is</li>
<li>if a NP-Complete problem can be solved in poly-time then all problems in NP can be solved in poly-time</li>
</ul>
<p>What does it mean to be NP-complete?</p>
<p>Consider the SAT problem from before. Recall from the previous section we demonstrated that 2-SAT is in NP, and we generalized this to show that SAP is in NP as well. Is SAT NP-Complete?</p>
<p>A Problem is NP-Complete if it meets the requirements</p>
<ul>
<li>1 - it is in NP</li>
<li>2 - it is the hardest problem in NP<ul>
<li>This means it is the least likely to have an efficient solution</li>
</ul>
</li>
</ul>
<p>For example if we use the SAT problem then "SAT is NP-Complete" means that SAT is the hardest problem in NP, which in turn means that If we can solve a problem like SAT in poly-time then this we can solve every problem in NP in poly-time. This would imply that there is what's called a reduction, from any problem in NP to SAT. ie MST, Colorings, TSP are reducible to SAT. Of course the reader is reminded that the reduction is an implication of being NP-Complete. See the visual below, Conversely if $P\ne NP$ then of course $SAT \ne P$. At this time there is no known polynomial time algo for SAT, there is also no known proof that $P\ne NP$. So we're in a bit of a limbo.</p>
<p><img src="CS6515_images/NP-002.png" width="300;"/></p>
<p>Let's look at how reduction works. Suppose we have problems A &amp; B. For arguements sake think of A as the coloring problem and B as the SAT problem. We want to show/prove the reduction from A to B, denoted $A \rightarrow B$ or $A \le B$. This is read B is at least as computationally hard as A. It means that when we show a reduction from A to B, we're showing that B is at least as hard computationally as A. So if we can solve B then we can solve A.</p>
<p><strong>Formally:</strong></p>
<ul>
<li>Both $A \rightarrow B$ read as A reduces to B,  or as reduction from A to B<ul>
<li>and $A \le B$ : B is as least as Hard as A</li>
</ul>
</li>
<li>Are equivalent, and mean the same thing</li>
<li>Both means that if we can solve problem B in polynomial time then we can use that algorithm to solve A in poly-time.          </li>
</ul>
<p>Now let's look at the mechanics of the reduction:</p>
<ul>
<li>Suppose there is a poly-time algo for SAT, <ul>
<li>we will use this like a black box to get a poly-time algo for the Colorings problem. </li>
</ul>
</li>
</ul>
<p><img src="CS6515_images/NP-003.png" width="500;"/></p>
<p>Let's go through this model. I represents the input Instance for the problem, so let's Say we're working with a coloring problem. This would be a graph G and integer K, so we need a function f that can transform these into the appropriate input for SAT, which you may recall is a boolean set of assignments. In short F(G,k) is the input for SAT.</p>
<p>Then SAT can do it's thing, we need not concern ourselves with what it's doing. And it spits out it's response. The No is trivial, but when we a solution S it will be in the form that SAT operates under. Consequently this will again need to be transformed back to the form that would represent a solution to the initial problem, Colorings.</p>
<p>a) f(Colorings Input) $\rightarrow$ f(G,K) input for SAT<br/>
b) Solution to f(G,k) S $\rightarrow$ h(S) solution for Colorings</p>
<p>We will need to prove that</p>
<ul>
<li>if S is a solution to f then h is a solution to the original G,k</li>
<li>Also if there is no solution to f then there is no solution to the original G,k</li>
<li>This means that we need both directions <ul>
<li>S is a solution to f(G,k) $\iff$ h(S) is a solution to (G,k)</li>
</ul>
</li>
</ul>
<p>Another Example<br/>
To show that Independent Sets (IS) problem is NP-Complete<br/>
Need to show that</p>
<ul>
<li>a) $IS \in NP$</li>
<li>b) $\forall A \in NP, A \rightarrow IS$<ul>
<li>note that this has to hold for all problems in NP,</li>
</ul>
</li>
</ul>
<p>Suppose we know that SAT is NP-Complete, then we have that $\forall A \in NP, A \rightarrow SAT$<br/>
We want to show that $SAT \rightarrow IS$<br/>
Then: by composition $A \rightarrow SAT \rightarrow IS$<br/>
T/f : by transitivity $A \rightarrow IS$</p>
<p>In summary<br/>
Suppose we have a problem that is known to be NP-Complete (like the supposing above)<br/>
Then to show the IS problem is NP-Complete</p>
<ul>
<li>Show that $IS \in NP$, just means show that it can be verified in poly-time</li>
<li><p>Show that $SAT \in NP$, we can replace SAT with any known problem that is NP-Complete</p>
</li>
<li><p>Then we've shown that $A \rightarrow SAT \rightarrow IS$</p>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="NP2:-NP-Complete-Proofs---Pt1">NP2: NP-Complete Proofs - Pt1<a class="anchor-link" href="#NP2:-NP-Complete-Proofs---Pt1">¶</a></h2><p>In this section we will illustrate an example of an NP-completeness proof. We will do prove that the 3SAT problem is NP-Complete. In doing so we will make use of the fact that SAT is NP-Complete, which was first proven by Steven Cook &amp; Leonid Levin in 1971. Incidentally, after Cook &amp; Levin proved SAT NP-Completeness, Dick Karp in 1972 used it to prove that another 21 problems were also NP-Complete. We will look at a few of these problems, beginning with 3SAT.</p>
<h3 id="3SAT-NP-Complete">3SAT NP-Complete<a class="anchor-link" href="#3SAT-NP-Complete">¶</a></h3><p>Recall the parameters of the 3SAT Problem</p>
<ul>
<li>Input: <ul>
<li>Boolean formula f in CNF form </li>
<li>n variables and m clauses</li>
<li>each clause may contain at most 3 literals ($\le 3$)</li>
</ul>
</li>
<li>Output:<ul>
<li>No, if no satisfying arguements exist</li>
<li>else the satisfying assignment, </li>
<li>where an assignment is a set of boolean, or true-false, values for n variables</li>
</ul>
</li>
</ul>
<p>The goal of our proof is to show</p>
<ol>
<li>$3SAT \in NP$</li>
<li>$SAT \rightarrow 3SAT$, Thus $\forall A \in NP, A \rightarrow 3SAT$, </li>
</ol>
<p><strong>Part 1</strong> To prove that $3SAT \in NP$ we show that for a given Solution S, in this problem S is a satisfying assignment, that we can verify the solution in polynomial time. To this end suppose we are given a CNF formula f and an assignment $x_1,x_2,\cdots,x_n$. We will go through each of the m clauses, and check if at least 1 literal in the clause is satisfied. This takes O(1) as each clause is limited to 3 literals. Thus to check all m clauses will require at most O(m) time. This is poly-time so our proof is complete.</p>
<p><strong>Part 2</strong> Now let's see how we can reduce the 3SAT problem.<br/>
Recall</p>
<ul>
<li>We need to find an f' such that f' maps the input from a SAT problem to a 3SAT problem.  </li>
<li>We also need to find a $\sigma'$ that maps the output from the 3SAT problem back to $\sigma$, the solution to a SAT problem</li>
<li>and of course <ul>
<li>$\sigma'$ satisfies f' $\iff$ $\sigma$ satisifes f</li>
</ul>
</li>
</ul>
<p><img src="CS6515_images/NP-004.png" width="500;"/></p>
<p>How do we find f'? f is the input to SAT and may very well contain clauses w more than 3 literals. So our f' must be able to transform any number of literals into clauses of at most 3 literals. Let's look at an example to see how we may transform</p>
<p>Consider f = $(x_3) \wedge (\bar{x_2} \vee x_3 \vee \bar{x_1} \vee \bar{x_4}) \wedge (x_2 \vee x_1)$</p>
<ul>
<li>f contains a clause with 4 literals and so cannot be fed directly into a 3SAT algo</li>
<li>clauses 1 and 3 are valid so we can leave them as is</li>
<li>for clause 2<ul>
<li>create a new variable y, and partition the clause as follows</li>
<li>$C'_2 = (\bar{x_2} \vee x_3 \vee y) \wedge (\bar{y} \vee \bar{x_1} \vee \bar{x_4})$</li>
<li>replace $C_2$ with $C'_2$</li>
<li>we claim that these are equivalent</li>
</ul>
</li>
<li>Claim: $C_2$ is satisfiable $\iff$ $C'_2$ is satisfiable</li>
<li>It follows then that if our claim is true then our formulas are also equivalent, and we have found our f' since the replacement of clauses would lead to an input that is valid for 3SAT </li>
</ul>
<p>We must prove our claim in order to move forward<br/>
Claim</p>
<ul>
<li>$C_2 = (\bar{x_2} \vee x_3 \vee \bar{x_1} \vee \bar{x_4})$</li>
<li>is equivalent to </li>
<li>$C'_2 = (\bar{x_2} \vee x_3 \vee y) \wedge (\bar{y} \vee \bar{x_1} \vee \bar{x_4})$</li>
</ul>
<p>Forward $C_2 \implies C'_2$, take a satisfying assignment for $C_2$</p>
<ul>
<li>if $x_2=F$ or $x_3=T$, then we take y=F, we choose y to satisfy the other clause</li>
<li>if $x_2=F$ or $x_3=T$, then we take y=T, again we choose y to satisy the other clause    </li>
<li>In either case both clauses of $C'_2$ are satisfied     </li>
</ul>
<p>Backwards: $C'_2 \implies C_2$, take a satisfying assignment for $C'_2$</p>
<ul>
<li>if y=T, then clause 1 is satisfied. This implies that $x_1=F$ or $x_4=F$, either case will satisfy $C_2$</li>
<li>sim if y=F, then clause 2 is satisfied. This implies that $x_2=F$ or $x_3=T$, either case will satisfy $C_2$</li>
<li>In either case $C_2$ is satisfied     </li>
</ul>
<p>We've shown both direction, and thus this completes the proof of our claim. However, we must generalize this for any clause where the number of literals is greater than 3.</p>
<p>Let's look at another example, this time with 5 literals</p>
<ul>
<li>let $C = (\bar{x_2} \vee x_3 \vee \bar{x_1} \vee \bar{x_4} \vee \bar{x_5})$</li>
<li>This time let's create two new variables: y and z</li>
<li>We will now find, or construct, $C'$ in a manner <ul>
<li>such that it is equivalent to the original </li>
<li>and its clauses are each of size $\le 3$</li>
</ul>
</li>
<li>To this end we use     </li>
<li>$C' = (\bar{x_2} \vee x_3 \vee y) \wedge ( \bar{y} \vee \bar{x_1} \vee z)\wedge ( \bar{z} \vee \bar{x_4} \vee \bar{x_5})$</li>
<li>what we've done here is create clauses using the original literals along with a replacement literal for the overflow terms</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>To prove that $C \implies C'$ all you need to</p>
<ul>
<li>select a literal from $C$ at random and observe the values in $C'$</li>
<li>Then set the values of y and z accordingly. </li>
</ul>
<p>To prove that $C' \implies C$ will require that we consider assignments of the new variables. Because of the 2nd clause which involves they are both assigned, and will imply values constraining the other two clauses</p>
<ul>
<li>Case 1: y=F </li>
<li>Case 2: z=T</li>
<li>Case 3: y=T and z=F,  </li>
</ul>
<p>Now what if our clause had k literals? We've seen so far that for k=4 we require 1 new variable, and we ended up with 2 clauses. For k=5 we needed 2 new variables, and created 3 new clauses. One may surmise that for k &gt; 3 we will need k-3 new variables, and k-2 clauses.</p>
<p>Let's now consider a clause C of size k, ie $a_1,a_2,\cdots,a_k$ where each $a_i$ is a literal.</p>
<ul>
<li>first we will create k-3 new variables ($y_1,y_2,\cdots,a_{k-3}$)         </li>
<li>Second we replace C with k-2 clauses<ul>
<li>$C' = (a_1 \vee a_2 \vee y_1) \wedge (\bar{y_1} \vee a_3 \vee y_2) \wedge \cdots $</li>
<li>$\cdots \wedge (\bar{y_{k-4}} \vee a_{k-2} \vee y_{k-3}) \wedge (\bar{y_{k-3}} \vee a_{k-1} \vee a_{k}) $</li>
</ul>
</li>
</ul>
<p>This will be our input to 3SAT, it defines a process or f' transformation that maps the input to a SAT problem to an input to the 3SAT.</p>
<p>Now we prove the $C \iff C'$<br/>
$C \implies C'$,</p>
<ul>
<li>Take an assignement of $a_1,a_2,\cdots,a_k$ that satisfies C </li>
<li>Then $a_i = TRUE$ for some i</li>
<li>So let $a_i$ be the min i where $a_i$ is satisfied</li>
<li>Since $a_i = True$ <ul>
<li>we have that the (i-1)st clause of C' is also satisfied,   </li>
<li>we set $y_1=y_2=\cdots=y_{i-2} = True$ to satisfy clauses 1 to (i-2)</li>
<li>we set $y_{i}=y_{i+1}=\cdots=y_{k-2} = False$ to satisfy the remaining clauses</li>
</ul>
</li>
</ul>
<p>$C' \implies C$,</p>
<ul>
<li>Take an assignment of $a_1,\cdots,a_k,y_1,\cdots,y_{k-2}$ that satisfies C'  </li>
<li>all we need to do is prove that at least one $a_i$ is true </li>
<li>So let's assume that $a_i$ is false for all i, and just ignore the y's</li>
<li>now we observe that<ul>
<li>clause 1: $y_1$ is true since a_1 and a_2 is false</li>
<li>clause 2: $y_2$ is true since $\bar{y_1}$ is false and a_3 is false</li>
<li>we keep going until the last clause </li>
<li>clause k-2: $\bar{y_{k-3}}$ is false because of the previous clause, and $a_{k-1},a_{k}$ are false by assumption<ul>
<li>this contradicts our initial supposition that $a_1,\cdots,a_k,y_1,\cdots,y_{k-2}$ satisfies C'  </li>
</ul>
</li>
<li>This means that a_i must be true for some i</li>
<li>which in turns means that $a_1,\cdots,a_k$ will satisfy C</li>
</ul>
</li>
</ul>
<p>So now that we have all our pieces we can design an algorithm to reduce $SAT \rightarrow 3SAT$</p>
<ul>
<li>Consider f for SAT            </li>
<li>Create f' for 3SAT as follows<ul>
<li>for each clause $C \in f$:<ul>
<li>if $|C| \le 3$ then add C to f'</li>
<li>if $|C| \gt 3$ then<ul>
<li>create k-3 new variables</li>
<li>create c' as outlined above </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>output f'</li>
</ul>
<p>Claim: f is satisfiable $\iff$ f' is satisfiable</p>
<p>$\implies$: Given an assignment S satisfying f</p>
<ul>
<li>for each clause C in f<ul>
<li>if $|C| \le 3$ then C is also satified in f'</li>
<li>if $|C| \gt 3$ then C' is also satisfied whenever C is satisfied (this was proven in preceding section)</li>
<li>thus f' is also satisfied</li>
</ul>
</li>
</ul>
<p>$\impliedby$ Given an assignment S' satisfying f'</p>
<ul>
<li>for some clause C' in f' at least one of the literals in C is also satisfied, this was proven in the preceding section where we showed that if all of them were false we would end up with a contradiction</li>
</ul>
<p>And that's all folks, we shown that: f is satisfiable $\iff$ f' is satisfiable</p>
<p>Now we move onto finding $\sigma'$ which will transform the output of 3SAT back to the output for SAT. Luckily this turns out to be quite trivial. Simply ignore the new variables created by f'. That's all. The output from 3SAT still contains all the original literals so by ignoring them we end up with an assignment to the original variables. Which is what we need to verify.</p>
<p><strong>Time Complexity</strong> One final thing to note is the size of f'. Recall that f has n variables and m clauses. So f' will have O(nm) variables and O(nm) clauses, since we added k-3 variables and k-2 clauses. Regardless these are both polynomial in the input size which is what matters the most.</p>
<p>Problems</p>
<ul>
<li>DPV 8.3 - Stingy SAT</li>
<li>DPV 8.8 - Exact 4SAT (each clause has exactly 4 literals)</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="NP3:-NP-Complete-Proofs---Pt2">NP3: NP-Complete Proofs - Pt2<a class="anchor-link" href="#NP3:-NP-Complete-Proofs---Pt2">¶</a></h2><p>In this section we will look at some more examples of NP-completness proofs in particular those focused on Graph Problems.</p>
<ul>
<li>IS: Independent Sets</li>
<li>IS Clique</li>
<li>Vertex Cover</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="IS-Independent-Sets">IS Independent Sets<a class="anchor-link" href="#IS-Independent-Sets">¶</a></h3><p><strong>Problem Definition</strong><br/>
For an undirected graph G=(V,E), subset $S \subset V$ is an independent set if no edges are contained in S.<br/>
ie $\forall x,y \in S; \; (x,y) \not\in E$</p>
<p>It's easy to find trivial independent sets. The null set is an independent set, as is the set containing just a single node, a singleton vertex. We will be looking for the maximum independent set, the set with the largest number of vertices. Consider the below illustration. The red vertices are an independent set as there is no edge between any two vertices in red. Also note that adding any of the white nodes would create a dependency and the set would no longer be independent.</p>
<p><img src="CS6515_images/NP-005.png" width="250;"/></p>
<p>First, consider whether Maximum Independent Sets, MIS, is "known to be" in NP or not. If you guessed this to be false then you would be correct. Given a solution S there is no way to verify, in poly-time, that it is of max size without solving the source problem, ie running the base algorithm. This can only be done if P=NP. Recall that we encountered a similar situation when we examined the knapsack problem. Since we could not verify a maximum directly we took a search style approach where we verified that the solution value was less than a boundary value. We incremented the boundary value until we found no solution.</p>
<p>We will take a similar approach and reformat the MIS problem to the a more general IS problem:</p>
<ul>
<li>Input : Undirected graph G=(V,E) and goal g</li>
<li>Output: Independent set S with size $|S| \ge g$ is one exists</li>
<li><p>: else No</p>
</li>
<li><p>Theorem: Independent Set problem is NP-Complete</p>
</li>
</ul>
<p>It will be straight forward to show that this version of the problem is a search problem and t/f lies in the class NP.</p>
<p><strong>Part 1:</strong> Recall that the first step is to show that the problem is in NP. This means that we must show that for a given solution S to the IS problem we can verify it's correctness in poly-time.</p>
<p>Suppose we're given the inputs G &amp; g and a solution S. We can check each pair of vertices $(x,y) \in S$ and verify if they are adjacent or not ie $(x,y) \not\in E$. This will require approx O($n^2$) time. Then we check that $|S| \ge g$, which will require O(n) time. If both of these are true then we have verified the correctness of S, and we have done so in O($n^2$) time. This proves the claim that IS is in NP.</p>
<p><strong>Part 2:</strong> Recall that the second step in an NP-Completeness proof is to show that the problem is at least as hard as every problem in the class NP. We take a problem that is known to be at least as hard as every other problem, like SAT or 3SAT, and we show a reduction to our new problem, IS. For this example we will use 3SAT as our known NP-complete problem and show it can be reduced to the IS problem. ie We prove $3SAT \rightarrow IS$</p>
<p>Recall that 3SAT takes a formula f with n variables and m clauses, where each clause is at most of size 3, ie $|C_i| \le 3$. IS is a graph problem with inputs G=(V,E) and a boundary g.</p>
<p>We first formulate a mapping h to transform the input to 3SAT into a graph G and boundary g. For the boundary we will take g=m, where m is the number of clauses and g is the upper bound on the max IS value, the size of the IS.</p>
<p>For each clause $C_i$, we create $|C_i|$ vertices corresponding to each literal. Since we can have up to m clauses of size 3 we may have up to 3m vertices.</p>
<p><strong>Clause Edges</strong><br/>
Consider a clause of size 3, $C=(x_1 \vee \bar{x_3} \vee x_2)$<br/>
We begin by creating a graph G with 3 vertices: $x_1,\bar{x_3},x_2)$, and we add an edge between each pair of vertices.</p>
<ul>
<li>Observe that an Independent Set S has $\le 1$ vertex per clause</li>
<li>but since g=m, our solution must have exactly 1 vertex/clause. This corresponds to the satisfied literal in the clause  </li>
</ul>
<p>Consider the following example, (NB the assignement here is random and produces a contradiction. It is not a valid Solution to the cnf formula on the left side.<br/>
<img src="CS6515_images/NP-006.png" width="250;"/></p>
<p>The above example can be further refined, by adding variable edges. This helps us to avoid invalid assignments that lead to contraditions.</p>
<p><strong>Variable Edges</strong><br/>
Consider the example above, add an edge between literals of the same variable. In our case add an edge from $x_1$ to $\bar{x_1}$.</p>
<ul>
<li>For all $x_i$, add edge between all $x_i$ and $\bar{x_i}$</li>
</ul>
<p><img src="CS6515_images/NP-007.png" width="250;"/></p>
<p>Here's another example which is more fulsome:</p>
<p>The first step is to write out the literals as nodes, in groups corresponding to clauses
<img src="CS6515_images/NP-008.png" width="250;"/></p>
<p>The next step is to connect the literals in a clause, these are our clause edges
<img src="CS6515_images/NP-009.png" width="250;"/></p>
<p>The next step is to connect literals to their negations, these are our variable edges
<img src="CS6515_images/NP-010.png" width="250;"/></p>
<p>Let's take an example of an IS, of size 4, from this graph. We've circled the 4 vertices in red
<img src="CS6515_images/NP-011.png" width="250;"/></p>
<p>This leads to the assignment x=F,y=F,w=F,z=?. You'll observe that it doesn't matter what z is. Further more this assignment satisfies the original formula f. This observation can, and will, be generalized for our proof.</p>
<p>More formally:</p>
<ul>
<li>f has a satisfying assignment $\iff$ G has an independent set of size $\ge$ g</li>
</ul>
<p><strong>Part 1</strong> $\implies$ consider a satisfying assignment for f, we will construct a graph G with an IS of size $\ge$ g.</p>
<ul>
<li>For each clause C, Take 1 of the satisfied literals. (f satified guarantees at least 1 satisfied literal in each clause)</li>
<li>add the corresponding vertex to S</li>
<li>Hence |S| will be of size at least m, (1 literal per clause * m clauses) and g = m - this proves S is of size $\ge$ g</li>
</ul>
<p>Now we need to prove S is an independent set. Observe that S has exactly 1 vertex per clause &amp; never contains both $x$ &amp; $\bar{x}$. Together this means that</p>
<ul>
<li>S contains no clause edges (if it did there would be more than 1 vertex per clause)</li>
<li>S contains no variable edges (if it did it would contain $x_i$ and $\bar{x_i}$ for some i)</li>
</ul>
<p>Together this means there are no edges in S, So S is an IS. and because S is of size $\ge$ g = m. We have proven that G has an independent set of size $\ge$ g. Now we move to the inverse direction</p>
<p><strong>Part 2</strong> $\impliedby$ Consider an independent set S of size $\ge$ g, and g=m. Then S will have exactly 1 vertex per clause. Each vertex corresponds to a literal in f so we set the corresponding literal to True. This means that every clause is will be satisfied and thus f is satisfied. But this doesn't mean that it is guaranteed to be a valid assignment. For that we note that there are no contradictory literals since there are no variable edges in S. This guarantees a valid assignment.</p>
<p>Together this proves that a reduction from 3SAT to IS is correct, and it shows how to take an IS and construct a Satisfying assignment. This completes our proof</p>
<p>Now Let's take a moment to go back to consider what this means to our original problem. Recall that when we began our discussion we considered the MAX independent set problem which is an optimization problem. We modified this to a Search problem by taking g=m rather than try to optimize g.</p>
<p>In our proof above we have shown that IS is in the class NP, and consequently there is a reduction from any problem in NP to IS. IS is at least as hard as any problem in NP. But Max-IS is not known to be in NP. However, it can be shown that there is a reduction from IS to Max-IS. This follows a similar logic to our previous example with knapsack where can determine the optimal solution by repeated verification using increasing g.</p>
<p>Since all problems in NP can be reduced to IS and IS can reduce to Max-IS. This means that Max-IS is at least as hard as every problem in NP. This doesn't mean that Max-IS in the class NP! In fact we've shown that Max-IS is not in NP. This leads us to the scenario called NP-hard.</p>
<p><strong>Definition: NP-Hard</strong><br/>
A problem that is NP-Hard is a problem that is not known to be in NP, but is at least as hard as every problem in NP.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Clique">Clique<a class="anchor-link" href="#Clique">¶</a></h3><p><strong>Define</strong></p>
<ul>
<li>A Clique is a fully connected subgraph.           </li>
<li>ie For an undirected graph G=(V,E) $S \subset V$ is a clique if $\forall x,y \in S;(x,y)\in E$</li>
<li>ie To be a clique all edges must be connected to each other </li>
</ul>
<p>Small cliques tend to be trivial to find. For instance 2 vertices with an edge between them is a trivial clique. The null set is also a trivial clique. We will generally want to focus on the largest Clique, or the clique of max size.</p>
<p>Example, Clique elements are in red
<img src="CS6515_images/NP-012.png" width="300;"/></p>
<p>This is in fact also a max clique. To understand why look/Consider each white node. Can any of the white nodes be added? No! none of them would retain the fully connected nature of the the red nodes.</p>
<p>Let's now formalize the Clique problem structure:</p>
<ul>
<li>Input: G=(V,E) &amp; goal g</li>
<li>Output: $S \subset V$ where S is a clique of size $|S| \ge g$ where one exists<ul>
<li>No otherwise</li>
</ul>
</li>
<li>Theorem: Clique Problem is NP-Complete</li>
</ul>
<p><strong>Part 1</strong> Clique problem is in NP<br/>
Suppose we're given a proposed solution to Clique, ie we're given input $(G,g)\in S$</p>
<ul>
<li>First <ul>
<li>we can check each edge in S: $\forall x,y \in S$ check that $(x,y) \in E$ </li>
<li>This will take us $O(n^2)$ time</li>
</ul>
</li>
<li>Second <ul>
<li>we check that $|S| \ge g$</li>
<li>This is rather straight forward and will take O(n) time</li>
</ul>
</li>
<li>Together this will take poly-time to verify the correctness of S. This proves that Clique is in NP</li>
</ul>
<p><strong>Part 2</strong> Clique problem is as hard as every problem in NP<br/>
To prove this claim we will take a known NP-Complete problem and show that there is a reduction to the Clique problem. So far we've looked at and proven 3 NP-Complete problems. SAT, 3SAT, and IS. So these will be our only options which one will we choose? Since Clique is by nature a graph problem it stands to reason that IS will be the simplest to use in our reduction.</p>
<p>Let's sketch out the differences between the two to better understand how we might perform a reduction. A Clique is a maximal set where every vertex is connected to every other vertex. IS is also a maximal set, but one where there are absolutely no edges between the vertices. In a way they are like complete opposites of one another. So we will look at the opposite of the Graph G for the IS problem.</p>
<p>For a graph G=(V,E) Let's define the opposite as $\bar{G}=(V,\bar{E})$, we can think of this as the complement of the original graph. It doesn't make sense to take the complete of the vertices so we leave these alone. The Complement of the edges are the set of all edges between two vertices that are NOT in E. ie $\bar{E}=\{(x,y):(x,y)\not\in E \}$. Armed with these new constructions we can formalize the notion of oppositeness.</p>
<p><strong>Observation:</strong> S is a clique in $\bar{G}$ $\iff$ S is an independent set (IS) in G. It's important to note that this an observation that stems from the fact that edges in $\bar{E}$ cannot be in $E$. It simply follows, or is a consequence of being a complement. $(x,y) \not\in E$ is basically a definition of an independent set S. if there's no edges connecting two vertices then the complement is the edge that connects them. Given this observation it should now be straight forward to show a reduction from $IS \rightarrow Clique $</p>
<p><strong>Reduction</strong><br/>
Given the input G=(V,E) and a goal g for the IS problem</p>
<ul>
<li>Let $\bar{G}$ and g be the input to the clique problem</li>
<li>Now apply the clique algo as a black box <ul>
<li>if we get a solution S to our input then S is also a solution to IS in G with a size at least g</li>
<li>else if clique returns no then a solution doesn't exist for IS</li>
</ul>
</li>
</ul>
<p>This completes our reduction. Putting both parts together shows that Clique is NP-Complete.</p>
<p>Finally, we move towards our last example. Vertex covers</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="VC-Vertex-Cover">VC Vertex Cover<a class="anchor-link" href="#VC-Vertex-Cover">¶</a></h3><p><a href="https://en.wikipedia.org/wiki/Vertex_cover">Wikipedia</a></p>
<p><strong>Definition:</strong> For an undirected graph G=(V,E) $S \subset V$ is vertex cover if S "covers every edge".<br/>
<strong>Alt Define:</strong> For an undirected graph G=(V,E) $S \subset V$ is vertex cover if S includes one endpoint of every edge<br/>
Formally: For every $(x,y)\in E$ either $x \in S$ and/or $y \in S$</p>
<p>Consider the below image. At least one endpoint of each edge is included, both endpoints can be included but this is not required by the definition. Obviously there is the trivial case of simply including all vertices. So as you can imagine we will generally be searching for a minimum vertex cover
<img src="CS6515_images/NP-013.png" width="300;"/></p>
<p><strong>Problem Structure</strong></p>
<ul>
<li>Input: undirected G=(V,E) &amp; budget b</li>
<li>Output: Vertex cover of S of size $|S| \le b$ if one exists<ul>
<li>No otherwise</li>
</ul>
</li>
</ul>
<p><strong>Claim</strong> Vertex Cover problem is NP complete.</p>
<p><strong>Part A: VC in NP</strong><br/>
Given an input to VC, (G,b), and a proposed solution S, can we verify that S is a VC with size at most b in polytime?</p>
<p>First we verify that for each edge in E one of it's endpoints are in S. ie $\forall (x,y) \in E;$ $x\in S$ or $y\in S$. This should take $O(n+m)$. Then we check that $|S| \le b$, which takes O(n) time. Together these imply a ploynomial runtime so VC is in NP and required.</p>
<p><strong>Part B: VC is at least as hard as any problem in NP</strong><br/>
As per our usual approach we want to choose a known problem in NP that we can use to perform a reduction. Because VC is a graph problem it makes the most sense to choose either IS or Clique. Since these are quite similar it shouldn't make much of a difference.</p>
<p>We choose IS and proceed to construct our reduction: $IS \rightarrow VC$. Take another look at the example of the vertex cover just above. Notice anything? Hint: Focus on the white nodes. You may notice that they form an independent set. Not maximal by any means but independent nonetheless.</p>
<p>Here's our example graph again, but this time the nodes painted red form a minimal vertex cover. 
<img src="CS6515_images/NP-014.png" width="250;"/>
Again we see the white nodes forming an independent set.</p>
<p>I'm sure you're beginning to pick up on where this is going.</p>
<p><strong>Claim</strong> S is a Vertex Cover $\iff$ $\bar{S}$ is an independent set. Once we prove this claim the reduction of IS to VC will be much simpler. The proof of the claim is rather straight forward.</p>
<p>$\implies$ Take a vertex cover S. Then for each edge (x,y) in E at least 1 endpoint is in S. Thus at most one endpoint is in $\bar{S}$. If at most one endpoint is in $\bar{S}$ then there cannot be any edges in $\bar{S}$. This means that $\bar{S}$ is an independent set.</p>
<p>$\impliedby$ take an IS set $\bar{S}$ then $\bar{S}$ cannot contain any edges. This means that every vertex in $\bar{S}$ is an endpoint. ie for all edges $(x,y) \in E$ only one can be in $\bar{S}$. Thus S will contain the complementing endpoint. In fact S contains $\ge 1$ edge endpoint for each edge. thus S covers every edge in S and is therefore a vertex cover</p>
<p>This completes the proof of the claim and we can move on to the reduction.</p>
<p><strong>Proof: $IS \rightarrow VC$</strong><br/>
Suppose we're given G=(V,E) and g for an Independent Set problem</p>
<ul>
<li>Let b=n-g        </li>
<li>Run vertex cover on G,b</li>
</ul>
<p>From our claim we know that: G has VC of size $\le n-g$ $\iff$ G has an IS of size $\ge g$.<br/>
Given a solution S for VC we return $\bar{S}$ as a solution to IS. If there no solution for VC there likewise there is no solution to IS problem</p>
<p>This completes the reduction and proves it's reduction</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Travelling-Salesman-Problem">Travelling Salesman Problem<a class="anchor-link" href="#Travelling-Salesman-Problem">¶</a></h3><p>from Youtube (<a href="https://www.youtube.com/watch?v=0sQ37m3whP4&amp;list=PLQfaHkBRINsxngvO_CIM74kUauPd-BGa3">https://www.youtube.com/watch?v=0sQ37m3whP4&amp;list=PLQfaHkBRINsxngvO_CIM74kUauPd-BGa3</a>)</p>
<p><strong>Problem Formulation</strong><br/>
Given a graph G=(V,E) and vertices $v_1$ and $v_n$ in that graph, what is the minimum weight path from $v_1$ to $v_n$ that visits every vertex exactly once.</p>
<p><img src="CS6515_images/NP-020.png" width="350;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Problems">Problems<a class="anchor-link" href="#Problems">¶</a></h3><ul>
<li>DPV 8.4 NP Completeness Erro</li>
<li>DPV 8.10 Proof by Generalization (Known $\rightarrow$ New)</li>
<li>DPV 8.14 Clique +IS</li>
<li>DPV 8.19 Kite</li>
</ul>
<p>There are basically two styles/approaches to NP Completeness reductions: (Known $\rightarrow$ New)</p>
<h3 id="Summary">Summary<a class="anchor-link" href="#Summary">¶</a></h3><p><strong>Method 1: Generalization</strong><br/>
In this approach we show that the NEW problem is a generalization of a known problem. The clique proof was a perfect example of this since we showed that Clique was just a modified reflection of the IS problem. VC was also of this flavour</p>
<p><strong>Method 2: Gadget proof</strong><br/>
The proof of 3SAT followed this style. In this approach we take the inputs to the known problem and add some structure, possibly new, to make it fit the new problem. In the 3SAT proof we created new variables that allowed us to restructure a SAT input with arbitrarily sized clauses into a 3SAT problem with clauses of size at most 3.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="NP4:-Knapsack">NP4: Knapsack<a class="anchor-link" href="#NP4:-Knapsack">¶</a></h2><p>In this section we will be looking at the subset-sum problem as an intermediate step in proving that Knapsack in NP-Complete. Recall that the runtime of the knapsack problem is O(nB). We explained earlier why O(nB) is not polynomial in it's runtime (due to the B term).</p>
<h3 id="Subset-Sum">Subset-Sum<a class="anchor-link" href="#Subset-Sum">¶</a></h3><p>We begin by defining the subset-sum problem:</p>
<ul>
<li>Input : positive integers $a_1,...,a_n$ &amp; t</li>
<li>Output: subset S of {1,...,n} s.t. $\sum_{i \in S} a_i = t$ if it exists<ul>
<li>else No</li>
</ul>
</li>
</ul>
<p>(NB: This is a great example of a dynamic programming problem: Find an algo to this that runs in O(nt) time.</p>
<p>The first question we always ask is ... is subset-sum in P? Of course not! We've shown this before via knapsack. Due to the B term it is not polynomial in the input size and thus cannot be solved in poly-time. It takes O(nB) and we would need to replace it with log(B). Let's now look to see if it's NP-Complete</p>
<p><strong>Proof Subset-sum $\in$ NP</strong><br/>
Given inputs $a_1,...,a_n$, t and S</p>
<ul>
<li>Check that $\sum_{i\ in S} a_i = t$, this will take time O(n log t)</li>
</ul>
<p>Since log t is polynomial in the input size we can conlude that Subset-sum can be verified in poly-time.</p>
<p><strong>Proof Subset-sum is at least as hard as every problem in the class NP</strong><br/>
We choose 3SAT as our known problem and demonstrate the reduction: $3SAT \rightarrow Subset-sum$</p>
<p>Input to subset-sum consists of 2n+2m+1 numbers:</p>
<ul>
<li>2n for the n variables, $v_1,v'_1$ for $x_1,\bar{x_1}$, $v_2,v'_2$ for $x_2,\bar{x_2}$, etc etc</li>
<li>2m for the m clauses, $s_1,s'_1$ for 1st clause, $s_2,s'_2$ for 2nd clause, etc etc</li>
<li>+1 for the sum, t</li>
</ul>
<p>Note that all these numbers can be quite large, up to $\le n+m$ digits long. Furthermore we will be working base 10, so as to avoid any carryover of bits when adding subsets of numbers. t is also quite large, of the magnitude $t \approx 10^{n+m}$.</p>
<p>Let's begin by looking at the $v_i$'s. In particular each $v_i$ corresponds to an $x_i$. So we include $v_i$ in S whenever $x_i$ is true. ie $v_i \in S \iff x_i = True$. Similarly $v'_i$ corresponds to $\bar{x_i}$ and we include $v'_i$ in S whenever $x_i$ is false. Of course an assignment to a 3SAT problem cannot include both cases as that would be a contradiction. So for the subset-sum problem to work only one scenario must exist. Either $v_i \in S$ or $v'_i \in S$ but we cannot include both nor can we include neither. We will achieve this by setting the i'th digit of $v_i,v'_i,t$  to 1, and for all other digits and numbers the i'th digit is set to 0. Recall as well that since we are using base 10 numbering each digit will behave independently of each other. Furthermore, the i'th digit of t will be 1 if only $v_i$ or $v'_i$ is in S, but not both/neither.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Here's an example. Rows represent literals and the columns represent the variables and the clauses. 
<img src="CS6515_images/NP-015.png" width="350;"/></p>
<p>We begin by placing ones along the diagonal for the variables $x_1,x_2,x_3$, and 0 in all other entries. Note that this is only for the variables so as to ensure each variable is in the solution/assignment. To ensure it is a satifying assignment we will use the columns for the clauses.</p>
<p>So digit n+j correspond to clause $C_j$.</p>
<ul>
<li>if $x_i \in C_j$ then place a 1 in digit n+j for $v_i$                           </li>
<li>if $\bar{x_i} \in C_j$ then place a 1 in digit n+j for $v'_i$<br/>
What this does is encode the clause into the literals $v_i,v'_i$  </li>
</ul>
<p>For our example we get the following
<img src="CS6515_images/NP-016.png" width="350;"/></p>
<p>Now let's take a minute to think about the row t for each clause column. In order to satisfy the clause we require 1,2, or 3 of the literals to be satisfied. So we take the max, 3, as the sum for each column. Now of course all three literals may not be satisfied. If not then we will need the buffer numbers in order to make up the difference. The buffer numbers are represented by the s variables. Consequently if only 1 literal in a clause is satisfied then we can make use of the buffer numbers to ensure the summation which in turn will satisfy the clause. 
<img src="CS6515_images/NP-017.png" width="350;"/></p>
<p>Each row can now represent a number. Just read it from the left to the right
<img src="CS6515_images/NP-018.png" width="350;"/></p>
<p>Let's summarize our clause reduction</p>
<ul>
<li>Digit n+j corresponds to clause $C_j$            <ul>
<li>if $x_i \in C_j$ put a 1 in digit n+j for $v_i$                 </li>
<li>if $\bar{x_i} \in C_j$ put a 1 in digit n+j for $v'_i$              </li>
</ul>
</li>
<li>Put a 3 in digit n+j of t</li>
<li>Use $s_j,s'_j$ as buffers<ul>
<li>put a 1 in digit n+j of $s_j$ &amp; $s'_j$</li>
</ul>
</li>
<li>put a 0 in digit n+j of all other numbers</li>
</ul>
<p>Now we can prove: Subset-sum has a solution $\iff$ 3SAT f is satisfiable</p>
<p>$\implies$ Take solution S to subset sum<br/>
For digit i where $1 \le i \le n$:</p>
<ul>
<li>to get 1 in digit i: include $v_i$ or $v'_i$ <strong>but not both</strong><ul>
<li>if $v_i \in S$ then set $x_i = T$</li>
<li>if $v'_i \in S$ then set $x_i = F$</li>
</ul>
</li>
<li>This yields the required assignment, next we demonstrate that it is satisfying </li>
</ul>
<p>We just saw, if we take a solution to the subset sum instances that we specified, then using the first n digits that specifies a true-false assignment for the n variables $x_1$ through $x_n$.</p>
<p>Now, using the remaining m digits, we're going to show that that assignment that we just specify corresponds to a satisfying assignment. It satisfies the input formula f. The first n digits correspond to the variables, and the next m digits correspond to clauses. So let's look at digit n+j, where j varies between one and n, so digit n+j corresponds to clause $C_j$.</p>
<p>In our definition of the subsets sum instance, we put a three in digit n+j of t. So we need to achieve a sum of 3 in digit n+j.  In order to get a sum of three in n+j, we have to include at least one of the numbers corresponding to the literal appearing in clause $C_j$. If we satisfy exactly one literal in $C_j$, then we use both $s_j$ and $s'_j$ as buffer numbers and get 3. If we satisfy exactly two literals in this clause, then we either include $s_j$ and $s'_j$ in our subset S, and that gives us a sum of three. If we satisfy exactly three literals in this clause, then we don't need to use either of these buffer numbers, S_j or S_j prime. Just using these three satisfied literals gives us a sum of three in this digit n plus j. If we satisfy zero of the literals in this clause, then there is no way to achieve a sum of three, thus we don't have a solution to the subset sum problem. We're assuming we have a solution, therefore we must have at least one satisfied literal in this clause. Thus, C_j is satisfied and thus, every clause is satisfied. And therefore, we have a satisfying assignment for f.</p>
<p>So we've shown that a solution to this subset sum instance corresponds to a satisfying assignment for our 3SAT formula. And we've actually shown how to transform this solution to the subset sum problem to find the satisfying assignment to the 3SAT's formula. Now let's prove the reverse implication. Let's prove that if we have a satisfying assignment for this 3SAT formula, it corresponds to the solution to the subset sum problem.</p>
<p>$\impliedby$ Let's take a satisfying assignment for f<br/>
We'll construct a solution to the subset-sum instance in the following manner.</p>
<ul>
<li>In our satisfying assignment<ul>
<li>if $x_i = T$: Add $v_i$ to S</li>
<li>if $x_i = F$: Add $v'_i$ to S</li>
<li>This means that the i'th digit of t is exactly 1, so the i'th digit of t is correct</li>
</ul>
</li>
<li><p>Now let's look at the jth clause.</p>
<ul>
<li>for clause $C_j$, $\ge 1$ literal in $C_j$ is satisfied</li>
<li>The sum corresponding to these literals give us a sum of 1, 2, or 3 in digit n+j. </li>
<li>but recall that t has a 3 in each of the n+j digits</li>
<li>so we can use S to the jth and/or S to the jth prime to get up to a sum of three</li>
<li>This ensures that digit n+j is correct and thus the last m digits are correct and the first digits are correct.</li>
</ul>
<p>Therefore, we have a solution to the subset-sum instance.  So we've proved this equivalence and we've proved that our reduction is correct.</p>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Knapsack">Knapsack<a class="anchor-link" href="#Knapsack">¶</a></h3><p>We leave it to the reader to prove that the knapsack problem is NP-Complete.<br/>
Some hints</p>
<ul>
<li>Use/build off the subset sum problem ( This will be your known NP-Complete problem )</li>
<li>Use the correct version of Knapsack ( the one that lies in NP )</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="NP5:-Halting-Problem">NP5: Halting Problem<a class="anchor-link" href="#NP5:-Halting-Problem">¶</a></h2><p><a href="https://en.wikipedia.org/wiki/Halting_problem">Wikipedia</a></p>
<p>We've now seen many NP-complete problems. When a problem is NP-complete it signifies to us that it's computationally difficult. Formally what does that mean? That means it is the most difficult problem in the class NP. So, if we can solve this NP-complete problem in polynomial time, then we can solve all problems in the class NP in polynomial time. But there is literally thousands of problems in the class NP from all scientific fields it's unlikely that we're going to derive a polynomial time algorithm for our NP-complete problem.</p>
<p>To be precise if P$\neq$NP, then that implies that there's no algorithm which can run in polynomial time on every input for this NP-complete problem. Notice the important distinction is on "every input". We may have an algorithm which takes polynomial time on some inputs or even on most inputs or almost every input, but there's no algorithm which is guaranteed to take polynomial time on every input.</p>
<p>Now we're going to look at the class of undecidable problems. These are problems which are computationally impossible. For an NP-complete problem it's unlikely to have an algorithm which solves the problem in polynomial time. In contrast, for an undecidable problem there is no algorithm which solves the problem on every input regardless of the running time of the algorithm. You run polynomial time, exponential time; there's no algorithm which is going to solve it on every input.</p>
<p>Now in 1936 the great Alan Turing proved that the halting problem is undecidable. And we're going to see the idea of that result now. Now this paper by Turing in 1936 introduced the notion which we now refer to as Turing Machine what Turing showed is that the halting problem is undecidable on a Turing Machine and a Turing Machine captures the power of a conventional computer. Now by conventional we're excluding things like quantum computer.
Now later, many other problems were showed to be undecidable, but the halting problem is quite nice, so we're going to dive into that proof.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Description">Description<a class="anchor-link" href="#Description">¶</a></h3><p>Input to the Halting problem: Is a program P, and an input I</p>
<ul>
<li>This input can be almost anything computable. Pseudocode, python program or lisp. It's not limited to any language, although you can restrict it for some instance of the problem.  </li>
</ul>
<p>Output of the Halting problem: True/False indicative of the program termination on the input</p>
<ul>
<li>In simpler terms all the Halting problem is concerned with is whether or not the program will terminate on the input I or if it will enter an infinite loop. It is not concerned with correctness. Another way to think of this is that the Halting problem answers the question: Will this program ever end?  </li>
</ul>
<h3 id="Example">Example<a class="anchor-link" href="#Example">¶</a></h3><p>Let's consider a simple program in C</p>
<div class="highlight"><pre><span></span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">){</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">}</span>
</pre></div>
<p>Suppose we use the input x=5, then we can easily see that x will always be odd, hence it will never terminate.</p>
<ul>
<li>so Halting(P,5) = False, since the program never ends using the input 5</li>
</ul>
<p>Now, let's prove the theorem that the halting problem is undecidable. The way we go about proving this theorem is by contradiction.</p>
<p><strong>PROOF Outline</strong></p>
<p>Suppose that we had an algorithm that solves the halting problem on every input. Now we're going to derive a contradiction and therefore, our assumption that there exists an algorithm which solves the halting problem is not true. And therefore, there is no algorithm which solves the halting problem.</p>
<p>What we're going to do is for this particular algorithm which solves the halting problem, we're going to construct an input for which this algorithm is incorrect. And therefore, our assumption that this algorithm solves the halting problem on every input is incorrect. thus we have a contradiction and have proven the theorem.</p>
<h3 id="Proof">Proof<a class="anchor-link" href="#Proof">¶</a></h3><p>So let's call this algorithm "Terminator". Terminator takes a pair of inputs, P and I. where P is a program, and I is an input for this program, and Terminator outputs true or false depending on whether this program P, on this particular input I, terminates eventually or not.</p>
<ul>
<li>If it eventually terminates, then an output is true.</li>
<li>If it has an infinite loop, then an output is false.</li>
</ul>
<p>Recall that we assumed that Terminator is correct, ie it solves the halting problem for every program P, and every input I.</p>
<p>Now we're going to construct a program Q and an input J, and we're going to show that when we run Terminator on this input pair Q, J, then its output/result is incorrect. Since Terminator is incorrect on this pair of inputs, therefore, Terminator does not solve the halting problem on every input. This gives us our contradiction, and therefore, that would complete the proof by contradiction.</p>
<p>How can we hope to construct this program Q? Well, one important piece is that we're assuming the existence of this program, Terminator. So we can use this algorithm, Terminator as a subroutine in our new algorithm Q. Now I don't know anything about the inner workings of Terminator so I have to use it as a black box, but I can use this as a subroutine. So we're going to use it as a subroutine to get our paradox or contradiction.</p>
<p>Let's consider the following constructed program</p>
<pre><code>HARMFUL(J):
1:    if TERMINATOR(J,J)  /* J is both the program as well the input */ 
2:        then GOTO(1)
3:     else return</code></pre>
<p>Remember that Terminator here is being used as a black box. All that matters is that it returns a boolean. Furthermore there is bit of a paradox happening here. In order to hit the else condition, terminator must return false. Which would imply that the terminator program encountered an infinite loop .... so how did it return?</p>
<p>(Incidentally: This program is named after an article by Dijkstra in 1968: "goto statement considered harmful")</p>
<p>Terminator(J,J): runs program J on input J</p>
<ul>
<li>returns TRUE if J(J) terminates</li>
<li>returns FALSE if J(J) never terminates</li>
</ul>
<p>This leads to two possible scenarios which we must examine</p>
<ul>
<li>if J(J) terminates: then Harmful(J) will never terminate</li>
<li>if J(J) never terminates: then Harmful(J) will terminate</li>
</ul>
<p>What if we passed the Harmful program to the Harmful program? ie HARMFUL(HARMFUL)</p>
<p>Let J = HARMFUL then</p>
<ul>
<li>Scenario A: Harmful(J) terminates then Harmful(Harmful) never terminates<ul>
<li>an obvious contradition</li>
</ul>
</li>
<li>Scenario B: Harmful(J) never terminates then Harmful(Harmful) terminates    <ul>
<li>another obvious contradiction</li>
</ul>
</li>
</ul>
<p>In both cases we have a contradition so the assumption that Terminator() exists is impossible.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="LP0:-Linear-Programming">LP0: Linear Programming<a class="anchor-link" href="#LP0:-Linear-Programming">¶</a></h2><p>Linear programming is a very general technique for solving optimization problems.
It handles any problem that can be formulated as an optimization over a set of variables with
a goal known as the objective function and the constraints can all be expressed as linear functions of the variables.</p>
<p>We'll see that max flow can be formulated as a linear program.
Also, we'll see that many problems from operations research can be expressed as LPs.
We'll describe the general idea of the simplex algorithm which is widely used for solving LPs.</p>
<p>We'll also look at one of the beautiful aspects of linear programming, LP duality.</p>
<p>Finally, we'll look at an application of LP to approximation algorithms. We'll use linear programming to design
an approximation algorithm to the classic SAT problem.</p>
<p>Let's dive in!!</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="LP1:-Linear-Programming">LP1: Linear Programming<a class="anchor-link" href="#LP1:-Linear-Programming">¶</a></h2><p>Let's begin by looking at how we express max flow as a linear program.</p>
<p>Recall, for the Max-Flow problem</p>
<ul>
<li>Input: Directed Graph G(V,E) with $c_e &gt; 0$ for $e \in E$</li>
</ul>
<p>Our LP formulation will be somewhat different. We'll have m variables, one for each flow along an edge. $f_e$ for each $e \in E$. Furthermore we'll have an objective function that we are trying optimize, Which should be a linear function of the variables.</p>
<p>Recall that in a max flow, we're trying to maximize the size of the flow, as measured by the size of the flow (a) out of the source vertex, or (b) into the sink vertex. Which one we choose doesn't matter as they must be the same.</p>
<ul>
<li>Objective Function: $max(\sum_{\overset{\rightarrow}{sv}} f_{sv})$</li>
<li>Constraints: <ul>
<li>for every $e \in E \; 0 \le f_e \le c_e$ - the flow can be at most the capacity</li>
<li>for every $v \in V - \{s,t\}$ <ul>
<li>$\sum_{\overset{\rightarrow}{wv}} f_{wv} = \sum_{\overset{\rightarrow}{vz}} f_{vz}$ Flow in == Flow out of v</li>
<li>flow must be conserved</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Consider the following toy example which models a basic production/manufacturing problem</p>
<h3 id="2Dim-Example">2Dim Example<a class="anchor-link" href="#2Dim-Example">¶</a></h3><p>Problem Statement</p>
<ul>
<li>A company makes products A &amp; B. We'd like to determine how many of each should we sell to maximize profit?</li>
</ul>
<p>Details</p>
<ul>
<li>The profit on each unit of A is \$1, and for B it is \\$6</li>
<li>The demand for A is $\le 300$ units, and for B it is $\le 200$ units</li>
<li>We have a total of 700 hours capacity among our employees<ul>
<li>a unit of A requires 1 hour </li>
<li>a unit of B requires 3 hours</li>
</ul>
</li>
</ul>
<p>In order to move forward we need to format this as an LP problem and we need to formulate our objective function.</p>
<ul>
<li>let $x_1$ be the # of units of A to produce in a day</li>
<li>let $x_2$ be the # of units of B to produce in a day</li>
<li>then our objective function is: $1 x_1 + 6 x_2$, which we will maximize</li>
</ul>
<p>Of course we can't forget about our constraints right?</p>
<ul>
<li>(supply) $1 x_1 + 3 x_2 \le 700$, this ensures we have the available man power </li>
<li>(demand) $0 \le x_1 \le 300$ and $0 \le x_2 \le 200$ to ensure we don't make more than the possible demand</li>
</ul>
<p><strong>Formally</strong></p>
<ul>
<li>Maximize $x_1 + 6x_2$</li>
<li>Subject to the constraints<ul>
<li>$x_1 \le 300$</li>
<li>$x_2 \le 200$</li>
<li>$x_1 \ge 0$</li>
<li>$x_2 \ge 0$</li>
<li>$x_1 + 3x_2 \le 700$</li>
</ul>
</li>
</ul>
<p>For a minute let's consider this from a geometric point of view in a 2 dimensional plane. (2dim for the 2 variables). For now let's ignore the objective function and just look at the five constraints. Each of the five constraints is a half plane. (recall that equality would produce a line, so an inequality produces a half plane containing all feasible values). In this situation all 5 constraints will come together to form a bounded area that is the intersection of all their planes. All points in the intersection are the feasible values of X where X is a solution to the objective function. Of course only one of these will be a max, optimal, value.</p>
<p><img src="CS6515_images/LP-001.png" width="350;"/></p>
<p>Consider the above diagram which illustrates our problem geometrically. Now we can apply our objective function using values inside our feasible region.</p>
<p>Example 1: $x_1 + 6x_2 = 600$ is one solution in the feasible region but is certainly not optimal.<br/>
Example 2: $x_1 + 6x_2 = 1300$ is another solution and is in fact the optimal solution.</p>
<p><img src="CS6515_images/LP-002.png" width="350;"/></p>
<p>Now, this was a simple two dimensional example. Let's look at a simple three dimensional example and then we'll get some idea of how it generalizes to higher dimensions.</p>
<p>Let's pause for a moment to consider from key issues that could arise:</p>
<ul>
<li>Handling fractional values. In our example we got a bit lucky in that the optimal value occured at an integer valued point. But this is not always the case. If you only want integer values then you can round fractions to the closest feasible integer. </li>
</ul>
<p>Key Point(1) here is that Linear programming optimizes over the entire feasible set and as such is solvable in polynomial time. (ie $LP \in P)$. But the problem of looking for the optimal integer valued solution is NP-complete. (ie integer-LP is NP-Complete).</p>
<p>Key Point(2) here is that in our problem the optimal solution lies at a vertex, or corner, of our feasible space. Suppose this optimal line intersects a point such z which is between two vertices, so it is not a vertex itself.  Therefore, it must be the case that one of these two endpoints of this edge must be better than this point Z, or it has to be the case that this optimal line intersects this edge. If this optimal line intersects this edge, then all the points on this edge are optimal. Therefore, the two endpoints are both optimal and therefore an optimal point lies at a vertex.</p>
<p>The last important concept is convexity. This feasible region, this tan region is convex. Because the feasible region is defined by the intersection of half spaces, it must be a convex set.</p>
<p>Since it's convex, if we have a vertex such as this point (100, 200) which is better than its neighbors, these two points, well, the feasible region can't go down and then back up.</p>
<p>So, if this point is better than its neighbors, then the entire convex region, the entire feasible region is below these two lines. Therefore, this point is optimal because if a point is better than its neighbors, then that vertex is an optimal point.</p>
<p><strong>That's the key point of convexity, the optimal point always lies at a vertex of this feasible region.</strong> There may be other points that are also optimal, but there will always be a vertex which is optimal.</p>
<p>These two key points lead to the simplex algorithm which we'll detail later. For now we will give a brief sketch.</p>
<ul>
<li>We'll start at some vertex of the polygon.</li>
<li>We'll check its neighbors and we'll look if any of its neighbors are better.</li>
<li>Then we'll move to a neighbor which has a better objective function</li>
<li>we continue until we find a vertex of the polygon in the feasible region <ul>
<li>which is better than all of its neighbours</li>
<li>and is therefore optimal</li>
</ul>
</li>
</ul>
<p>That's the basic idea of the simplex algorithm. Now let's move on to another three dimensional example.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="3Dim-Example">3Dim Example<a class="anchor-link" href="#3Dim-Example">¶</a></h3><p>Let's twist the previous example and make it a bit harder.</p>
<ul>
<li>Three products A,B, C</li>
<li>Profit: \$1 for A, \$6 for B, and \$10 for C</li>
<li>demand: $\le 300$ for A, $\le 200$ for B, unlimited demand for C</li>
<li>hourly Supply: $\le 1000$ in total, A takes 1, B takes 3, C takes 2</li>
<li>Packaging: $\le 500$ in total, B takes 1 and C takes 3, A takes nothing</li>
</ul>
<p>We formulate as an LP problem. Using three variables: x1 for A, x2 for B, x3 for C</p>
<ul>
<li>we want to max(x1 + 6x2 + 10x3)</li>
<li>subject to the constraints<ul>
<li>demand : $x1 \le 300$ and $x2 \le 200$</li>
<li>supply: $x1 + 3x2 + 2x3 \le 1000$</li>
<li>Packaging: $x2 + 3x3 \le 500$</li>
</ul>
</li>
<li>$x1,x2,x3 \ge 0$ </li>
<li>we always add this as it doesn't make sense otherwise</li>
</ul>
<p>Here's a geometric view
<img src="CS6515_images/NP-019.png" width="450;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Standard-Formulation">Standard Formulation<a class="anchor-link" href="#Standard-Formulation">¶</a></h3><p>By now you should see a bit of pattern emerging in these types of problems.<br/>
Now we look at how to formulate an arbitrary problem.</p>
<ul>
<li>for variables $x_1, x_2, ... x_n$</li>
<li>your objective function will be of the form<ul>
<li>max(c1*x1 + c2*x2 + ... + cn*xn)</li>
</ul>
</li>
<li>you're contraints will be of the form<ul>
<li>a11*x1 + ... a1n*xn $\le$ b1</li>
<li>you can have up to m constraints</li>
<li>am1*x1 + ... amn*xn $\le$ bm</li>
</ul>
</li>
<li>of course, never forget your non negativity constraint<ul>
<li>x1,x2,...,xn $\ge$ 0</li>
</ul>
</li>
</ul>
<p>This is pretty annoying to type out so let's switch to matrix notation</p>
<ul>
<li>Variable Vector = $x^T$ which is our variables x1, x2, ... xn</li>
<li>Co-efficients = $c^T$</li>
<li>then our objective function is $max(c^T x)$ ( we will always use max - see comments below )</li>
<li>similarly our contraints can be formulated as <ul>
<li>$A x \le b$ </li>
<li>where A is the constraint coefficients, m x n Matrix</li>
<li>b is the boundary values vector</li>
<li>x is the variable vector </li>
</ul>
</li>
<li>our non-negativity constraint is straigh forward<ul>
<li>$x \ge 0$</li>
</ul>
</li>
</ul>
<p>Some Considerations to formulate a problem into the standard form above:</p>
<ul>
<li>For minimum problems multiply by -1 and maximize</li>
<li>for a problem of the form $A x \ge b$, where b is the lower bound, <ul>
<li>we once again multiply by -1 to get the equivalent $(-1) A x \ge b$  </li>
</ul>
</li>
<li>for an equality problem $A x = b$<ul>
<li>we use two objective functions</li>
<li>$(-1) A x \ge b$  </li>
<li>and $ A x \ge b$  </li>
</ul>
</li>
<li>what about a constraint like $x &lt; 100$<ul>
<li>NOT ALLOWED - LP does not allow for this as it is ill-defined. LP problems must be well defined</li>
</ul>
</li>
<li>What about an unconstrained x? when the problem doesn't force it to be positive nor negative<ul>
<li>This one is trickier, here we set $x = x^+ - x^-$</li>
<li>where $x^+$ and $x^-$ represent the magnitude of x</li>
<li>and $x^+ \ge 0$</li>
<li>and $x^- \ge 0$</li>
</ul>
</li>
</ul>
<p>Geometric View</p>
<ul>
<li>We have n variables meaning n dimensions</li>
<li>n+m constraints</li>
<li>what is our feasible region?<ul>
<li>Each constraint represents a half space</li>
<li>and the feasible region is the area constrained by the n+m constraints</li>
<li>so the feasible region is the region inside the n+m half spaces</li>
<li>this is a convex polyhedron</li>
</ul>
</li>
</ul>
<p>Let's now consider the vertices of the feasible region</p>
<ul>
<li>is the set of points satisfying</li>
<li>(1) n constraints with = (equality)</li>
<li>and (2) m constraints with $\le$ (inequality)</li>
<li>this works out to ... $\le {n+m \choose n}$ vertices<ul>
<li>this is exponential in n</li>
</ul>
</li>
<li>Further it has at most nm neighbours<ul>
<li>$\le nm$ neighbours</li>
</ul>
</li>
</ul>
<p>LP Algorithms:
Polynomial-time</p>
<ul>
<li>Ellipsoid algos</li>
<li>interior point methods</li>
</ul>
<p>Exponential time:</p>
<ul>
<li>Simplex Methods<ul>
<li>this is by far the most widely used despite it's terrible runtime</li>
<li>guaranteed to return the optimal solution</li>
<li>Works great on huge LP problems</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Simplex-Method">Simplex Method<a class="anchor-link" href="#Simplex-Method">¶</a></h3>
<pre><code>Start at x=0
look for neighbour vertex with higher objective value
if found then move to neighbor vertex
    go back to look
else 
    output current vertex x

# the x that is returned
# is guaranteed to be the global optimal</code></pre>
<p>In case you didn't notice the algo is walking from vertex to vertex in a strictly uphill fashion. The optimal value is at the top of this hill.</p>
<h3 id="Simplex-Example-3Dim">Simplex Example 3Dim<a class="anchor-link" href="#Simplex-Example-3Dim">¶</a></h3><p>Let's demonstrate simplex on our 3dim problem from earlier</p>
<p>Recall : We formulate as an LP problem. Using three variables: x1 for A, x2 for B, x3 for C</p>
<ul>
<li>we want to max(x1 + 6x2 + 10x3)</li>
<li>subject to the constraints<ul>
<li>$x1 \le 300$ </li>
<li>$x2 \le 200$</li>
<li>$x1 + 3x2 + 2x3 \le 1000$</li>
<li>$x2 + 3x3 \le 500$</li>
<li>$x1,x2,x3 \ge 0$ </li>
</ul>
</li>
</ul>
<pre><code>start             profit
(0  ,  0,  0)      0
(300,  0,  0)    300
(300,200,  0)   1500
(300,200, 50)   2000
(200,100,100)   2400     
    -&gt; from here there are no neighbours with a higher obj value
    -&gt; t/f we declare this to be the optimal point</code></pre>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="LP2:-LP-Geometry">LP2: LP Geometry<a class="anchor-link" href="#LP2:-LP-Geometry">¶</a></h2><p>Recall our basic LP formulation:</p>
<ul>
<li>For n variables</li>
<li>we may have up to m constraints in the form $Ax \le b$ with $x \ge 0$</li>
<li>an objective function of the form $max(c^T x)$</li>
</ul>
<p>With the characteristics</p>
<ul>
<li>the feasible region is a convex set<ul>
<li>where the feasible region is the set of x values that satisfy the m constraints</li>
</ul>
</li>
<li>each m constraint defines a half space</li>
<li>the intersections of these regions is the vertices</li>
</ul>
<p>What we get when the above is true is a convex polyhedron</p>
<p>The simplex method walks along the vertices, or corners, of the polyhedron. In general the optimum will always lie at a vertex of the feasible region. However, there are a few situations which provoke an exception.</p>
<ul>
<li>Exception 1: when the LP is infeasible<ul>
<li>this can happen when the feasible region is blank/empty</li>
<li>consider<ul>
<li>func: max(5x-7y)</li>
<li>st: $x + y \le 1$, $3x + 2y \ge 6$, $x,y \ge 0$</li>
</ul>
</li>
<li>take the time to draw this out to see that the feasible region is in fact empty</li>
<li>infeasiblity has nothing to do with the obj function</li>
</ul>
</li>
</ul>
<p><img src="CS6515_images/LP-003.png" width="350;"/></p>
<ul>
<li>Exception 2: When the LP is unbounded<ul>
<li>this can happen when the optimal is arbitrarily large</li>
<li>consider<ul>
<li>func: max(x+y)</li>
<li>st: $ x - y \le 1$, $x + 5y \ge 3$, $x,y \ge 0$</li>
</ul>
</li>
<li>if you were to draw this out you'd see that the objective function can keep increasing as y increases up the y-axis</li>
<li>consider, a slight change to the previous example<ul>
<li>func: $max(2x - 3y)$</li>
<li>st: $ x - y \le 1$, $x + 5y \ge 3$, $x,y \ge 0$</li>
</ul>
</li>
<li>in this situation there is in fact in fact an optimum</li>
<li>if you were to draw this out you would see a feasible region similar to the previous configuration</li>
<li>However, the maximum of the objective function lies at the very bottom vertex</li>
</ul>
</li>
</ul>
<p>There is a key difference between $\max(x+y)$ and $\max(2x - 3y)$. Consider what the value is as x and y increase? In the first case the result is an increasing value, hence the difficulty.</p>
<p>Both of these situations beg the question, how can we determine if a region is feasible/bounded? For now we consider the feasibility. In the next section we look at handling unbounded objective through the use of LP Duality</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Feasiblity">Feasiblity<a class="anchor-link" href="#Feasiblity">¶</a></h3><p>In order to determine if an LP is feasible we begin with our LP in standard form:</p>
<ul>
<li>$max(c^T x)$</li>
<li>Such that<ul>
<li>$Ax \le b$, and </li>
<li>$x \ge 0$</li>
</ul>
</li>
</ul>
<p>Is there an x that satisfies all the constraints?</p>
<p>Here's how to answer this question:</p>
<ul>
<li>Step 1: make a new variable z ( not a vector, just a single variable )</li>
<li>Step 2: Consider a single constraint<ul>
<li>say $a_1 x_1 + a_2 x_2 + \cdots + a_n x_n \le b$ </li>
<li>and $x_1,x_2,\cdots,x_n \ge 0$</li>
</ul>
</li>
<li>Step 3: Add z into our constraint<ul>
<li>i.e. $a_1 x_1 + a_2 x_2 + \cdots + a_n x_n + z \le b$</li>
<li>we make no change to the nonnegativity constraint</li>
<li>z is unconstrained and can take on any value whatsoever (positive or negative)</li>
<li>if z is sufficiently close to zero we get the trivial solution</li>
<li>so we set $z = -\infty $ ( but in fact it will be a finite number and not actually infinity</li>
</ul>
</li>
<li>Step 4: find a solution<ul>
<li>to $a_1 x_1 + a_2 x_2 + \cdots + a_n x_n + z \le b$</li>
<li>when $z \ge 0$<ul>
<li>if a solution can be found when z is greater than 0 </li>
<li>then you can drop z and still have a solution</li>
</ul>
</li>
</ul>
</li>
<li>Step 5: Generalize to all constraints<ul>
<li>if you found a z in step 4 then you need to do this for all coefficients $a_1$ to $a_n$</li>
<li>In other words you need to find a $z \ge 0$ for the modified LP <ul>
<li>max(z)</li>
<li>for $Ax + z \le b$ </li>
<li>with $x \ge 0$</li>
</ul>
</li>
<li>This new LP will always be feasible, </li>
<li>partly because there is always a trivial solution for a small enough z</li>
<li>however, only when z is greater than 0 can the solution carry over to the original LP<ul>
<li>because we can just ignore/drop the z term</li>
</ul>
</li>
<li>if the optimal value of z is negative, <ul>
<li>then we can say the original LP is infeasible</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="LP-Duality">LP Duality<a class="anchor-link" href="#LP-Duality">¶</a></h3><p>Overview</p>
<ul>
<li>Motivating example to build our intuition</li>
<li>General Form </li>
<li>Weak duality Theorem</li>
<li>Strong duality Theorem</li>
</ul>
<p>recall our earlier example</p>
<ul>
<li>we want to max(x1 + 6x2 + 10x3)</li>
<li>subject to the constraints<ul>
<li>$x1 \le 300$ </li>
<li>$x2 \le 200$</li>
<li>$x1 + 3x2 + 2x3 \le 1000$</li>
<li>$x2 + 3x3 \le 500$</li>
<li>$x1,x2,x3 \ge 0$ </li>
</ul>
</li>
</ul>
<p>Also recall that we ran the simplex method to find that</p>
<ul>
<li>optimal point = (200,200,100)</li>
<li>with objective value = 2400, profit</li>
</ul>
<p>So how do we verify that this is an optimal point? without running the algo again. What if someone told us this was the optimal solution, how can we check their claim?</p>
<p>What we will try to do is find an upper bound on the objective function, such that the upper bound value is equal to/at most our profit solution. This will imply that our solution is in fact a maximum.</p>
<p>For our above example</p>
<ul>
<li>consider  y = $(y_1,y_2,y_3,y_4) = (0,1/3,1,8/3)$ ( we will show later how we came up with these values )</li>
<li>here y is 4 dimensional </li>
</ul>
<p>for the purposes of our explanation we will number the number the constraints from above</p>
<ul>
<li><ol>
<li>$x1 \le 300$ </li>
</ol>
</li>
<li><ol>
<li>$x2 \le 200$</li>
</ol>
</li>
<li><ol>
<li>$x1 + 3x2 + 2x3 \le 1000$</li>
</ol>
</li>
<li><ol>
<li>$x2 + 3x3 \le 500$</li>
</ol>
</li>
<li><ol>
<li>we ignore the non-negative constraint</li>
</ol>
</li>
</ul>
<p>Now we know that any feasible point must satisfy all these constraints, but it must also satisfy any linear combination of the constraints</p>
<ul>
<li>so for example $y_1*(1) + y_2*(2) + y_3*(3) + y_4*(4)$ should also be satified</li>
</ul>
<p>Let's write this all out</p>
<ul>
<li>$y_1*(1) + y_2*(2) + y_3*(3) + y_4*(4)$</li>
<li>$\iff y_1 x1 + y_2 x2 + y_3(x1 + 3x2 + 2x3) + y_4(x2 + 3x3) \le 300 y_1 + 200 y_2 + 1000y_3 + 500y_4 $<ul>
<li>what we've done here is multiply the constraints by the y vector in rowwise fashion to get the linear combination</li>
<li>each constraint, yn represents a row/constraint</li>
</ul>
</li>
<li>Now we can simplify and collect terms</li>
<li>$\iff x1(y_1+y_3) + x2(y_2+3y_3+y_4) + x_3(2y_3+3y_4) \le 300 y_1 + 200 y_2 + 1000y_3 + 500y_4$</li>
<li>Now we use the values to reduce </li>
<li>for $(y_1,y_2,y_3,y_4) = (0,1/3,1,8/3)$</li>
<li>we get $x_1 + 6x_2 + 10x_3 \le 2400 $</li>
<li>pretty funny right? </li>
<li>We did all that work just to come up with the obj function of the original LP in our example. of course that's the point though. </li>
<li>In doing so we have verified our original solution</li>
</ul>
<p>Let's turn our attention back to our y values. how did we come up with the y values? There must be some logic right?</p>
<p>It should be noted that any solution that is at least the original obj value function is also valid so long as the inequality still holds. ie $x_1 + 9x_2 + 25x_3$ is also valid because it still serves as an upper bound on the obj function</p>
<p>What we need is our co-efficients to be at least the coefficients in the obj function.<br/>
t/f</p>
<ul>
<li>$(y_1 + y_3) \ge 1$ due to x1 term </li>
<li>$(y_2 + 3y_3 + y_4) \ge 6$ due to x2 term</li>
<li>$(2y_3 + 3y_4) \ge 10$ due to x3 term</li>
</ul>
<p>Since these serve as an upper bound on the obj function, We will want to determine the smallest upper bound. Which means we want to find the minimum of the right hand side of the inequality.</p>
<ul>
<li>ie $min(300 y_1 + 200 y_2 + 1000y_3 + 500y_4)$</li>
</ul>
<p>Of course this should look very suspicious to you, it's another LP. In fact this is known as the DUAL LP. Where as in the original LP we had $\le$ constraints now we have $\ge$ constraints. In the original LP, aka the PRIMAL LP we had a maximization objective function now we have a minimization function. Of course we know how to adjust an LP to get into a canonical form for solving so solving the dual lp is not much different than solving the original LP.</p>
<p>It pays to take a moment to consider the symmetry between the two LPs.</p>
<ul>
<li>the number of constraints in the primal lp is the number of variables in the dual lp</li>
<li>the number of variables in the primal lp is the number of constraints in the dual lp</li>
<li>the coefficients of the primal objective function are the lower bounds of the constraints in the dual lp</li>
<li>the upper bound constraints in the primal lp are the co-efficients of the dual lp obj. function</li>
<li>the left hand side of the constraints in the dual lp <ul>
<li>are a linear combination of the contributing constraints in the primal lp</li>
</ul>
</li>
</ul>
<p><strong>NB: The Dual LP must also have non-negative constraints!!</strong></p>
<p><img src="CS6515_images/NP-021.png" width="350;"/></p>
<p>Now things can get a little weird</p>
<ul>
<li>Suppose you have a primal lp, call it $LP_{primal}$</li>
<li>then we can find/create the dual LP, call it $LP_{dual}$<ul>
<li>to put it in canonical form we multiply by -1, </li>
<li>results in flipping the inequalities in the constraints</li>
<li>as well as flipping the min to a max in the objective function </li>
</ul>
</li>
<li>Now what's the dual of -1*$LP_{dual}$?</li>
<li>Suppose we compute the dual of the dual   <ul>
<li>we get another dual </li>
<li>but then we go through the transformation process to get it into canonical form</li>
</ul>
</li>
<li>the result should be the original primal LP with a third variable<ul>
<li>this third LP is for all intents and purposes the original primal LP</li>
</ul>
</li>
</ul>
<p><strong>NP: The dual of the dual LP is simply the primal LP</strong><br/>
<strong>Dual(Dual) = Primal</strong></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Example">Example<a class="anchor-link" href="#Example">¶</a></h4><p>You are given the following LP</p>
<ul>
<li>$max(5x_1-7x_2+2x_3)$</li>
<li>Subject to <ul>
<li>$x_1 + x_2 - 4x_3 \le 1$</li>
<li>$2x_1 - x_2 \ge 3 $</li>
<li>$x_1,x_2,x_3 \ge 0$</li>
</ul>
</li>
</ul>
<p>You are asked the following questions:</p>
<ul>
<li><ol>
<li>How many variables/contraints are in the dual LP</li>
</ol>
</li>
<li><ol>
<li>What is the dual LP?</li>
</ol>
</li>
</ul>
<ol>
<li>Should be rather straight forward. </li>
</ol>
<ul>
<li>There will be 2 variables in the dual LP, one for each constraint</li>
<li>There will be 3 constraints in the dual LP, one for each of the primal LPs variables</li>
</ul>
<ol>
<li>Computing the dual LP is purely mechanical.</li>
</ol>
<ul>
<li>First we re-write the primal LP in canonical form </li>
<li>$max(5x_1 - 7x_2 + 2x_3)$</li>
<li>Subject to <ul>
<li>$  x_1 + x_2 - 4x_3 \le 1$</li>
<li>$-2x_1 + x_2 \le -3 $ - we multiplied by -1 to flip the inequality</li>
<li>$  x_1,x_2,x_3 \ge 0$</li>
</ul>
</li>
</ul>
<p>now we can begin filling out the coefficients of the dual LP</p>
<ul>
<li><ol>
<li>we begin with our shell<ul>
<li>$min(a y_1 + b y_2)$ s.t. $? \ge ?$; $? \ge ?$; $? \ge ?$; $y_1,y_2 \ge 0$ </li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>we can read off the lower bounds from the original LP    <ul>
<li>$min(a y_1 + b y_2)$ s.t. $? \ge 5$; $? \ge -7$; $? \ge 2$; $y_1,y_2 \ge 0$ </li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>the constraints upper bounds become the dual LP obj function co-efficients<ul>
<li>$min(1 y_1 + -3 y_2)$ s.t. $? \ge 5$; $? \ge -7$; $? \ge 2$; $y_1,y_2 \ge 0$ </li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>Now we can fill in the co-efficients of the constraints by looking at each variable in the primal<ul>
<li>think of this in terms of matrices, all the x1 variables should be a single column</li>
<li>$y_1 + -2y_2  \ge 5$; </li>
<li>$y_1 + y_2 \ge -7$; </li>
<li>$-4y_1 \ge 2$; </li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>Finally we can put this all together in a single LP. All we're doing here is clean up and presentation, no math :)<ul>
<li>$min(1 y_1 + -3 y_2)$ </li>
<li>Subject to <ul>
<li>$y_1 + -2y_2  \ge 5$; </li>
<li>$y_1 + y_2 \ge -7$; </li>
<li>$-4y_1 \ge 2$; </li>
<li>$y_1,y_2 \ge 0$</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="LP3:-Duality---Weak-and-Strong">LP3: Duality - Weak and Strong<a class="anchor-link" href="#LP3:-Duality---Weak-and-Strong">¶</a></h2><p><strong>Weak Duality Theorem</strong><br/>
For a feasible point x for the primal lp, and a feasible point y for the dual LP</p>
<ul>
<li>we have $c^T x \le b^T y$ <ul>
<li>ie $LP_{primal}(x) \le LP_{dual}(y)$ for feasible points x and y</li>
</ul>
</li>
<li>in other words the dual LP at any feasible point is an upper bound on the primal LP</li>
</ul>
<p><strong>Corollary</strong></p>
<ul>
<li>if $c^T x = b^T y$ for feasible points x,y then x,y are optimal<ul>
<li>it should be stressed that this is a one-way implication. </li>
<li>two optimal points need not imply equal objective values</li>
<li>there are certain conditions where it is a two way implication and that is called the strong duality theorem. this will be covered a few lines below</li>
</ul>
</li>
</ul>
<p><strong>Corollary 2</strong></p>
<ul>
<li>if the primal LP $c^T x$ is unbounded then the dual LP is infeasible</li>
<li>similarly </li>
<li>an unbounded dual LP would imply an unfeasible primal lp, since the primal is the dual of the dual     </li>
</ul>
<p>You should be able to see why this is true. An unbounded lp cannot have an upper bound</p>
<p>Recall how we check an LP</p>
<ul>
<li>Given an LP in canonical form<ul>
<li>$max(c^T x)$ subject to $Ax \le b$ and $x \ge 0$</li>
</ul>
</li>
</ul>
<ul>
<li>to check for feasiblity<ul>
<li>we add an extra variable z to the constrain ie $Ax + z \le b$ </li>
</ul>
</li>
</ul>
<ul>
<li>to check for unboundedness<ul>
<li>we could check if the dual is feasible or not</li>
<li>if the dual is infeasible then by weak duality the primal could be infeasible or it could be unbounded<ul>
<li>so we check if the primal is feasible</li>
<li>assuming it has one feasible point</li>
<li>then we check if the dual is infeasible</li>
</ul>
</li>
<li>if the primal has at least one feasible point, then all that is left in unbounded</li>
</ul>
</li>
</ul>
<p><strong>Strong Duality Theorem</strong></p>
<ul>
<li>Primal LP is feasible and bounded $\iff$ dual LP is feasible and unbounded </li>
<li>Equivalently</li>
<li>Primal LP has an optimal solution x<em> $\iff$ Dual LP has an optimal solution y</em><ul>
<li>ie $c^T x^* = b^T y^*$</li>
</ul>
</li>
</ul>
<p>Although it is not very obvious the max-flow min-st.cut theorem is a mirror/reflection of this theorem. It turns out that</p>
<ul>
<li>$c^T x^*$ - is the max-flow through a flow network</li>
<li>$b^T y^*$ - is the capacity of the min-st.cut</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="LP4:-Max-SAT-Approximation">LP4: Max-SAT Approximation<a class="anchor-link" href="#LP4:-Max-SAT-Approximation">¶</a></h2><p>In several sections already we've looked at variations of the SAT problem. In the chapter we will look at yet another version called the max sat.</p>
<p>SAT Input/output ( Search Problem )</p>
<ul>
<li>IN : Boolean formula f in CNF form with n variables and m clauses</li>
<li>OUT: Assignment satisfying f or NO if no assignment exists</li>
<li>Is NP-Complete</li>
</ul>
<p>Max-SAT is just a slight variation (Optimization problem)</p>
<ul>
<li>IN : same as before</li>
<li>OUT: Assignment that maximizes the # of satisfied clauses</li>
<li>Is NP-Hard!!</li>
</ul>
<p>This is still a Hard problem, but because it is no longer a search problem it is no longer in the class NP. We have no way of verifying that the number of clauses satisfied is maximized, thus it cannot be in P. But also Max-SAT is at least as hard as SAT. It's straight forward to reduce SAT to Max-SAT.</p>
<p>Since Max-SAT is np-hard we cannot hope to solve this in polynomial time so we will attempt to approximate it using LP Programming.</p>
<p>Problem Formulation:<br/>
For a formula f with m clauses, let $m^*$ denote the max # of SAT clauses</p>
<ul>
<li>Clearly $m^* \le m$<ul>
<li>when $m^* = m$ then f is satisfiable</li>
</ul>
</li>
</ul>
<p>We will construct an algorithm A on the input f to get l, where l is the number of clauses satisfied by assignment output of A.</p>
<p>So how does l compare to $m^*$? We are going to guarantee that $l \ge \frac{m^*}{2}$. If this holds for every/any input f then we this is a 1/2 approximation algorithm. later we will see how we can push this to formulate a 1/3 approximation.</p>
<p>We will look at three algorithms:</p>
<ol>
<li>A simple algo that results in an 1/2 approx for Max-SAT</li>
<li>An LP based algo that results in a (1-1/e) approximation. This is slightly better than the first</li>
<li>Combo of the above - Best of 2: that results in a 3/4 approximation</li>
</ol>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Simple-Algo">Simple Algo<a class="anchor-link" href="#Simple-Algo">¶</a></h3><p>Consider an input f in CNF form with</p>
<ul>
<li>variables $x_1,...,x_n$</li>
<li>and clauses $c_1,...,c_m$</li>
</ul>
<p>We will choose a random assignment of true/false for each variable, for simplicity we choose each t/f value with probability of 0.5</p>
<p>ie $\text{ Set }x_i = \begin{cases}
        T &amp; \text{with probability 0.5} \\
        F &amp; \text{with probability 0.5} \\
\end{cases}$</p>
<p>Now we measure the performance of this assignment</p>
<ul>
<li>Let w = number of satisified clauses, then</li>
<li>Then we could compute the expected value of w as $E[W] = \sum_0^m l * pr(W=l) $<ul>
<li>if you tried this it would be rather difficult. Problem here is that clauses can contain the same variables thus there is a high correlation that would make computing these probabilities challenging</li>
<li>but we could look at each clause in isolation</li>
</ul>
</li>
</ul>
<p>Let's consider a single clause $c_j$<br/>
$\text{ Then } w_i = \begin{cases}
        1 &amp; \text{if c_j is satisifed} \\
        0 &amp; \text{otherwise} \\
\end{cases}$</p>
<p>If we now sum these Ws we should get the number of satisfied clauses W. ie $W = \sum_{j=0}^m w_j$<br/>
So now we can work some magic on the original expected value as</p>
<ul>
<li>$E[W] = E[\sum_{j=0}^m w_j]$</li>
<li>$= \sum_{j=0}^m E[w_j]$</li>
<li>This is much easier to compute as it is just one clause </li>
<li>$E[w_j] = $ 1 x Pr($w_j = 1$) + 0 x Pr($w_j = 0$)</li>
<li>which can easily reduce to </li>
<li>$E[w_j] = $ Pr($w_j = 1$)</li>
</ul>
<p>Now that we know how to compute the expected value let's turn our attention back to the clauses</p>
<ul>
<li>Suppose $c_j = (x_1 \vee \bar{x_2} \vee \bar{x_3} \vee \cdots \vee \bar{x_k} )$</li>
<li>What is the probability this is NOT, or is, satisifed</li>
<li>In order to NOT be satisifed all the literals must be false </li>
<li>this has probabilty of $2^{-k}$ </li>
<li>thus the probabilty of being satisifed is $1 - 2^{-k}$</li>
</ul>
<p>If you look at this long enough you may start to wonder how k affects this. As k grows in size (which is the number of variables) then $2^{-k}$ approaches 0 and thus $1 - 2^{-k}$ gets better and better. The worst case is when k is small like 1. In this case we get $1 - 1/2$ which is 1/2. So we can say that</p>
<ul>
<li>$E[w_j] \ge 1/2 $</li>
<li>when we multiply this for the m clauses we get $\ge m/2 $</li>
</ul>
<p>In Summary:<br/>
We've shown E[# of satisfied clauses] = E[W] = $\sum_{j=1}^m E[w_j] \ge \frac{m}{2}$<br/>
This demonstrates the randomized algorithm resulting in a 1/2 approximation</p>
<p>We can de-randomize this algorithm to produce a deterministic algorithm based on conditional expectations.<br/>
Consider the following algo</p>
<ul>
<li>for i -&gt; n<ul>
<li>Take $x[i] = T$ and $x[i] = F$</li>
<li>Compute the expected performance of each possibility</li>
<li>Take the best of the two settings</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Ek-SAT">Ek-SAT<a class="anchor-link" href="#Ek-SAT">¶</a></h3><p>Let's now consider the Ek-SAT problem, Ek for Exact k SAT. In this problem we are looking of exactly k satisfied clauses.</p>
<p>Furthermore for our purposes and for simplicity let's consider the case when k=3 and every clause has size exactly 3. Now what is the probability that Pr($C_j$ is satisfied)? There is one setting of three literals where the clause will not be satisfied which results in a 1/8 probability. Thus the Probabilty of satisfaction is 7/8. t/f for the max E3-SAT problem we achieve a 7/8 approximation algorithm. We can generalize this to say that for size=k, we will achieve a ($1 - 2^{-k}$) approximation for max-Ek-SAT.</p>
<p>As an aside it has been proven that to do better than 7/8 for the max-E3-SAT is NP-Hard. If for some bizarre reason you could do better than 7/8 then you might have just proven that P=NP. But you should really check your work again.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="ILP---Integer-LP">ILP - Integer LP<a class="anchor-link" href="#ILP---Integer-LP">¶</a></h3><p>In this section we will look at the second topic: Which is using Linear programming to get an approximation algorithm for maxSAT. In order to do this we must first introduce a stronger form of LP programming called Integer Programming. Which is similar to regular LP but it adds the condition that our decision variables must be integers.</p>
<p>Recall our general format for an LP problem in canonical form</p>
<ul>
<li>Obj max($c^T x$)</li>
<li>Subject to <ul>
<li>$Ax \le b$</li>
<li>$x \ge 0$</li>
</ul>
</li>
<li>For integer programming we add a condition<ul>
<li>$x \in \mathbb{Z}$, Integers</li>
<li>whereas in our previous LP x could be any number </li>
</ul>
</li>
</ul>
<p>Although it may not be obvious this is a significant departure from the previous version. In the real LP we are looking at a feasible region, Now we are looking at a grid of points inside the feasible region. These are not necassarily the vertices as previous.</p>
<p>Recall that LP has the convexity property that dictates that the optimal point lie at a vertex. This helped us ensure that we could find and target it. This property no longer holds in ILP. A direct consequence of this is that ILP becomes NP-Hard.</p>
<p>Furthermore, while LP is linear time solvable, making $LP \in P$, Integer LP is NP-Hard. In a moment we will show how max-SAT reduces to ILP. In fact many of the problems we've looked at already can be reduced to ILP.</p>
<p>Since ILP is NP-Hard we know it can't be solved in polynomial time. So here's our game plan:</p>
<ul>
<li>First we will show how to reduce SAT, or max-SAT to ILP</li>
<li>then we will look at LP relaxation<ul>
<li>this allows us to ignore the integer value constraint.</li>
<li>this also means we're going to look at the best real number point X</li>
</ul>
</li>
<li>then we're going to use this real number point X <ul>
<li>which is the optimal solution to the Linear Program to find an integer point which is nearby.</li>
<li>That's going to give us a feasible solution to the Integer Linear Program,</li>
</ul>
</li>
<li>then we'll see how far away it is from the optimal solution.<ul>
<li>this will give us our approximation algorithm to the Max-SAT</li>
</ul>
</li>
</ul>
<p>Let's now proceed to prove that ILP is NP-Hard. As stated above we will do this by reducing Max-SAT to ILP.</p>
<p>Suppose we have input f for max_SAT. For arguement sakes f has n variables and m constraints.</p>
<ul>
<li>to construct our ILP program we will <ul>
<li>add a variable $y_i$ for each $x_i$ in f</li>
<li>and we will add a clause $z_j$ for each clause $c_i$ in f</li>
</ul>
</li>
<li>additionally we add an integer constraints on each<ul>
<li>ie $0 \le y_i \le 1$ - is 1 when $x_i = True$ and 0 when $x_i = False$ </li>
<li>ie $0 \le z_i \le 1$ - takes the value 1 when the clause $c_j$ is satisfied otherwise 0</li>
<li>where each can only take on a value of 1 or 0</li>
</ul>
</li>
</ul>
<p>Let's do an example</p>
<ul>
<li>Let $C = (x_5 \vee x_3 \vee x_6) $ and suppose all three are set to false </li>
<li>then $y_5=0, y_3=0, y_6=0$, consequently $z_j = 0$ since it's not satisfied</li>
<li>for any other setting/assignment of variables $z_j$ = 0 or 1. </li>
<li>we can't force, or set, $z_j$ to be 1. <ul>
<li>What we can and will do is use this as the optimal value</li>
<li>optimizing our $z_j$ will now be in the form of an objective function</li>
</ul>
</li>
<li>one other thing we need to do is create a constraint<ul>
<li>to do this we add a simple constraint</li>
<li>$y_5 + y_3 + y_6 \ge z_j $</li>
<li>this ensures that $y_5=0, y_3=0, y_6=0$ results in $z_j=0$</li>
</ul>
</li>
</ul>
<p>Here's another example</p>
<ul>
<li>Let $C=(\bar{x_1} \vee x_3 \vee x_2 \vee \bar{x_5})$<ul>
<li>furthermore, Suppose we have $x_1=T,x_3=F,x_2=F,x_5=T$</li>
</ul>
</li>
<li>Then the corresponding y's are $y_1=1,y_3=0,y_2=0,y_5=1$    </li>
<li>and $z_j=0$ since again the clause is not satisfied<ul>
<li>This time around we can't use the same constraint as before as the sum of our y's would yield 2 and we need 0</li>
<li>t/f we will need to tweak our constraints</li>
<li>Consider $(1-y_1) + y_3 + y_2 + (1-y_5) \ge z_j$</li>
<li>we've constructed our contraints to flip our positive values into negatives</li>
<li>Now any other assignement will yield $z_j \ge 0$</li>
</ul>
</li>
<li>more formally<ul>
<li>take positive literals as $C_j^+$</li>
<li>take negative literals as $C_j^-$</li>
</ul>
</li>
</ul>
<p>Now we can formalize our reduction as</p>
<ul>
<li>Obj: max($\sum_{j=1}^m z_j$)</li>
<li>subject to the constraints<ul>
<li>$\forall i \in \{1,2,...,n\}: \; 0 \le y_i \le 1$</li>
<li>$\forall j \in \{1,2,...,n\}: \; 0 \le z_i \le 1$</li>
<li>and $(\sum y_i \in C_j^+) + \sum (1-y_i) \in C_j^- \; \ge z_j$</li>
<li>and $y_i's \text{ and } z_j's$ are integers</li>
</ul>
</li>
</ul>
<p>Now take a moment to consider the above</p>
<ul>
<li>What is the optimal value of the objective function?<ul>
<li>well thats just $z_1^* + z_2* + ... + z_n^*$ </li>
<li>recall that this is $m^*$ which is the max number of satisfied clauses</li>
</ul>
</li>
</ul>
<p><strong>LP Integer Relaxation</strong></p>
<p>Now all we will do is drop is the last constraint ($y_i's \text{ and } z_j's$ are integers). This doesn't break our formulation. What it does however is allow us to treat it like a standard LP problem.</p>
<p>Of course we don't want to get confused, between the ILP version and the LP version so we create some different variables.</p>
<ul>
<li>Let $\hat{y}^* \;,\; \hat{z}^*$ be the optimal values to the relaxed lp </li>
<li>then the optimal solution is just $\hat{z_1}^* + \hat{z_2}^* + ... + \hat{z_n}^*$</li>
</ul>
<p>Now how can we relate this back to the ILP? As you can imagine the ILP solution is in the feasible region of the LP, so at most it could be the same as the LP solution. Could it be greater than the LP solution? nope! This is all to say that the LP optimal solution is effectively an upper bound on the ILP optimal solution.</p>
<p>Thus, we relate the two quite easily</p>
<ul>
<li>$m^* = z_1^* + z_2^* + ... + z_n^* \le \hat{z_1}^* + \hat{z_2}^* + ... + \hat{z_n}^*$</li>
<li>Unlike the hollywood walk of streets these are optimal, poor joke? perhaps</li>
</ul>
<p>Now how do we tranlate these back to integers? Quite simply: we round them. But we're too advanced now to do basic up/down rounding. We will use fancy probabilistic rounding.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>Randomized Rounding</strong></p>
<p>Continuing our goal of translating $\hat{y_i}^* \text{ and } \hat{z_i}^*$ to integers $y_i \text{ and } z_i$ that are as close as possible to $y_i^* \text{ and } z_i^*$</p>
<p>Recall that in both our ILP as well as our LP formulation we have the constraints</p>
<ul>
<li>$\forall i \in \{1,2,...,n\}: \; 0 \le y_i \le 1$</li>
<li>$\forall j \in \{1,2,...,n\}: \; 0 \le z_i \le 1$</li>
</ul>
<p>Consequently these will be between 0 and 1 and can be thought of as probabilities. So we will round up/down with the same probability. Note that we don't round the $z_i's$ we only care about the y's</p>
<p>So $\text{ Set }y_i = \begin{cases}
        1 &amp; \text{with probability: } \hat{y_i^*}   \\
        0 &amp; \text{with probability: } 1-\hat{y_i^*} \\
\end{cases}$</p>
<p>All in all this is rather simple.</p>
<p>Let's now take a look at the expected value of this randomized rounding</p>
<ul>
<li>Let W = # of satisfied clauses in ILP with randomized rounding<ul>
<li>since the assignment is random W is t/f a Random Variable</li>
</ul>
</li>
</ul>
<p>Similar to the previous analysis of expectations we will proceed clause by clause.<br/>
$\text{ Let }w_j = \begin{cases}
    1 &amp; \text{ if } c_j \text{ is satisifed }   \\
    0 &amp; \text{ if } c_j \text{ is NOT satisifed }  \\
\end{cases}$</p>
<p>If we sum over the j's then $W = sum_j w_j$<br/>
Then:  E[W] = $\sum_j E[w_j] = \sum_j Pr(c_j \text{ is satisfied }) \ge (1-\frac{1}{e}) \hat{z_j}^*$</p>
<p>The final term here is the value of the objective function for the Linear program, which we've shown is <strong>at least</strong> the value of the Integer Linear program for the max-SAT, m*</p>
<p>Thus</p>
<ul>
<li>E[W] = $Pr(c_j \text{ is satisfied }) \ge (1-\frac{1}{e}) \hat{z_j}^* \ge (1-\frac{1}{e})m^*$</li>
</ul>
<p>Now it remains to show/prove</p>
<ul>
<li>Pr($c_j$ is satisifed) $\ge (1-\frac{1}{e}) \hat{z_j}^* $</li>
</ul>
<p>Consider the j'th clause $c_j$ of size k, ie suppose $c_j = (x_1 \vee ... \vee x_k)$. We will stick to positive literals for now to make things simple. It does matter though, due to symmetry.</p>
<ul>
<li>Since all literals are positive we will have a constraint of the form <ul>
<li>$\hat{y_1^*} + \cdots + \hat{y_k^*} \ge \hat{z_j^*}$</li>
</ul>
</li>
<li>So what is the probability that this is satisfied?<ul>
<li>ie Pr($c_j$ is satisfied)</li>
<li>which we can also write as 1 - Pr($c_j$ is unsatisfied)</li>
</ul>
</li>
<li>Recall that Pr($c_j$ is unsatisfied)<ul>
<li>is $\prod_i (1-\hat{y_i^*})$</li>
</ul>
</li>
<li>So how do we relate this product back to the sum in the constraint?</li>
<li>To relate the two we'll need to use the arithmetic/geometric mean inequality</li>
</ul>
<p><strong>AM-GM Inequality</strong><br/>
Suppose we have a series of positive numbers : $x_i,...,x_k \ge 0$</p>
<ul>
<li>The Arithmetic Mean is defined as : $\frac{1}{k} \sum_i x_i$</li>
<li>The Geometric mean is defined as : $(\prod_i x_i)^{1/k}$ </li>
</ul>
<p>Also recall the property that: Arthmetic Mean $\ge$ Geometric Mean, which is always true for non-neg numbers</p>
<p>For our problem</p>
<ul>
<li>we set $w_i = 1-\hat{y_i^*}$, which results in a series of positive numbers</li>
<li>Then our arithmetics mean is<ul>
<li>$\frac{1}{k} \sum_i (1-\hat{y_i^*})$</li>
</ul>
</li>
<li>Our geometric mean is<ul>
<li>$(\prod_i (1-\hat{y_i^*}))^{1/k}$ </li>
</ul>
</li>
<li>put together this means<ul>
<li>$\frac{1}{k} \sum_i (1-\hat{y_i^*}) \ge \left[\prod_i (1-\hat{y_i^*})\right]^{1/k}$</li>
</ul>
</li>
<li>which we write slightly as     <ul>
<li>$\left[\frac{1}{k} \sum_i (1-\hat{y_i^*})\right]^k \ge \prod_i (1-\hat{y_i^*})$</li>
<li>we just moved the root from the left side to the right as an exponent</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Now let's derive our original result, the lemma, from this result</p>
<ul>
<li>Pr($c_j$ is satisfied)</li>
<li>$= 1 - \prod_i (1-\hat{y_i^*})$</li>
<li>$\ge 1 - \left[\frac{1}{k} \sum_i (1-\hat{y_i^*})\right]^k $</li>
<li>$= 1 - \left[1 - \frac{1}{k} \sum_i \hat{y_i^*}\right]^k $<ul>
<li>focus for a moment on the last term $\sum_i \hat{y_i^*}$</li>
<li>this is the contraint on $\hat{z_j^*}$!!!</li>
<li>so it must be at least 0</li>
<li>now we substitute it into our equation to get another inequality</li>
</ul>
</li>
<li>$\ge 1 - \left[1 - \frac{1}{k} \hat{z_j^*} \right]^k $  </li>
</ul>
<p>Now we will use a bit of calculus to move z outside the exponent.</p>
<ul>
<li>Let $\alpha = \hat{z_j^*} $</li>
<li>We've shown above that : Pr($c_j$ is satisfied) $ge 1 - \left[1 - \frac{\alpha}{k} \right]^k $</li>
</ul>
<p>The next step will require a proof of it's own</p>
<ul>
<li>We claim that $f(\alpha) \ge \left( 1-(1-\frac{1}{k})^k \right) \alpha$</li>
<li>where $f(\alpha) = 1 - \left[1 - \frac{\alpha}{k} \right]^k $ from above</li>
</ul>
<p>At a high level the steps to the proof are as follows</p>
<ul>
<li>first we show that the second derivative is less than 0, ie $f"(\alpha) &lt; 0$</li>
<li>this means it's concave and has only one peak area</li>
<li>next let $\beta = \left( 1-(1-\frac{1}{k})^k \right) $</li>
<li>then what can you say about $\beta \alpha$? well it's just a line</li>
<li>if you overlay the two atop each other you should see that<ul>
<li>between [0,1], which are the possible values of $\alpha$</li>
<li>$f(\alpha)$ is always greater</li>
</ul>
</li>
<li>so just check the values at 0 and 1, and you're good to go </li>
</ul>
<p><img src="CS6515_images/LP-004.png" width="450;"/></p>
<p>Finally we can put all the pieces together</p>
<ul>
<li>Pr($C_j$ is satisfied)</li>
<li>$= 1 - \prod_i (1- \hat{y_i^*})$</li>
<li>$\ge 1 -  (1 - \hat{z_j^*}/k)^k$</li>
<li>$\ge \left[1 -  (1 - 1/k)^k \right] \hat{z_j^*}$</li>
<li>$\ge \left[1 -  1/e \right] \hat{z_j^*}$</li>
</ul>
<p>The last line is just a geometric mean taylor expansion and reduction</p>
<ul>
<li>$e^{-x} = 1 - x + \frac{x^2}{2!} -  \frac{x^3}{3!} \pm ...  $</li>
<li>$e^{-x} \ge 1 - x$</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Summary">Summary<a class="anchor-link" href="#Summary">¶</a></h3><p>The process we used here is quite widespread and deserves some attention. In many situations of NP-hard problems it can be applied as follows</p>
<p>Given an NP-Hard problem</p>
<ul>
<li>reduce to ILP</li>
<li>relax to LP and solve</li>
<li>apply randomized rounding</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Comparison:-Simple-v-LP">Comparison: Simple v LP<a class="anchor-link" href="#Comparison:-Simple-v-LP">¶</a></h3><p>At the start of this section we said there would be three possible methods.</p>
<p>first let's compare the two previous methods on each of the problems examined so far</p>
<p><img src="CS6515_images/LP-005.png" width="450;"/></p>
<p>Suppose we took the best of each approach? The max in that row? Then the min approximation would be 3/4. Caused by the k=2 case.
Our third and final algo is the most basic of all, which just takes the max.</p>
<p>Given a formula f</p>
<ul>
<li>Run simple algo to get $m_1$</li>
<li>Run LP algo to get $m_2$</li>
<li>Take the best of the two </li>
</ul>
<p>Then E[max{$m_1,m_2$}] $\ge \frac{3}{4}m^*$</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="End-of-Notes">End of Notes<a class="anchor-link" href="#End-of-Notes">¶</a></h2><p>Hooray you made it to the bottom</p>
<p><img alt="SegmentLocal" src="CS6515_images/dancing_cat.gif" title="segment"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="EX1-:-GR4-Markov-Chains-and-Page-Rank---todo">EX1 : GR4 Markov Chains and Page Rank - todo<a class="anchor-link" href="#EX1-:-GR4-Markov-Chains-and-Page-Rank---todo">¶</a></h2><p>In our last an final section we present the page rank algorithm. For the uninitiated the page-rank algorithm is the mathematical formulation of the importance of a webpage. It was designed by the Google's founders Sergey Brin and Larry page and it's most well known application is Google's search results.</p>
<p>But before we jump into the algorithm we will need to review some mathematics. In particular the page rank algorithm is an example of a Markov Chain.</p>
<p><img src="CS6515_images/GR4-001.png" width="450;"> 
Here' a simple example of a markov chain as a directed graph with probabilities. Each vertex is intended to represent a student's state. Each edge weight is the probability that the student moves along that edge from the source state/vertex to the destination/state.</img></p>
<ul>
<li>These provide a mapping between states.        </li>
<li>P(i,j) = Pr(transition from i -&gt; j) $\in [0,1]$            </li>
<li>and $\sum_i P(i,j) = 1$ since this is a probability distribution           </li>
</ul>
<p>It should also be noted that Graphs illustrating Markov chains often contain self-loops.<br/>
It should also be noted that transition probabilities imply an unknown end state. it's not like a cost or weight per se. It's the probability that from a current state you end up in an adjacent state.</p>
<p>In General for a markov state</p>
<ul>
<li>we have N states {1,2,...,N}</li>
<li>and a N x N transition matrix P, where P(i,j) = Pr(i $\to$ j)<ul>
<li>incidentally P is also called a stochastic matrix</li>
<li>because the end state is generally not assumed. </li>
</ul>
</li>
</ul>
<pre><code>For our directed graph example above this is our transition matrix
Where   Row 1 = Kishore
        Row 2 = Check email
        Row 3 = Starcraft
        Row 4 = Sleep

.5  .5   0   0     
.2   0  .5  .3
 0  .3  .7   0
.7   0   0  .3


Consider where would happen if we square this 

P^2 =   .35  .55  .25  .15      
        .31  .25  .35  .09
        .06  .21  .64  .09
        .56  .35   0   .09
Here P(i,j) = Pr(i-&gt;j in 2 steps)

What this provides us is an easy to use look up chart to determine the probability of being in a state after two time steps.

Suppose we start in state 2.  
What is the probability of being in state 1 after 2 time steps? 
from our original graph 
    = (.2)(.5) + (.3)(.7)
    = 0.31
from our P^2 matrix
    =&gt; P(2,1) = 0.31</code></pre>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><img src="CS6515_images/GR4-001.png" width="450;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><img src="CS6515_images/NP-003.png" width="500;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>$\color{red}{\text{Bernoulli}}$</strong></p>
<p><img src="CS6515_images/GA1-011.png" width="500;"/></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Slack-Notes">Slack Notes<a class="anchor-link" href="#Slack-Notes">¶</a></h2><h3 id="DP-vs-Memoization">DP vs Memoization<a class="anchor-link" href="#DP-vs-Memoization">¶</a></h3><p>Standard DP is bottom up. You build all subproblem solutions as you move through the input knowing you'll need it eventually.</p>
<p>Memoization is top to bottom. You start with your final number needed. Then look for the parts needed to get that answer. Then if those weren't already solved. Look up the solution to those pieces. You keep doing this until you find the solutions you need and build it back up like standard DP. But if you've already solved it. You save it to be used later.</p>
<p>This is helpful if your function is called for many inputs as some inputs will have been calculated once already and doesn't need to be calculated again.</p>
<hr/>
<p>memoization is usually found with recursion, like having to calculate the 10th fib number and calling fib(10), which recursively calls lower values but stores those results for later use</p>
<p>bottom up DP is usually found with standard iteration, like from 1 to n</p>
<p>( In general ) memoization has recursion, DP using tabulation is iterative</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>some thing something</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Exam-2">Exam 2<a class="anchor-link" href="#Exam-2">¶</a></h2><ul>
<li><p>Topics: (RA[1:2]) + (GR[1:3]) + (MF[1:5])</p>
</li>
<li><p>Kruskal: Scan all edges by increasing weight; if adding an edge doesn't create a cycle, add it to the MST</p>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Week-2">Week 2<a class="anchor-link" href="#Week-2">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Appendix">Appendix<a class="anchor-link" href="#Appendix">¶</a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Feasible-&amp;-Saturating-Solution">Feasible &amp; Saturating Solution<a class="anchor-link" href="#Feasible-&amp;-Saturating-Solution">¶</a></h3><p><img src="CS6515_images/MF5-010.png" width="500;"/></p>
<p><img src="CS6515_images/MF5-011.png" width="500;"/></p>
</div>
</div>
</div>
</div></div></body>
</html>
