<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.537">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>My document</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="ISYE8803_HDDA_files/libs/clipboard/clipboard.min.js"></script>
<script src="ISYE8803_HDDA_files/libs/quarto-html/quarto.js"></script>
<script src="ISYE8803_HDDA_files/libs/quarto-html/popper.min.js"></script>
<script src="ISYE8803_HDDA_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="ISYE8803_HDDA_files/libs/quarto-html/anchor.min.js"></script>
<link href="ISYE8803_HDDA_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="ISYE8803_HDDA_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="ISYE8803_HDDA_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="ISYE8803_HDDA_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="ISYE8803_HDDA_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">ISYE 8803 - HDDA</h2>
   
  <ul>
  <li><a href="#read-me" id="toc-read-me" class="nav-link active" data-scroll-target="#read-me">Read Me</a></li>
  <li><a href="#isye-8803---high-dimensional-data-analytics" id="toc-isye-8803---high-dimensional-data-analytics" class="nav-link" data-scroll-target="#isye-8803---high-dimensional-data-analytics">ISYE 8803 - High Dimensional Data Analytics</a></li>
  <li><a href="#m1-functional-data-analysis" id="toc-m1-functional-data-analysis" class="nav-link" data-scroll-target="#m1-functional-data-analysis">M1 Functional Data Analysis</a>
  <ul class="collapse">
  <li><a href="#m1l1-introduction" id="toc-m1l1-introduction" class="nav-link" data-scroll-target="#m1l1-introduction">M1L1 Introduction</a>
  <ul class="collapse">
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  </ul></li>
  <li><a href="#m1l3-polynomial-regression" id="toc-m1l3-polynomial-regression" class="nav-link" data-scroll-target="#m1l3-polynomial-regression">M1L3 Polynomial Regression</a>
  <ul class="collapse">
  <li><a href="#splines" id="toc-splines" class="nav-link" data-scroll-target="#splines">Splines</a></li>
  </ul></li>
  <li><a href="#m1l4-b-splines" id="toc-m1l4-b-splines" class="nav-link" data-scroll-target="#m1l4-b-splines">M1L4 B-Splines</a>
  <ul class="collapse">
  <li><a href="#example-fat-content-prediction" id="toc-example-fat-content-prediction" class="nav-link" data-scroll-target="#example-fat-content-prediction">Example: Fat content prediction</a></li>
  </ul></li>
  <li><a href="#m1l5-smoothing-splines" id="toc-m1l5-smoothing-splines" class="nav-link" data-scroll-target="#m1l5-smoothing-splines">M1L5 Smoothing Splines</a></li>
  <li><a href="#m1l6-kernel-smoothers" id="toc-m1l6-kernel-smoothers" class="nav-link" data-scroll-target="#m1l6-kernel-smoothers">M1L6 Kernel Smoothers</a></li>
  <li><a href="#m1l7-functional-principal-component-analysis-fpca" id="toc-m1l7-functional-principal-component-analysis-fpca" class="nav-link" data-scroll-target="#m1l7-functional-principal-component-analysis-fpca">M1L7 Functional Principal Component Analysis FPCA</a></li>
  </ul></li>
  <li><a href="#m2-image-analysis" id="toc-m2-image-analysis" class="nav-link" data-scroll-target="#m2-image-analysis">M2 Image Analysis</a>
  <ul class="collapse">
  <li><a href="#intro" id="toc-intro" class="nav-link" data-scroll-target="#intro">Intro</a></li>
  <li><a href="#image-analysis-levels" id="toc-image-analysis-levels" class="nav-link" data-scroll-target="#image-analysis-levels">Image Analysis Levels</a></li>
  <li><a href="#basic-functions---opencv" id="toc-basic-functions---opencv" class="nav-link" data-scroll-target="#basic-functions---opencv">Basic Functions - OpenCV</a></li>
  <li><a href="#image-transformations" id="toc-image-transformations" class="nav-link" data-scroll-target="#image-transformations">Image transformations</a>
  <ul class="collapse">
  <li><a href="#transformations" id="toc-transformations" class="nav-link" data-scroll-target="#transformations">Transformations</a></li>
  </ul></li>
  <li><a href="#convolutions-and-filtering" id="toc-convolutions-and-filtering" class="nav-link" data-scroll-target="#convolutions-and-filtering">Convolutions and Filtering</a>
  <ul class="collapse">
  <li><a href="#convolutions" id="toc-convolutions" class="nav-link" data-scroll-target="#convolutions">Convolutions</a></li>
  </ul></li>
  <li><a href="#image-segmentation" id="toc-image-segmentation" class="nav-link" data-scroll-target="#image-segmentation">Image Segmentation</a>
  <ul class="collapse">
  <li><a href="#otsus-method" id="toc-otsus-method" class="nav-link" data-scroll-target="#otsus-method">Otsu’s Method</a></li>
  <li><a href="#k-means-clustering-method" id="toc-k-means-clustering-method" class="nav-link" data-scroll-target="#k-means-clustering-method">K-Means Clustering Method</a></li>
  </ul></li>
  <li><a href="#edge-detection" id="toc-edge-detection" class="nav-link" data-scroll-target="#edge-detection">Edge Detection</a></li>
  </ul></li>
  <li><a href="#m3-tensor-data-analysis" id="toc-m3-tensor-data-analysis" class="nav-link" data-scroll-target="#m3-tensor-data-analysis">M3 Tensor Data Analysis</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a></li>
  <li><a href="#basic-operations" id="toc-basic-operations" class="nav-link" data-scroll-target="#basic-operations">Basic Operations</a>
  <ul class="collapse">
  <li><a href="#multiplication" id="toc-multiplication" class="nav-link" data-scroll-target="#multiplication">Multiplication</a></li>
  <li><a href="#kronecker-product" id="toc-kronecker-product" class="nav-link" data-scroll-target="#kronecker-product">Kronecker Product</a></li>
  <li><a href="#khatri-rao-product" id="toc-khatri-rao-product" class="nav-link" data-scroll-target="#khatri-rao-product">Khatri-Rao Product</a></li>
  <li><a href="#hadamard-product" id="toc-hadamard-product" class="nav-link" data-scroll-target="#hadamard-product">Hadamard Product</a></li>
  </ul></li>
  <li><a href="#tensor-decomp---cp-method" id="toc-tensor-decomp---cp-method" class="nav-link" data-scroll-target="#tensor-decomp---cp-method">Tensor Decomp - CP Method</a>
  <ul class="collapse">
  <li><a href="#cp-decomposition---computation" id="toc-cp-decomposition---computation" class="nav-link" data-scroll-target="#cp-decomposition---computation">CP-Decomposition - Computation</a></li>
  <li><a href="#example-heat-transfer-data" id="toc-example-heat-transfer-data" class="nav-link" data-scroll-target="#example-heat-transfer-data">Example: Heat Transfer Data</a></li>
  </ul></li>
  <li><a href="#tucker-decomposition" id="toc-tucker-decomposition" class="nav-link" data-scroll-target="#tucker-decomposition">Tucker Decomposition</a>
  <ul class="collapse">
  <li><a href="#hosvd---higher-order-svd" id="toc-hosvd---higher-order-svd" class="nav-link" data-scroll-target="#hosvd---higher-order-svd">HOSVD - Higher Order SVD</a></li>
  <li><a href="#tucker-decomp-als-algo" id="toc-tucker-decomp-als-algo" class="nav-link" data-scroll-target="#tucker-decomp-als-algo">Tucker Decomp: ALS Algo</a></li>
  <li><a href="#example-heat-transfer-v2" id="toc-example-heat-transfer-v2" class="nav-link" data-scroll-target="#example-heat-transfer-v2">Example: Heat Transfer v2</a></li>
  </ul></li>
  <li><a href="#tensor-applications-pt1" id="toc-tensor-applications-pt1" class="nav-link" data-scroll-target="#tensor-applications-pt1">Tensor Applications-Pt1</a>
  <ul class="collapse">
  <li><a href="#example-heat-degradation-v3" id="toc-example-heat-degradation-v3" class="nav-link" data-scroll-target="#example-heat-degradation-v3">Example: Heat Degradation v3</a></li>
  </ul></li>
  <li><a href="#tensor-applications-pt2" id="toc-tensor-applications-pt2" class="nav-link" data-scroll-target="#tensor-applications-pt2">Tensor Applications-Pt2</a>
  <ul class="collapse">
  <li><a href="#process-optimization" id="toc-process-optimization" class="nav-link" data-scroll-target="#process-optimization">Process Optimization</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#m4-optimization-pt1" id="toc-m4-optimization-pt1" class="nav-link" data-scroll-target="#m4-optimization-pt1">M4 Optimization Pt1</a>
  <ul class="collapse">
  <li><a href="#convex-optimization" id="toc-convex-optimization" class="nav-link" data-scroll-target="#convex-optimization">Convex Optimization</a></li>
  <li><a href="#first-order-methods" id="toc-first-order-methods" class="nav-link" data-scroll-target="#first-order-methods">First-Order Methods</a>
  <ul class="collapse">
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a></li>
  <li><a href="#accelerated-gradient-descent" id="toc-accelerated-gradient-descent" class="nav-link" data-scroll-target="#accelerated-gradient-descent">Accelerated Gradient Descent</a></li>
  <li><a href="#stochastic-gradient-descent" id="toc-stochastic-gradient-descent" class="nav-link" data-scroll-target="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
  </ul></li>
  <li><a href="#second-order-methods" id="toc-second-order-methods" class="nav-link" data-scroll-target="#second-order-methods">Second Order Methods</a>
  <ul class="collapse">
  <li><a href="#newtons-method" id="toc-newtons-method" class="nav-link" data-scroll-target="#newtons-method">Newton’s Method</a></li>
  <li><a href="#gauss-newton-method" id="toc-gauss-newton-method" class="nav-link" data-scroll-target="#gauss-newton-method">Gauss-Newton Method</a></li>
  <li><a href="#quasi-newton-method-bfgs" id="toc-quasi-newton-method-bfgs" class="nav-link" data-scroll-target="#quasi-newton-method-bfgs">Quasi-Newton Method (BFGS)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#misc" id="toc-misc" class="nav-link" data-scroll-target="#misc">Misc</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">My document</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="read-me" class="level2">
<h2 class="anchored" data-anchor-id="read-me">Read Me</h2>
<p>This page uses quarto:</p>
<ul>
<li>https://quarto.org</li>
<li>https://quarto.org/docs/output-formats/html-basics.html</li>
</ul>
<hr>
</section>
<section id="isye-8803---high-dimensional-data-analytics" class="level1">
<h1>ISYE 8803 - High Dimensional Data Analytics</h1>
<p>Professor: Kamran Paynabar, PhD.</p>
<p>Topics:</p>
<ul>
<li>Functional Data Analysis</li>
<li>Image Analysis</li>
<li>Tensor Data Analysis</li>
<li>Optimization Prt-1</li>
<li>Optimization Prt-2</li>
<li>Regularization</li>
<li>Applications of Regularization</li>
</ul>
<p>Grading</p>
<ul>
<li>7 x Homework 35% (5% apiece)</li>
<li>2 x Exams 65% (32.5% each)</li>
</ul>
<p>Exams are open-book and take-home style. They are not proctored, 1 week from open date to close date starting whenever and submitting prior to due date, and in format similar to homeworks.</p>
<p><strong>Resources &amp; Data</strong></p>
<ul>
<li>https://lib.stat.cmu.edu/datasets/</li>
</ul>
<hr>
</section>
<section id="m1-functional-data-analysis" class="level1">
<h1>M1 Functional Data Analysis</h1>
<p>Functional data can be roughly defined as a quantity or impulse whose variations represent information and is often represented as a function of time or space. The simplest example is an image where the pixel value is a function of it’s location</p>
<section id="m1l1-introduction" class="level2">
<h2 class="anchored" data-anchor-id="m1l1-introduction">M1L1 Introduction</h2>
<p>Understand the definition of big data</p>
<p>Understand concepts such as - curse of dimensionality - low-dimensional learning - define functional Data</p>
<p>Big data revolves around the three V(s) - Volume - the size of the data - Velocity - speed at which it is being generated - Variety - Data types in use (csv, images &amp; videos, sensor readings, signal collection)</p>
<p>High Dimensional Data is defined as data with a large number of attributes. They need not be features though. so for example an image.</p>
<p>Here n is the size of the data, and p is the number of dimensions</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>——-</th>
<th>small n</th>
<th>Large n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>small p</td>
<td>traditional stats with limited samples</td>
<td>Classic Large Sample Theory - Big Data Challenge</td>
</tr>
<tr class="even">
<td>large p</td>
<td>HD Statistics and optimization - High Dimensional data challenge</td>
<td>Deep learning and Deep Neural Networks</td>
</tr>
</tbody>
</table>
<p>Big Data Challenge: Data is too large to be stored on one machine. Frameworks like MapReduce and spark apply here</p>
<p>In this class we focus on the intersection of Large p and small n.&nbsp;This gives rise to the curse of dimensionality. - In many algorithms we will need <span class="math inline">\((\frac{1}{\epsilon})^p\)</span> evaluations in order to obtain a solution within <span class="math inline">\(\epsilon\)</span> of the optimum. If p is large then the number of evaluations increases considerably</p>
<p>HD Analytics challenge is mainly related to “curse of dimensionality”: - Model learning issue: As distance between observations increases with the dimensions, the sample size required for learning a model drastically increases. – Solution: Feature extraction and dimension reduction through low dimensional learning</p>
<p>It is often the case that high dimensional data will have a low dimensional structure. Meaning that we can often reduce the data to it’s most important structure without much loss</p>
<p>Low-Dimensional Learning Methods (That we will cover)<br>
Functional Data Analysis - Splines - Smoothing Splines - Kernels</p>
<p>Tensor Analysis - Multilinear Algebra - Low Rank Tensor Decomposition</p>
<p>Rank Deficient Methods - (Functional) Principal Component Analysis (FPCA) - Robust PCA (RPCA) - Matrix Completion</p>
<section id="linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression">Linear Regression</h3>
<p>Suppose we have a collection of n IID data points <span class="math inline">\((x_1,y_1),...,(x_n,y_n)\)</span>, where x are independent (explanatory) and y is our response, dependent.<br>
- Our goal is to build a function <span class="math inline">\(f(x)\)</span> to model the relationship between x(s) and the y(s) - Most common approach is to minimize the sum of square errors loss function - <span class="math inline">\(\sum_i (y_i - f(x_i))^2\)</span> - to do so we will want to impose the structure constraint - <span class="math inline">\(f(x) = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p\)</span></p>
<p>Expressed in Linear Algebraic Matrix Notation - <span class="math inline">\(y = \beta X + \epsilon\)</span><br>
+ y = 1 x n vector + X = n(row) x p(col) matrix (this may have an initial col of 1s to get an intercept) + <span class="math inline">\(\beta\)</span> = 1 x n vector + <span class="math inline">\(\epsilon\)</span> = 1 x n vector - minimize + <span class="math inline">\(L = \epsilon^T \epsilon = (y - X \beta)^T * (y - X \beta)\)</span> - Resulting Estimate + <span class="math inline">\(\hat{\beta} = (X^T X)^{-1} X^T y\)</span></p>
<p>Example: See data - Chapter12_Runger2006.csv - perform a 2 variable linear regression (note you need to add an intercept column) - you should get the following result + <span class="math inline">\(\hat{y} = 2.26379 + 2.74427 x_1 + 0.01253 x_2\)</span> + or something very similar</p>
<p><img src="images/M1_001.png" align="left"></p>
<p><strong>Geometric Interpretation</strong></p>
<p>An alternative way of thinking about regression. Recall from above<br>
- <span class="math inline">\(\hat{\beta} = (X^T X)^{-1} X^T y\)</span> - if we take <span class="math inline">\(H = (X^T X)^{-1} X^T\)</span>, which you should observe is only dependent on X - then we can rewrite the formula as - <span class="math inline">\(\hat{\beta} = H y\)</span> - This H matrix is called the hat matrix and is the hyperplane spanned by the input vectors X - <span class="math inline">\(\hat{y}\)</span> is the projection that represents the vector or predictions obtained by the least square method</p>
<p><br clear="both"></p>
<p><strong>Properties</strong></p>
<p>You can take the following for granted - prrofs can be found online and are trivial</p>
<ol type="1">
<li>Ordinary Least Square Estimator are unbiased</li>
</ol>
<ul>
<li>ie <span class="math inline">\(E(\hat{\beta}) = \beta\)</span></li>
</ul>
<ol start="2" type="1">
<li>Covariance matrix <span class="math inline">\(cov(\hat{\beta}) = \sigma^2 (X^T X)^{-1}\)</span></li>
</ol>
<ul>
<li><span class="math inline">\(\sigma\)</span> is the residual and can be estimated by
<ul>
<li><span class="math inline">\(\hat{\sigma}^2 = SSE / (n-p)\)</span></li>
<li>and <span class="math inline">\(SSE = \sum(y_i - \hat{y_j})\)</span></li>
</ul></li>
<li>According to the Gauss-Markov Theorem, the Least square estimate (LSE) has the minimum variance and it is unique</li>
</ul>
<p><strong>Regression Feature Extraction</strong></p>
<p>Let’s now turn our attention to how Regression can be used in the context of feature extraction</p>
<p>Suppose we have some high dimensional data like a signal.<br>
- You can fit a standard polynomial regression model such as + <span class="math inline">\(y = \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 t^3\)</span>.</p>
<p>Instead of keeping all the data we can take the co-efficients from the model and form a new feature vector like so - <span class="math inline">\(\hat{\beta} = [\beta_0 \beta_1 \beta_2 \beta_3]^T\)</span> - This can even be used to recreate the original data with varying accuracy</p>
</section>
</section>
<section id="m1l3-polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="m1l3-polynomial-regression">M1L3 Polynomial Regression</h2>
<p>Topics: - Local vs global polynomial Regression - Splines and piecewise polynomial regression - Splines basis and truncated power basis</p>
<blockquote class="blockquote">
<p>Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data. For this reason, polynomial regression is considered to be a special case of multiple linear regression.<br>
Wikipedia</p>
</blockquote>
<p>Recall that an m’th order polynomial regression can be written as</p>
<ul>
<li><span class="math inline">\(y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_m x^m + \epsilon\)</span></li>
</ul>
<p>So for example a cubic model</p>
<ul>
<li><span class="math inline">\(y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3\)</span></li>
<li>can be written in matrix form as
<ul>
<li><span class="math inline">\(\begin{bmatrix} 1 &amp; X &amp; X^2 &amp; X^3 \\ 1 &amp; ? &amp; ? &amp; ? \\ 1 &amp; ? &amp; ? &amp; ? \end{bmatrix}\)</span></li>
</ul></li>
<li>and we then can write the model as
<ul>
<li><span class="math inline">\(Y = X \beta + \epsilon\)</span></li>
</ul></li>
</ul>
<p>Non-linear regression involves 1 or more nonLinear</p>
<p><span class="math inline">\(y = \begin{cases} a_1 (x-c)^{b_1} + d + \epsilon &amp; \text{if } x \gt c \\ a_2 (-x+c)^{b_2} + d + \epsilon &amp; \text{if } x \le c \end{cases}\)</span></p>
<p>Notice that the non linearity is in the parameters. Note that both can be used to model a nonlinear relationship.</p>
<p><img src="images/M1_003.png" width="550px" align="right"></p>
<p>Disadvantages of polynomial Regression<br>
- Remote areas are sensitive to outliers: If your data contains outliers at, or near, the endpoints it’ll mess with with your results - Less flexible due to global functional structure. It assumes that the entire range of data fits a single relationship behaviour. However it is often the case that multiple relationships may exist in the data. + to see an example: <span class="math inline">\(y = \sin^3 (2 \pi x^3) + e \;\; e \sim N(0,0.1^2)\)</span> - A single function across all datapoints is less than ideal when the data resembles multiple types of relationships. for such a situation you need to apply different modelling approaches for each area, region, or interval, of data. For this we need splines</p>
<section id="splines" class="level3">
<h3 class="anchored" data-anchor-id="splines">Splines</h3>
<p>Splines are a Linear combination of piecewise polynomial functions under continuity assumption.</p>
<ul>
<li>Partition the domain of x into continuous intervals then fit a polynomial for each interval seperately.</li>
<li>then each fitted polynomial can be concatenated together to form a single, aggregate, function</li>
<li>This provides flexibility and local fitting</li>
</ul>
<p>Let’s walkthrough a partition procedure, these are generally called knots</p>
<p>Suppose <span class="math inline">\(x \in [a,b]\)</span></p>
<ul>
<li>then determine k knots, such that <span class="math inline">\((e_1,...,e_k) \in (a,b)\)</span>
<ul>
<li>with <span class="math inline">\(e_0=a\)</span> and <span class="math inline">\(e_{k+1}=b\)</span></li>
</ul></li>
<li>Fit a polynomial in each interval under the continuity conditions
<ul>
<li>and integrate them using</li>
<li><span class="math inline">\(f(X) = \sum_{m=1}^k \beta_m h_m(X)\)</span></li>
<li>this is just a Linear combination of the local functions</li>
</ul></li>
</ul>
<p><strong>Example</strong></p>
<p><img src="images/M1_004.png" align="left"></p>
<p><strong>Piecewise Constant</strong> This is a 0-degree polynomial fitting</p>
<p>Suppose we want to use k=3 knots.<br>
then Let <span class="math inline">\(h_1(X)=I(X &lt; e_1)\)</span>, <span class="math inline">\(h_2(X)=I(e_1 &lt; X &lt; e_2)\)</span>, <span class="math inline">\(h_1(X)=I(e_2 &lt; X)\)</span> where I is the indicator function - Then <span class="math inline">\(f(X) = \sum_{m=1}^3 \beta_m h_m(X) \;\; \rightarrow \;\; \hat{\beta_m}=\bar{Y}_m\)</span></p>
<p><strong>NOTE</strong> The indicator function <span class="math inline">\(I = 1\)</span> when <span class="math inline">\(X &lt; e_1\)</span>, and 0 otherwise</p>
<p><strong>NOTE</strong> the h functions are often called basis functions<br>
<br>
<br>
<br>
<strong>Piecewise Linear</strong>, 1st order polynomial</p>
<p>Let <span class="math inline">\(h_{m+3} = h_m(X)X, m = 1,...3\)</span> ( this is in addition to the piecewise constant h functions for 1,2,3</p>
<ul>
<li>then <span class="math inline">\(f(X) = \sum_{m=1}^6 \beta_m h_m(X)\)</span></li>
<li>this suffers from discontinuity</li>
</ul>
<p><br clear="both"></p>
<hr>
<p><img src="images/M1_005.png" align="left"></p>
<p><strong>Continuous Piecewise Linear</strong></p>
<p>for <span class="math inline">\(f(X) = \sum_{m=1}^6 \beta_m h_m(X)\)</span></p>
<ul>
<li>impose continuity constraint for each knot<br>
</li>
<li><span class="math inline">\(f(e_1^-)=f(e_1^+) \;\; \rightarrow \;\; \beta_1 + e_1 \beta_4 = \beta_2 + e_1 \beta_5\)</span><br>
</li>
<li>Total degrees of Freedom DOF=4=6-2, 6 parameters, 2 constraints<br>
<br>
<br>
<br>
<br>
<strong>Piecewise-Linear Basis Function - aka Truncated Power Basis</strong></li>
</ul>
<p>Similar to the Continuous approach, here we incorporate the constraints into the basis functions<br>
- <span class="math inline">\(h_1(X)=1 \;\; h_2(X)=X \;\; h_3(X) = (X - e_1)_+  \;\; h_4(X) = (X - e_4)_+\)</span></p>
<p><strong>Note</strong> The truncation function<br>
<span class="math inline">\((X - e_1)_+ = \begin{cases} 0 \text{ when } (X &lt; e_1) \\ X - e_1  \text{ when } (X &gt; e_1) \end{cases}\)</span></p>
<p><br clear="both"></p>
<hr>
<p><strong>Cubic Polynomials</strong></p>
<p><img src="images/M1_002.png" align="right"></p>
<p>In this situation we encourage smoothness by adding constraints based on their derivatives.</p>
<p>Continuity constraints for smoothness:<br>
- <span class="math inline">\(f(e_j^-)=f(e_j^+)\)</span> - <span class="math inline">\(f'(e_j^-)=f'(e_j^+)\)</span> - first derivative from both sides - <span class="math inline">\(f"(e_j^-)=f"(e_j^+)\)</span> - second derivative</p>
<p>Becomes:<br>
- <span class="math inline">\(h_1(X)=1 \;\; h_3(X) = X^2 \;\; h_5(X) = (X - e_1)_+^3\)</span> - <span class="math inline">\(h_2(X)=X \;\; h_4(X) = X^3 \;\; h_6(X) = (X - e_2)_+^3\)</span> - all together this should yield the curve at the bottom right in the image</p>
<ul>
<li>splines df is calculated by (num_region x num_params_inEachRegion) - (num_knots x num_constraints_perKnot)</li>
<li>which for this example is (3 x 4) - (2 x 3) = 12 - 6 = 6 <br clear="both"></li>
</ul>
<hr>
<p><strong>Order-M Splines</strong> A generalization of the above</p>
<p>Piecewise polynomials of order M-1, continuous derivatives up to order M-2 - M = 1 Piecewise constant splines - M = 2 linear splines - M = 3 quadratic splines - M = 4 cubic splines</p>
<p><strong>Truncated power basis functions</strong></p>
<p>Are defined by the equations</p>
<ul>
<li><span class="math inline">\(h_j(X) = X^{j-1}, j=1,...,M\)</span></li>
<li><span class="math inline">\(h_{M+l}(X) = (X - e_l)_+^{M-1}, l=1,...,K\)</span>
<ul>
<li>Total degrees of freedom is K+M</li>
<li><span class="math inline">\(f(X) = \sum_{m=1}^{M+K} \beta_m h_m(X)\)</span></li>
</ul></li>
</ul>
<p>Remarks</p>
<ul>
<li><p>Cubic spline is the lowest order spline for which the knot discontinuity is not visible to human eyes</p></li>
<li><p>Knots selection: A simple method is to use x-quantiles. However the choice of knots is a variable/model selection problem.</p></li>
</ul>
<p>Estimation using least squares</p>
<p>Using the above basis functions</p>
<ul>
<li><span class="math inline">\(H = [h_1(X) \;\; h_2(X) \;\; h_3(X) \;\; h_4(X) \;\; h_5(X) \;\;  h_6(X)]\)</span></li>
<li>becomes <span class="math inline">\(\hat{\beta} = (H^T H)^{-1} H^T y\)</span>
<ul>
<li>Linear Smoother: <span class="math inline">\(\hat{y} = H \hat{\beta} = (H^T H)^{-1} H^T y = S y\)</span></li>
<li>Degrees of Freedom: df = trace(S)</li>
</ul></li>
<li>Truncated power basis functions are simple and algebraically appealing</li>
<li>However they’re not efficient for computation and are ill posed and calculating the inverse is often numerically unstable.</li>
<li>to resolve this we will turn to bsplines in the next section</li>
</ul>
<p><strong>See Examples1.py BSpline Example at top</strong></p>
</section>
</section>
<section id="m1l4-b-splines" class="level2">
<h2 class="anchored" data-anchor-id="m1l4-b-splines">M1L4 B-Splines</h2>
<p>Objectives:</p>
<ul>
<li>Computational issues of splines</li>
<li>Understanding B-Splines basis, and how it can be used in regression</li>
<li>Define a smoother matrix and computing degrees of freedom</li>
</ul>
<p><strong>Computational issues of Splines</strong></p>
<p>Recall that truncated power basis functions are simple and mathematically appealing. However they are not comutationally efficient and numerically unstable.</p>
<p>Consider the truncated power basis for a cubic spline, which we recently introduced</p>
<ul>
<li><span class="math inline">\(h_1(X)=1 \;\; h_3(X) = X^2 \;\; h_5(X) = (X - e_1)_+^3\)</span></li>
<li><span class="math inline">\(h_2(X)=X \;\; h_4(X) = X^3 \;\; h_6(X) = (X - e_2)_+^3\)</span></li>
</ul>
<p>For a positive range of X, it should be clear that <span class="math inline">\(h_1,h_2,h_3\)</span> will be strongly correlated since they’re all derived from X. This in turns will introduce a singularity in the H matrix leading to numerical instability. This is caused by a determinant close to 0.</p>
<p><img src="images/M1_006.png" width="450px" align="right"></p>
<p>Alternative basis vectors for piecewise ploynomials that are efficient were proposed by DeBoor in 1978.</p>
<ul>
<li>Each basis function has a local support, it is non-zero over at most M consecutive intervals.
<ul>
<li>Look at the line starting at 0 in the Order 2 BSpline diagram at right</li>
<li>notice how for every other interval it is 0, but also non zero</li>
</ul></li>
<li>the basis matrix is banded, dark blue represent 0 values</li>
<li>the low bandwidth of this matrix reduces the linear dependency of the columns and reduces the numerical instability</li>
</ul>
<p><img src="images/M1_007.png" align="left"></p>
<p><br clear="both"></p>
<p>Now let’s turn our attention to how to create a B-Spline Basis of order M, with k knots</p>
<ul>
<li>Let <span class="math inline">\(B_{j,m}(x)\)</span> be the j’th B-Spline basis function of order m, with m <span class="math inline">\(\le\)</span> M,</li>
<li>for the knot sequence <span class="math inline">\(\tau\)</span></li>
<li>create <span class="math inline">\(a &lt; e_1 &lt; ... &lt; e_k &lt; b\)</span></li>
</ul>
<p>Define the augmented knots seq <span class="math inline">\(\tau\)</span>:</p>
<ul>
<li><span class="math inline">\(\tau_1 \le ... \le \tau_M \le e_0\)</span>
<ul>
<li>(before the lower bound <span class="math inline">\(e_0\)</span>)</li>
</ul></li>
<li>and <span class="math inline">\(\tau_{M+j} = e_j\)</span> j=1,2,…,K</li>
<li><span class="math inline">\(e_{K+1} \le \tau_{M+K+1} \le \tau_{M+K+2} \le .... \le \tau_{2M+K}\)</span>
<ul>
<li>after the lower bound</li>
</ul></li>
</ul>
<p>for j = 1,2,…,2M+K+1,<br>
<span class="math inline">\(B_{j,1}(x) = \begin{cases} 1 \;\; \text{if } \tau_j \le x \lt \tau_{j+1} \\ 0 \;\; \text{ otherwise } \end{cases}\)</span></p>
<p>for j = 1,2,…,2M+K-m,<br>
<span class="math inline">\(\large B_{j,m}(x) = \frac{ x - \tau }{ \tau_{j+m-1} - \tau } B_{j,m-1}(x) + \frac{ \tau_{j+m} - x}{\tau_{j+m} - \tau_{j+1}} B_{j+1,m-1}(x)\)</span></p>
<p><strong>Example in Matlab</strong></p>
<p>Excercise: Recreate this in your favourite language.<br>
Tip: https://stackoverflow.com/questions/33282368/plotting-a-2d-heatmap</p>
<p><img src="images/M1_008.png"> <br clear="both"></p>
<p>Consider and compare</p>
<p><img src="images/M1_009.png"> <br clear="both"></p>
<p><strong>Smoother/Projection Matrix</strong></p>
<p>Consider a regression spline basis B</p>
<p><span class="math inline">\(\hat{f} = B(B^T B)^{-1} B^T y = H y\)</span></p>
<ul>
<li>H is the smoother matrix (aka projection matrix)</li>
<li>H is idempotent</li>
<li>H is symmetric</li>
<li>Degrees of freedom: trace(H)</li>
</ul>
<section id="example-fat-content-prediction" class="level3">
<h3 class="anchored" data-anchor-id="example-fat-content-prediction">Example: Fat content prediction</h3>
<p>ref: M1_FunctionalDataAnalysis/meat.csv<br>
Source (https://lib.stat.cmu.edu/datasets/tecator)</p>
<ul>
<li>A beef distributor wants to know the fat content of meat from spectrometric curves, which correspond to the absorbance measured at 100 wavelengths.</li>
<li>She obtains the spectrometric curves for 215 finely chopped meat, these are our functional predictors</li>
<li>Additionally, through a chemical process she estimates the fat content of each piece ( these are our response variables )
<ul>
<li>however this is a costly and time consuming process</li>
</ul></li>
<li>Goal is to build a model that will predict the fat content of each piece of meat using the spectrometric curve</li>
</ul>
<p>Our process (for you to do)</p>
<ul>
<li>split the data into a train (195 curves) and test dataset (20 curves)</li>
<li>Regular approach
<ul>
<li>Build a linear regression using the 100 measurements from the spectrometer as predictors</li>
</ul></li>
<li>Functional Approach
<ul>
<li>Use B-Spline to model each curve and extract features</li>
</ul></li>
<li>Estimate B-Spline co-efficients are used as predictive features that can be used in building the fat regression model</li>
<li>Mean Square errors for the predictions on test should be roughly
<ul>
<li>for Regular Approach : RMSE = 27.02</li>
<li>for B Spline Approach : RMSE = 14.25</li>
</ul></li>
</ul>
<div id="4349485f-746e-49d0-b2af-997aed848935" class="cell" data-tags="[]" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.ndimage <span class="im">import</span> gaussian_filter</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D  <span class="co"># noqa: F401 E261</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance_matrix</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> LeaveOneOut</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.interpolate <span class="im">import</span> splrep</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.interpolate <span class="im">import</span> BSpline</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> files.helpers <span class="im">as</span> hlp</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="f6ba9f5c-6174-42ed-81c4-6123bc47fdf6" class="cell" data-tags="[]" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># * FAT CONTENT PREDICTION</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline linear model</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>nTest <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>meat <span class="op">=</span> pd.read_csv(<span class="st">"files/m1_meat.csv"</span>, index_col<span class="op">=</span><span class="dv">0</span>).values</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># If no pandas: meat = np.loadtxt('meat.csv',delimiter=',',skiprows=1,usecols=np.arange(1,102))</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(meat)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>train,test <span class="op">=</span> meat[:<span class="op">-</span>nTest, :], meat[<span class="op">-</span>nTest:, :]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>regressor <span class="op">=</span> LinearRegression()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>X,y <span class="op">=</span> train[:, :<span class="op">-</span><span class="dv">1</span>], train[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>regressor.fit(X, y)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>pred1 <span class="op">=</span> regressor.predict(test[:, :<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>mse1 <span class="op">=</span> <span class="bu">sum</span>(((test[:, <span class="op">-</span><span class="dv">1</span>]<span class="op">-</span>pred1)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>nTest</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Fat content, MSE using raw features: </span><span class="sc">{</span>mse1<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mse1)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Fat content, RMSE using raw features: </span><span class="sc">{</span>rmse<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Fat content, MSE using raw features: 27.246033798016676
Fat content, RMSE using raw features: 5.219773347379815</code></pre>
</div>
</div>
<div id="66f14c7a-2ab9-474f-a1e1-10416f78cee0" class="cell" data-tags="[]" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># B-splines</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> meat[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>xx <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>knots <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">8</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> hlp.BSplineBasis(xx, knots, deg)[:,:<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>Bcoef <span class="op">=</span> np.linalg.lstsq(B, X.T)[<span class="dv">0</span>].T</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>meat2 <span class="op">=</span> np.hstack([Bcoef, meat[:, <span class="op">-</span><span class="dv">1</span>][:, <span class="va">None</span>]])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> meat2[:<span class="op">-</span>nTest, :]</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>test  <span class="op">=</span> meat2[<span class="op">-</span>nTest:, :]</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>regressor <span class="op">=</span> LinearRegression()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> train[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> train[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>regressor.fit(X, y)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>pred2 <span class="op">=</span> regressor.predict(test[:, :<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>mse2 <span class="op">=</span> <span class="bu">sum</span>(((test[:, <span class="op">-</span><span class="dv">1</span>]<span class="op">-</span>pred2)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>nTest</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Fat content, MSE using spline features: </span><span class="sc">{</span>mse2<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mse2)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Fat content, RMSE using spline features: </span><span class="sc">{</span>rmse<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Fat content, MSE using spline features: 10.60595786815531
Fat content, RMSE using spline features: 3.2566789630166664</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\monze\AppData\Local\Temp\ipykernel_18608\1100264321.py:9: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.
To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.
  Bcoef = np.linalg.lstsq(B, X.T)[0].T</code></pre>
</div>
</div>
</section>
</section>
<section id="m1l5-smoothing-splines" class="level2">
<h2 class="anchored" data-anchor-id="m1l5-smoothing-splines">M1L5 Smoothing Splines</h2>
<p>Objectives:</p>
<ul>
<li>Understanding B Spline basis boundary issues</li>
<li>Introduce natural cubic spline basis</li>
<li>Define smoothing splines</li>
<li>discuss cross validation for tuning penalty parameters</li>
</ul>
<p><strong>Boundary Effects on Splines</strong></p>
<p><img src="images/M1_010.png" width="450px" align="left"></p>
<p>Consider the following setting with the fixed training data:</p>
<ul>
<li><span class="math inline">\(y_i = f(x_i) + \epsilon_i\)</span></li>
<li><span class="math inline">\(\epsilon_i \sim iid(0,\sigma^2)\)</span></li>
<li>Var(<span class="math inline">\(\hat{f}(x)\)</span> = $h(x)^T (H^T H)^{-1} h(x) ^2 $
<ul>
<li>here <span class="math inline">\(H\)</span> is our basis matrix</li>
<li>and <span class="math inline">\(h(x)^T\)</span> is the row in <span class="math inline">\(H\)</span> that corresponds to the x observation</li>
</ul></li>
</ul>
<p>Behaviour of splines tends to be sporadic near the boundaries, extrapolation can be problematic. Observing the image we can see that the variance near the boundaries, both left and right, tend to be very high. Furthermore in both cases we can see that the cubic spline with 2-knots has the highest variance near the boundaries. This is caused in part by the high complexity of the cubic spline, with it’s many parameters. The global linear and polynomial have less parameters and tend to be a bit better behaved.</p>
<p>We will try to reduce the variance near the boundaries by using linear splines, rather than cubic, which leads us to Natural splines</p>
<p><br clear="both"></p>
<hr>
<p><strong>Natural Cubic Splines</strong></p>
<ul>
<li>Additional constraints are added to make the function linear beyond the boundary knots</li>
<li>Assuming the function is linear near the boundaries, where there is less information, is often reasonable</li>
<li>Cubic spline: Linear on (<span class="math inline">\(-\infty,e_1]\)</span> and <span class="math inline">\([e_k,\infty)\)</span></li>
<li>Prediction variance decreases</li>
<li>The price is the bias near the boundaries</li>
<li>Degrees of freedom is K, the number of knots</li>
<li>to form the basis matrix
<ul>
<li>Each of these basis functions has zero second and third derivatives in the linear region</li>
<li><span class="math inline">\(\large N_1(x)=1 \;\; N_2(x) = x \;\; N_{k+2}(x) = (d_k(x) - d_{k-1}(x))\)</span></li>
<li><span class="math inline">\(\large d_k = \frac{ (x-e_k)_+^3 - (x-e_K)_+^3  }{ e_K - e_k}\)</span> for k = 1,2,…,K-2</li>
</ul></li>
<li>The first two terms <span class="math inline">\(N_1,N_2\)</span> form our linear basis at the edges. For all other intervals we use the cubic as needed</li>
<li>(In R) B = ns(x,df,intercept) will return the basis function</li>
</ul>
<p>Penalized Residual Sum of Squares</p>
<ul>
<li><p><span class="math inline">\(\large \min_f \frac{1}{n} \sum_1^n [y_i - f(x_i)]^2 + \lambda \int_a^b [f''(x)]^2 dx\)</span></p></li>
<li><p>first term measures the closeness of the model to the data (which is related to bias)</p></li>
<li><p>Second term penalizes the curvature of the function (which is related to variance)</p></li>
<li><p>Avoid knot selection</p>
<ul>
<li>You could select as many knots as there are observations,</li>
<li>but this will increase the number of local polynomials which in turn increase the complexity of the model</li>
<li>All this results in an increase in the prediction variance</li>
</ul></li>
</ul>
<p><img src="images/M1_011.png" align="right"></p>
<p>We can control for the increase in prediction variance by introducing a penalty function. This will push the model towards greater linearity</p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is smoothing parameter to control the trade-off between bias and variance</li>
<li><span class="math inline">\(\lambda = 0\)</span> interpolates the data - may lead to overfitting</li>
<li><span class="math inline">\(\lambda = \infty\)</span> Leads to linear least squares regression</li>
</ul>
<p><br clear="both"></p>
<p>It can be shown that the minimzer is a natural cubic spline</p>
<ul>
<li><span class="math inline">\(\large \hat{f}(x) = \sum_{j=1}^n \theta_j N_j(x)\)</span></li>
<li>where <span class="math inline">\(N_j's\)</span> are a set of natural cubic spline basis with knots at each of the unique <span class="math inline">\(x_i's\)</span></li>
</ul>
<p>Let’s now consider how we may be optimize, in this case minimize, the least square problem in this situation.</p>
<p>We begin by reducing to the following form (in Matrix Notation)</p>
<ul>
<li>RSS(<span class="math inline">\(\theta,\lambda\)</span>) = <span class="math inline">\((y-N\theta)^T (y-N\theta) + \lambda \theta^T \Omega \theta\)</span>
<ul>
<li><span class="math inline">\(\{N\}_{ij} = N_j(x_i)\)</span> This is the matrix of Natural Cubic Spline bases</li>
<li><span class="math inline">\(\theta\)</span> is the matrix of co-efficients that need to be estimated</li>
<li><span class="math inline">\(\Omega_{jj'} = \int N_j^{''}(x) N_{j'}^{''}(x) dx\)</span>, is the matrix of second derivatives</li>
</ul></li>
<li>Optimizing/Minimizing RSS(<span class="math inline">\(\theta,\lambda\)</span>)<br>
</li>
<li>Leads to the solution
<ul>
<li><span class="math inline">\(\hat{\theta} = (N^T N + \lambda \Omega)^{-1} N^T y\)</span></li>
</ul></li>
</ul>
<p>Smoothing spline estimator is a linear smoother</p>
<ul>
<li><span class="math inline">\(\hat{f} = N (N^T N + \lambda \Omega)^{-1} N^T y\)</span></li>
<li><span class="math inline">\(= S_{\lambda} y\)</span>
<ul>
<li><span class="math inline">\(S_{\lambda}\)</span> is the smoother matrix, it is also symmetric</li>
<li><span class="math inline">\(S_{\lambda}\)</span> is not idempotent (Recall for a matrix X, if X*X=X then X is idempotent)</li>
<li><span class="math inline">\(S_{\lambda}\)</span> is positive definite
<ul>
<li>ie <span class="math inline">\(a^T \Omega a = \int \left[ \sum a_j N_j^{''}(x) \right]^2 dx &gt; 0\)</span></li>
</ul></li>
</ul></li>
<li>Degrees of freedom DOF = trace(<span class="math inline">\(S_{\lambda}\)</span>)</li>
</ul>
<hr>
<p><strong>Choice of tuning Parameters</strong></p>
<p>Divide data into 3 independent sets: Training, Validation, and test. Then for some choice of <span class="math inline">\(\lambda\)</span> we would go through the process of building the model and evaluating it. This would be repeated to multiple choices of <span class="math inline">\(\lambda\)</span>s</p>
<p><img src="images/M1_012.png" align="left" width="450px"> <br clear="both"></p>
<p>If an independent validation dataset is not affordable, K-fold cross validation (CV) or leave-1-out CV can be used.</p>
<p>Here is an example of K-fold with K=5.</p>
<p><img src="images/M1_013.png" align="left" width="450px"> <br clear="both"></p>
<hr>
<p><strong>Model Selection Criteria</strong></p>
<p>Akaike Selection Criteria</p>
<ul>
<li>AIC = <span class="math inline">\(-2 \log(L)+2k\)</span> (Where k is the number of estimated parameters and L is the likelihood function</li>
<li><span class="math inline">\(2k\)</span> serves as the penalty function and as you can see it will increase as the number of parameters increase</li>
<li>Ideally we want this as low as possible</li>
</ul>
<p>Bayesian Information Criteria</p>
<ul>
<li>BIC = <span class="math inline">\(-2 \log(L) + k \log(n)\)</span></li>
<li>where n is the sample size</li>
</ul>
<p>Generalized Cross Validation: GCV</p>
<ul>
<li><span class="math inline">\(\large \hat{\lambda} = \arg \min_{\lambda} GCV(\lambda) =  \arg \min_{\lambda} \frac{|| Y - \hat{Y}||^2 / n}{ (1-n^{-1} tr(\hat{S}(\lambda)))^2  }\)</span></li>
</ul>
</section>
<section id="m1l6-kernel-smoothers" class="level2">
<h2 class="anchored" data-anchor-id="m1l6-kernel-smoothers">M1L6 Kernel Smoothers</h2>
<p>Objectives</p>
<ul>
<li>To define kernel functions</li>
<li>To understand KNN Regression / weighted kernel reg / and both linear and polynomial kernel regression</li>
</ul>
<p><strong>K-Nearest Neighbour (KNN)</strong></p>
<p><img src="images/M1_014.png" width="450px" align="left"></p>
<p>To help us build our understanding we create some toy data (left)</p>
<p>KNN Average <span class="math inline">\(\hat{f}(x_0) = \sum w(x_0,x_i) y_i\)</span></p>
<ul>
<li>where <span class="math inline">\(w(x_0,x_i) = 1/K\)</span> if <span class="math inline">\(x_i \in N_k(x_0)\)</span> and 0 otherwise</li>
<li>what’s happening here is that we take the avg of points near to <span class="math inline">\(x_0\)</span> wieghted by their distance from <span class="math inline">\(x_0\)</span></li>
</ul>
<p>Remarks</p>
<ul>
<li>Simple average of the k nearest observations to <span class="math inline">\(x_0\)</span> to produce local averages</li>
<li>Equal weights are assigned to all neighbours</li>
<li>the fitted function is in form of a step function (non-smooth function)</li>
</ul>
<p><br clear="both"></p>
<hr>
<p><strong>Kernel Function</strong></p>
<p>A Kernel function is any non-negative real-valued integrable function that satisfies the following conditions:</p>
<ol type="1">
<li><p><span class="math inline">\(\int_{-\infty}^{\infty} K(u) du = 1\)</span></p></li>
<li><p>K is an even function: <span class="math inline">\(K(-u) = K(u)\)</span></p></li>
<li><p>It has finite second moment: $_{-}^{} u^2 K(u) du &lt; $</p></li>
</ol>
<p><img src="images/M1_015.png" width="500px" align="right"></p>
<p>Examples of Kernel Functions</p>
<ul>
<li>Symmetric Beta family kernel
<ul>
<li>Uniform kernel (d=0) <span class="math inline">\(\large K(u,d) = \frac{ (1-u^2)^d }{ 2^{2d+1} B(d+1,d+1)} I(|u| &lt; 1)\)</span></li>
<li>Epanechnikov kernel (d=1)</li>
<li>Bi/Tri-Weight (d=2,3)</li>
</ul></li>
<li>Tri-Cube Kernel <span class="math inline">\(K(u) = (1-|u|^3)^3  I(|u| &lt; 1)\)</span><br>
</li>
<li>Gaussian Kernel <span class="math inline">\(K(u) = 1 / \sqrt{2\pi} \exp(-u^2)\)</span></li>
</ul>
<p>Note that both symmetric and tricube have local support, Gaussian, in theory, has global support</p>
<p><br clear="both"></p>
<hr>
<p><strong>Kernel Smoother Regression</strong></p>
<p>Kernel Regression is the application of a kernel function to the regression problem</p>
<ul>
<li>Is weighted local averaging that fits a simple model seperately at each query point <span class="math inline">\(x_0\)</span></li>
<li>More weights are assigned to closer observations</li>
<li>Localization is defined by the weighting function</li>
<li>for any point
<ul>
<li><span class="math inline">\(\large  \hat{f}(x_0) = \frac{\sum K_{\lambda}(x_0,x_i)y_i}{\sum K_{\lambda}(x_0,x_i)}\)</span></li>
<li>where <span class="math inline">\(K_{\lambda}(x_0,x_i) = K(|x_0 - x_i|/\lambda)\)</span></li>
</ul></li>
<li>K is a kernel function</li>
<li><span class="math inline">\(\lambda\)</span> is the so-called bandwidth or window width that defines the width of the neighbourhood</li>
<li>Kernel regression requires very little training; all calculations are done at the evaluation time</li>
</ul>
<p><img src="images/M1_016.png" width="500px" align="left"></p>
<p>Let’s revisit now our example from the start</p>
<p>We use a Epanechnikov kernel from the beta family with <span class="math inline">\(\lambda=0.2\)</span>. Notice how much smoother the resulting model is compared to the jaggedness of the Nearest Neighbour. The yellow region region indicates the weighted assigned to that area</p>
<p><br clear="both"></p>
<p><strong>Choice of <span class="math inline">\(\lambda\)</span></strong> (ref: Examples.m::KERNEL SMOOTHERS::RBF kernel)</p>
<ul>
<li><span class="math inline">\(\lambda\)</span> defines the width of the neighbourhood<br>
</li>
<li>Only points within <span class="math inline">\([x_0 - \lambda, x_0 + \lambda]\)</span> recieve positive weights in kernels with the support of [-1,1]</li>
<li>“Larger” <span class="math inline">\(\lambda\)</span>: gets you a smoother estimate, larger bias, smaller variance</li>
<li>“Smaller” <span class="math inline">\(\lambda\)</span>: rougher estimate, smaller bias, larger variance</li>
</ul>
<p>The following criteria can be used for determining <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li>Leave-one out cross validation</li>
<li>K Fold cross validation</li>
<li>Generalized cross validation</li>
</ul>
<p>One major drawback of local averaging is that is can be biased on the boundaries of the domain due to the asymmetry of the kernel in that region As well as a lack of data near the boundaries</p>
<hr>
<p><strong>Local Linear Regression</strong></p>
<p>Locally weighted linear regression model is estimated by</p>
<ul>
<li><span class="math inline">\(\large \underset{\beta_0(x_0),\beta_1(x_0)}{\arg \min} \sum K_{\lambda}(x_0,x_1) \left[ y_i - \beta_0(x_0) - \beta_1(x_0) \right]\)</span><br>
</li>
<li>here <span class="math inline">\(K_{\lambda}\)</span> is the weighting function
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the intercept</li>
<li><span class="math inline">\(\beta_1\)</span> is the slope</li>
<li>it should also be noted that <span class="math inline">\(x_0\)</span> is a fixed number, meaning this betas are functions of <span class="math inline">\(x_0\)</span></li>
</ul></li>
</ul>
<p>The estimate of the function at <span class="math inline">\(x_0\)</span> is then</p>
<ul>
<li>$(x_0) = _0(x_0) + _1(x_0) x_0 $</li>
</ul>
<p>Local linear regression corrects the bias on the boundaries</p>
<p>Of course we can also apply this idea to polynomial regression</p>
<p><strong>Local Polynomial Regression</strong></p>
<p><img src="images/M1_017.png" width="300px" align="left"></p>
<p>Locally weighted polynomial regression model can be estimated by</p>
<ul>
<li><span class="math inline">\(\large \underset{\beta_0(x_0),\beta_1(x_0)}{\arg \min} \sum_{i=1}^n K_{\lambda}(x_0,x_1) \left[ y_i - \beta_0(x_0) - \sum_{j=1}^p \beta_j(x_0) x_i^j \right]^2\)</span></li>
</ul>
<p>The estimate of the function at <span class="math inline">\(x_0\)</span> is then</p>
<ul>
<li>$(x_0) = <em>0(x_0) + </em>{j=1}^p _j(x_0) x_0^j $</li>
</ul>
<p><br clear="both"></p>
<p>Local polynomial regression corrects the bias in the curvature regions</p>
<ul>
<li>Higher order polynomials result in a lower bias and a higher variance</li>
<li>Local linear fits can help reduce linear bias on the boundaries</li>
<li>Local quadratic fits are effective for reducing bias due to curvature in interior bias due to curvature in interior region, but not in boundary regions</li>
</ul>
</section>
<section id="m1l7-functional-principal-component-analysis-fpca" class="level2">
<h2 class="anchored" data-anchor-id="m1l7-functional-principal-component-analysis-fpca">M1L7 Functional Principal Component Analysis FPCA</h2>
<p>Objectives:</p>
<ul>
<li>How to perform PCA on functional data?</li>
<li>To understand KL theorem and identify eigen-functions and values</li>
<li>To demonstrate feature extraction using FPCA</li>
</ul>
<p>Functional PCA is similar to PCA. It seeks to reduce the dimensions of the functional data by extracting a smaller set of uncorrelated features which captures most of the data variations. These features are known as the PCA Scores</p>
<p>In order to perform/apply FPCA we assume that the data takes a <strong>Signal Functional Form</strong>. For this section Signal and Functional are used interchangably and mean the same thing</p>
<p><span class="math inline">\(s_i(t) = \mu(t) + e_i(t)\)</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(s_i(t)\)</span>: observed signals i=1,…,N</li>
<li><span class="math inline">\(\mu(t)\)</span>: Continuous functional mean - Not subscripted as it applies across the entire span</li>
<li><span class="math inline">\(e_i(t)\)</span>: realizations from a stochastic process with mean function 0 and covariance function C(t,t’).
<ul>
<li>This includes/captures both random noise and signal-to-signal variations</li>
</ul></li>
</ul>
<p><strong>Karhunen-Loeve Theorem</strong></p>
<p>Since the signal variations come from the noise function, <span class="math inline">\(e_i(t)\)</span>, we focus on this term to reduce the dimensions</p>
<p>We begin by decomposing <span class="math inline">\(e_i(t)\)</span> into an infinite number of orthogonal basis function</p>
<ul>
<li><span class="math inline">\(\large e_i(t) = \sum_{k=1}^{\infty} \xi_{ik} \phi_k(t)\)</span>
<ul>
<li>where <span class="math inline">\(\phi_k(t)\)</span> are our basis functions</li>
<li>and <span class="math inline">\(\xi_{ik}\)</span> are zero-mean and uncorrelated co-efficients</li>
<li>ie <span class="math inline">\(E(\xi_{ik}) = 0\)</span> and <span class="math inline">\(E[(\xi_{ik})^2] = \lambda_k\)</span></li>
</ul></li>
<li>and <span class="math inline">\(\phi_k(t)\)</span> are eigen-functions of the covariance function
<ul>
<li><span class="math inline">\(C(t,t') = cov( e(t) , e(t') )\)</span> in general this is not known and will need to be estimated based on the known data</li>
<li>ie <span class="math inline">\(C(t,t') = \sum_{k=1} \lambda_k \phi_k(t) \phi_k(t')\)</span></li>
</ul></li>
<li>$_1 _2 … $are ordered eigen-values.</li>
<li>The eigen-functions can be obtained by solving
<ul>
<li><span class="math inline">\(\int_0^M C(t,t') \phi_k(t) dt = \lambda_k \phi_k(t')\)</span></li>
</ul></li>
</ul>
<p><strong>Functional PCA</strong></p>
<p>The variance of <span class="math inline">\(\xi_{ik}\)</span> decays quickly with k. Thus, only a few <span class="math inline">\(\xi_{ik}\)</span>, would be enough to accurately approximate the noise function. These few are also known as the FPCA Scores and these are sufficient for summarizing the original data.</p>
<ul>
<li><span class="math inline">\(e_i(t) \approx \sum_{k=1}^K \xi_{ik} \phi_k(t)\)</span></li>
</ul>
<p>Thus our signals decomposition is given by</p>
<ul>
<li><span class="math inline">\(s_i(t) = \mu(t) + e_i(t)\)</span></li>
<li><span class="math inline">\(\cong \mu(t) + \sum_{k=1}^K \xi_{ik} \phi_k(t)\)</span>
<ul>
<li>note that we’ve just replaced the original second term, the noise, with our approximation</li>
</ul></li>
</ul>
<p>Model Estimation. In practice we generally encounter two types of data</p>
<ul>
<li>Complete signals: sampled regularly</li>
<li>Incomplete signals: sampled irregularly, sparse, fragmented</li>
</ul>
<p>We begin by estimating the mean function</p>
<p><strong>Estimation of Mean Function</strong></p>
<p>Given historical Signals <span class="math inline">\(s_i(t_{ij})\)</span></p>
<p>where</p>
<ul>
<li>i=1,…,N is the signal index</li>
<li>j=1,…,m_i is the observation index in each signal</li>
<li><span class="math inline">\(s_i(t_{ij}) \cong \mu(t_{ij}) + \sum_{k=1}^K \xi_{ik} \phi_k(t_{ij})\)</span></li>
</ul>
<p>We can estimate the mean function <span class="math inline">\(\hat{\mu}(t)\)</span> using local linear regression by minimizing</p>
<ul>
<li>$ <em>{i=1}^n </em>{j=1}^{m_i} W() ^2 $</li>
<li>Solution: <span class="math inline">\(\hat{\mu(t)} = \hat{c}_{0,t}\)</span></li>
</ul>
<hr>
<p><strong>Estimation of the Covariance Function</strong></p>
<p>First, we use estimated mean functions to estimate the raw covariance function <span class="math inline">\(\hat{C}(t,t')\)</span> for each signal</p>
<ul>
<li><span class="math inline">\(\large \hat{C}_i(t_{ij},t_{ik}) = ( s_i(t_{ij}) - \hat{u}(t_{ij}) ) ( s_i(t_{ik}) - \hat{u}(t_{ik}) )\)</span></li>
</ul>
<p>to estimate the covariance surface <span class="math inline">\(\hat{C}(t,t')\)</span>, we will use a 2 dimensional local weighted quadratic regression. In this equation t, and t’ each represent the two dimensions. W represents the weighting function</p>
<ul>
<li><span class="math inline">\(\large \underset{c_,c_1,c_2}{\min} \underset{{i=1}}{\sum} \underset{1 \le j \neq k \le m_i}{\sum} W(\frac{t_{ij} - t}{h} , \frac{t_{ik} - t}{h}) \left[ \hat{C}_i(t_{ij},t_{ik}) - c_0 - c_1(t - t_{ij}) - c_2 (t' - t_{ik})\right]^2\)</span></li>
<li>Solution: <span class="math inline">\(\hat{C}_i(t,t') = \hat{c}_0(t,t')\)</span></li>
</ul>
<p>Solve the estimate covariance function <span class="math inline">\(\hat{\phi}_k(t)\)</span> is estimated by discretizing the estimated covariance function <span class="math inline">\(\hat{C}(t,t')\)</span></p>
<p>Now that we can estimate the mean and covariance, we are in a position to compute the FPCA scores</p>
<p><strong>Computing FPC-Scores</strong></p>
<p>Computing eigen-function <span class="math inline">\(\hat{\phi}_k(t_j)\)</span> by solving <span class="math inline">\(\int_0^M \hat{C}(t,t') \hat{\phi}_k(t) dt = \hat{\lambda}_k \hat{\phi}_k(t')\)</span></p>
<ul>
<li><span class="math inline">\(\int_0^M \hat{\phi}_k(t) \times \hat{\phi}_m(t) dt = \begin{cases} 1 &amp; \text{, if } m = k \\ 0 &amp; \text{, if } m \neq k   \end{cases}\)</span></li>
<li>solved by discretizing the estimated covariance function <span class="math inline">\(\hat{C}(t_j , t'_j)\)</span></li>
</ul>
<p>Computing FPC-Scores <span class="math inline">\(\hat{\xi}_{ik}\)</span>: <span class="math inline">\(\large \xi_{ik} = int_0^M (s_i(t) - \hat{\mu}(t)) \phi_k(t) dt\)</span></p>
<ul>
<li>Numerical integration where <span class="math inline">\(t_0 = 0\)</span></li>
<li><span class="math inline">\(\large \hat{\xi}_{ik} = \sum_{j=1}^J (s_i(t) - \hat{\mu}(t)) \phi_k(t) (t_j - t_{j-1})\)</span>
<ul>
<li>the first term is the centered signal</li>
<li>the second term is the estimated eigen-function</li>
<li>the third term is the sampling interval ( may be dropped for regularly sampled signals at the unit intervals</li>
</ul></li>
</ul>
<p>Consider the following example where there is some missing data</p>
<p><img src="images/M1_018.png" width="500px"></p>
<p>We start by estimating the mean and covariance</p>
<p><img src="images/M1_019.png" width="500px"></p>
<p>Now we can solve for the eigen-vectors and eigen-functions to get the FPCA scores</p>
<p><img src="images/M1_020.png" width="500px"></p>
<p>For another real world style example</p>
<ul>
<li>See: Example.R::Functional data classification</li>
</ul>
<hr>
</section>
</section>
<section id="m2-image-analysis" class="level1">
<h1>M2 Image Analysis</h1>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>Goals:</p>
<ul>
<li>How to quantify an image</li>
<li>defining differences, and conversions, between RGB, Gray Scale, and Black and white</li>
<li>Controlling size and resolution</li>
<li>Using basic functions in Matlab (or python?)</li>
</ul>
</section>
<section id="image-analysis-levels" class="level2">
<h2 class="anchored" data-anchor-id="image-analysis-levels">Image Analysis Levels</h2>
<p>Image analysis is the process of processing raw images and extracting useful information for decision making.</p>
<ul>
<li>Level 0: Image Representation
<ul>
<li>(acquisition, sampling, quantization, compression)</li>
</ul></li>
<li>Level 1: Image to Image transformations
<ul>
<li>(enhancements, filtering, restoration, smoothing, segmentation)</li>
</ul></li>
<li>Level 2: Image to vector transformation
<ul>
<li>(feature extraction and dimension reduction)</li>
</ul></li>
<li>Level 3: Feature to decision mapping
<ul>
<li>next module</li>
</ul></li>
</ul>
<p>Image analysis uses and combines multiple fields:</p>
<ul>
<li>Signal Processing</li>
<li>Computational Photography</li>
<li>Computer vision</li>
<li>Computer graphics</li>
<li>Machine learning</li>
<li>Statistics</li>
<li>Applied Math</li>
</ul>
<p>What is an image?</p>
<ul>
<li>an image is generally represented in computers as a 2, or 3, dimensional matrix or light values, aka intensity
<ul>
<li>mathematically we denote this as a function <span class="math inline">\(f(x_1,x_2)\)</span> where the value is the intensity at that point in space</li>
<li>and a 3-dim function is <span class="math inline">\(f(x_1,x_2, x_3)\)</span> but there is a slight nuance here</li>
<li>the first dimension is often the channel and so the function returns the intensity at <span class="math inline">\((x_2, x_3)\)</span> for that channel</li>
</ul></li>
<li>a pixel is just an element in the matrix, or the value of <span class="math inline">\(f\)</span> at a point in space
<ul>
<li>Black and white images can be represented as a matrix of zero(s) and one(s)</li>
<li>greyscale images will be a 2dim matrix whose values range from 0 to 255</li>
<li>Colour images, aka RGB, are similar to greyscale except they include a third dimension to represent the colour</li>
<li>this results in 3 2-dim matrices</li>
</ul></li>
</ul>
</section>
<section id="basic-functions---opencv" class="level2">
<h2 class="anchored" data-anchor-id="basic-functions---opencv">Basic Functions - OpenCV</h2>
<p>A Few Notes on OpenCV</p>
<ul>
<li><strong>NOTE 1</strong> OpenCV is the leading Computer Vision Library. However, it doesn’t play nice with jupyter notebooks hence we often resort to matplot for some display functions</li>
<li><strong>NOTE 2</strong> One quirk about OpenCV is that it reads colour images in BGR format. Be careful here as it easy to forget when manipulating</li>
<li><strong>NOTE 3</strong> I took a course in Computer Vision if your interested in learning more
<ul>
<li>https://monzersaleh.github.io/GeorgiaTech/CS6476_ComputerVision.html</li>
</ul></li>
<li><strong>NOTE 4</strong> There are many great tutorials out there, here’s a couple I found
<ul>
<li>for getting started: https://learnopencv.com/getting-started-with-opencv/</li>
<li></li>
</ul></li>
</ul>
<p>Here’s a little demo</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> cv2.imread(<span class="st">'images/M2_test.jpg'</span>,<span class="dv">0</span>)     <span class="co"># here we just read in greyscale</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># cv2.imwrite('grayscale.jpg',t)</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Show the image with matplotlib</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>plt.imshow(np.asarray(t), cmap<span class="op">=</span><span class="st">'gray'</span>, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">255</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># without cmap matplot uses it's default scaling </span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative to view the image in black and white</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># first apply a threshold to convert to black and white</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>(thresh, img_bw) <span class="op">=</span> cv2.threshold(t2, <span class="dv">127</span>, <span class="dv">255</span>, cv2.THRESH_BINARY)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_bw, cmap<span class="op">=</span><span class="st">'Greys'</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Convert between Image Formats :
<ul>
<li>img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</li>
<li>there are many conversion flags</li>
<li>see: https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html</li>
</ul></li>
</ul>
<p><strong>Resizing</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imread(<span class="st">'images/M2_test.jpg'</span>)     <span class="co"># here we just read in greyscale</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(img.shape)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># this can be greater or less than the original image</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>new_size <span class="op">=</span> (<span class="dv">450</span>, <span class="dv">300</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>img_down <span class="op">=</span> cv2.resize(img, new_size, interpolation<span class="op">=</span> cv2.INTER_LINEAR)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># alternatively we can use a factor </span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>img_down <span class="op">=</span> cv2.resize(img, <span class="va">None</span>, fx<span class="op">=</span> <span class="fl">0.7</span>, fy<span class="op">=</span> <span class="fl">0.7</span>, interpolation<span class="op">=</span> cv2.INTER_LINEAR)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_down)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="image-transformations" class="level2">
<h2 class="anchored" data-anchor-id="image-transformations">Image transformations</h2>
<ul>
<li>Using Transformations techniques</li>
<li>Image histograms and how to shift and stretch it</li>
<li>Enhance an image using transformations</li>
</ul>
<p>Histograms represents the distribution of intensity values</p>
<ul>
<li>count the number of x,y pairs such that <span class="math inline">\(f(x,y)=i\)</span>, where i is some constant</li>
<li>the histogram is an estimate of the probability density function of the underlying random process</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>im_gry <span class="op">=</span> cv2.cvtColor(im, cv2.COLOR_BGR2GRAY) </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> cv2.calcHist([im_gry], [<span class="dv">0</span>], <span class="va">None</span>, [<span class="dv">256</span>], [<span class="dv">0</span>, <span class="dv">256</span>])</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the histogram</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Grayscale Histogram"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Bins"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"# of Pixels"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.plot(hist)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="dv">0</span>, <span class="dv">256</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="transformations" class="level3">
<h3 class="anchored" data-anchor-id="transformations">Transformations</h3>
<p>A transformation is really just some function that is applied to the image. The simplest example of this is a threshold</p>
<p><span class="math inline">\(g(x,y) = T( f(x,y))\)</span></p>
<p><strong>Threshold</strong></p>
<p><span class="math inline">\(g(x,y) = T( f(x,y)) = \begin{cases} 1 \text{ if } f(x,y) &gt; p \\ 0 \text{ if } f(x,y) \le p \end{cases}\)</span></p>
<p><strong>Shifting</strong></p>
<p>The brightness of an image can be changed by shifting its histogram.</p>
<p>Suppose the pixel values are ranged between L and U, for a greyscale image L=0 and U=255.<br>
Then the transformation function, T, is defined by</p>
<p><span class="math inline">\(g(x,y) = T( f(x,y)) = \begin{cases} U \text{ if } f(x,y) &gt; U - s \\ L \text{ if } f(x,y) \le L - s \\ f(x,y) \text{ otherwise } \end{cases}\)</span></p>
<p>Here s is some constant value</p>
<p><strong>Contrast</strong></p>
<p>The contrast of an image is defined by the difference between the max and min pixel intensity. It can be changed by stretching the histogram.<br>
The transformation function is defined by</p>
<p><span class="math inline">\(g(x,y) = T(f(x,y)) = \left( \frac{f(x,y) - min(f(x,y))}{ max(f(x,y)) - min(f(x,y)) } \right) \lambda\)</span></p>
<p><strong>Grey Level Resolution</strong></p>
<p>Gray Level resolution refers to change in shades or levels of gray in an image.</p>
<ul>
<li>The number of different colors in an image depends on the bits per pixel (bpp)
<ul>
<li><span class="math inline">\(L = 2^{bpp}\)</span></li>
</ul></li>
</ul>
<p>Grey level transformations can also be used for different image inchantments</p>
<ul>
<li>Linear - produces an image that resembles a negative
<ul>
<li><span class="math inline">\(g(x,y) = T(f(x,y)) = (L - 1) - f(x,y)\)</span></li>
</ul></li>
<li>Log
<ul>
<li><span class="math inline">\(g(x,y) = T(f(x,y)) = c \log(f(x,y)+1)\)</span></li>
</ul></li>
<li>Power-Law
<ul>
<li><span class="math inline">\(g(x,y) = T(f(x,y)) = c f(x,y)^{\gamma}\)</span></li>
</ul></li>
</ul>
</section>
</section>
<section id="convolutions-and-filtering" class="level2">
<h2 class="anchored" data-anchor-id="convolutions-and-filtering">Convolutions and Filtering</h2>
<ul>
<li>Defining convolutions</li>
<li>Applying image filtering using image convolutions with masks</li>
<li>Denoising and sharpening an image using convolutions</li>
</ul>
<section id="convolutions" class="level3">
<h3 class="anchored" data-anchor-id="convolutions">Convolutions</h3>
<p>Mathematically these are defined as follows</p>
<ul>
<li>Continuous: <span class="math inline">\(\large (f \ast g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t-\tau) d\tau = \int_{-\infty}^{\infty} f(t-\tau) g(\tau) d\tau\)</span></li>
<li>Discrete: <span class="math inline">\(\large (f \ast g)(n) = \sum_{m=-\infty}^{\infty} f(m)g(n-m) = \sum_{m=-\infty}^{\infty} f(n-m) g(m)\)</span></li>
</ul>
<p>Convolutions are widely used in image processing for denoising, blurring, sharpening, embossing, and edge detection</p>
<p><strong>Image Filtering</strong></p>
<p>Is the application of a mask, aka kernel, to an image using convolutions. Different filters can be used for blurring, sharpening, edge detection etc etc. A mask is defined as a matrix convolved with an image.</p>
<p><strong>I have diverged from the lectures on the topic of convolutions as I felt they were somewhat lacking. Convolutions are both simple and yet extremely powerful and are used in many fields. Most notably image processing and signal processing.</strong></p>
<p><img src="images/M2_convolutions.gif" width="550px" align="left"></p>
<p>At a high level a convolution is the repeated application of matrix multiplication with a summation. Take a good look at what is happening in the gif at left. In each step there is a matrix multiplication followed by a summation. Then the matrix takes a stride step to it’s next position.</p>
<p>You may also be interested to know that</p>
<ul>
<li><span class="math inline">\(f \ast g = g \ast f\)</span> ie it’s commutative</li>
<li><span class="math inline">\(\int (f \ast g) = \int f \times \int g\)</span> ie the integral of the convolution is the multiplication of the integrals</li>
</ul>
<p>Fun stuff eh? Here are a few more</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># First three are for edge detection</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>k1 <span class="op">=</span> [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>],[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],[<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>]]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>k2 <span class="op">=</span> [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="op">-</span><span class="dv">4</span>,<span class="dv">1</span>],[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>k3 <span class="op">=</span> [[<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>],[<span class="op">-</span><span class="dv">1</span>,<span class="dv">8</span>,<span class="op">-</span><span class="dv">1</span>],[<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># the next three perform blurring</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.ones(<span class="dv">3</span>,<span class="dv">3</span>)<span class="op">/</span><span class="dv">9</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> [[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>],[<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">2</span>],[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><br clear="both"></p>
<p>A standard approach for denoising is to use a gaussian filter mask.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">2</span>], [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>]] <span class="op">/</span> <span class="dv">16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A second more complex approach is to use local regression with a smooth basis, ie splines. this employs a Kronecker product.</p>
<p>For some 2dim splines B</p>
<ul>
<li><p><span class="math inline">\(B = B_2 \bigotimes B_1\)</span></p></li>
<li><p><span class="math inline">\(\hat{H} = B(B' B)^{-1} B'\)</span></p></li>
<li><p><span class="math inline">\(\hat{H}_i = B_i (B'_i B_i)^{-1} B'_i\)</span></p></li>
<li><p><span class="math inline">\(\hat{H} = \hat{H}_2 \bigotimes \hat{H}_1\)</span></p></li>
<li><p><span class="math inline">\(\hat{y} = (\hat{H}_2 \bigotimes \hat{H}_1) y\)</span></p></li>
<li><p>or simply</p></li>
<li><p><span class="math inline">\(\hat{Y} = \hat{H}_1 \bigotimes \hat{H}_2\)</span></p></li>
</ul>
</section>
</section>
<section id="image-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="image-segmentation">Image Segmentation</h2>
<ul>
<li>Define image segmentation</li>
<li>Apply Otsu’s model for image segmentation</li>
<li>Use k-means for partitioning images</li>
</ul>
<p>The goal of image segmentation is to partition an image into multiple sets of pixels/segments. Image segmentation has been widely used for object detection, face and fingerprint recognition, as well as medical imaging and video surveillance.</p>
<p>Various Methods exist for image segmentation</p>
<ul>
<li>Local and global thresholding</li>
<li>Otsu’s method</li>
<li>K-Means clustering</li>
</ul>
<p>Thresholding is the simplest type of segmentation. It converts a greyscale image to a binary one by applying a threshold function on a histogram. Here t is the threshold</p>
<ul>
<li><span class="math inline">\(p(x,y) = \begin{cases} white \text{ when } p(x,y) \ge t \\ black \text{ when } p(x,y) &lt; t \end{cases}\)</span></li>
</ul>
<section id="otsus-method" class="level3">
<h3 class="anchored" data-anchor-id="otsus-method">Otsu’s Method</h3>
<p>Here the goal is to determine the threshold t that, given an image histogram, minimizes or maximizes the intra-class variance.</p>
<p>Formulation</p>
<ul>
<li>Determine t by minimizing, or maximizing, the intra(or inter)-class variance defined by
<ul>
<li><span class="math inline">\(\sigma_{\omega}^2 = \omega_1(t) \sigma_1^2(t) + \omega_2(t) \sigma_2^2(t)\)</span></li>
</ul></li>
<li>Weight <span class="math inline">\(\omega_i(t)\)</span> are the probabilities of the two classes seperated by threshold <span class="math inline">\(t\)</span> and <span class="math inline">\(\sigma_i^2(t)\)</span> variance of these classes</li>
<li>Class probability:
<ul>
<li><span class="math inline">\(\large \omega_1(t) = \sum_{i=1}^t p(i),\; \omega_2(t) = 1 - \omega_1(t)\)</span></li>
</ul></li>
<li>Class Mean:
<ul>
<li><span class="math inline">\(\large \mu_1(t) = \frac{\sum_i^t i p(i)}{\omega_1} ; \;\; \mu_2 = \frac{\sum_{i=t}^{255} i p(i)}{\omega_2}\)</span></li>
</ul></li>
<li>Inter class variance:
<ul>
<li><span class="math inline">\(\large \sigma_b^2(t) = \sigma^2 - \sigma_{\omega}^2 = \omega_1(t) \omega_2(t) (\mu_1(t) - \mu_2(t))^2\)</span></li>
</ul></li>
</ul>
<p>Algorithm</p>
<pre><code>read(orig_image).convert_to_greyscale

get_histogram_counts(num_buckets=256) 

## Calculating group means
p = counts / sum(counts)
omega1 = cumsum(p);
omega2 = 1 - cumsum(p)
mu = cumsum(p * (1:num_bins))
mu_T = mu(end)
mu1 = mu. / omega1;
mu2 = (mu_T - mu1) ./ omega2

## Find the max value of sigma_b^2 
sigma_b_sq = (mu1-mu2)^2 (omega1 .* omega2)
maxval = max(sigma_b_sq)
idx = mean(find(sigma_b_sq == maxval)) 

## Thresholding 
level = (idx-1) / (num_bins - 1)
bw_image = orig_image &gt; (level*256)

## final image</code></pre>
</section>
<section id="k-means-clustering-method" class="level3">
<h3 class="anchored" data-anchor-id="k-means-clustering-method">K-Means Clustering Method</h3>
<p>K-means clustering is a method for partitioning a set of observations to K clusters such that the within-cluster variation is minimized.<br>
<span class="math inline">\(\large \sigma_{\omega}^2 = \sum_{j=1}^K \sum_{i=0}^t (x_i - \mu_j)^2\)</span></p>
<p>Algorithm: Given an integer k, and an image or features<br>
- a) Re-arrange the image pixles + such that the number of rows in the resulting matrix is equal to the number of pixels + and the number of columns is the same as the number of color channels + for example a 10 x 10 RGB image becomes a matrix of 100 x 3 - b) Randomly select K centres<br>
+ for ex for a greyscale image K is a number between 0 and 255 - c) Assign each pixel to the closest cluster (this is usually done based on proximity to the centre) - d) Update the cluster mean (center) - e) Repeate steps c &amp; d until convergence</p>
</section>
</section>
<section id="edge-detection" class="level2">
<h2 class="anchored" data-anchor-id="edge-detection">Edge Detection</h2>
<ul>
<li>How to perform edge detection using image derivatives</li>
<li>Sobel Edge detection</li>
<li>Using python for edge detection</li>
</ul>
<p><img src="images/M2_001.png" width="350px" align="left" padding="3px;" margin="3px;"></p>
<p>Edges are significant local changes of intensity in an image. Edge detection focuses on detecting pixels with a sudden intensity change.<br>
Often, points that lie on an edge are detected by</p>
<ul>
<li>Detecting the local maxima or minima of the first derivative</li>
<li>Detecting the zero-crossings of the second derivative</li>
</ul>
<p>Since we are often dealing with discrete pixels we approximate the gradient using finite difference</p>
<ul>
<li><span class="math inline">\(\large \frac{\partial f}{\partial x} = \frac{f(x+h_x,y) - f(x,y)}{h_x}\)</span></li>
<li><span class="math inline">\(\large \frac{\partial f}{\partial y} = \frac{f(x,y+h_y) - f(x,y)}{h_y}\)</span></li>
</ul>
<p>So for example for a 3 x 3 matrix:<br>
<span class="math inline">\(\begin{bmatrix}
a_0 &amp; a_1 &amp; a_2 \\
a_7 &amp; [i,j] &amp; a_3 \\
a_6 &amp; a_5 &amp; a_4
\end{bmatrix}\)</span></p>
<p>we compute the derivative at (i,j) as<br>
- <span class="math inline">\(\large \frac{\partial f}{\partial x} = (a_2+ca_3+a_4)-(a_0+ca_7+a_6)\)</span> - <span class="math inline">\(\large \frac{\partial f}{\partial y} = (a_6+ca_5+a_4)-(a_0+ca_1+a_2)\)</span> - for some constant weight c given to the closest pixels to the center of the mask</p>
<p><br clear="both"></p>
<p>Setting c=2 we get the Sobel operator</p>
<p><span class="math inline">\(M_x = \begin{bmatrix} -1 &amp; 0 &amp; 1 \\ -2 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 1 \end{bmatrix} \; \; \; M_y = \begin{bmatrix} -1 &amp; -2 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 2 &amp; 1 \end{bmatrix}\)</span></p>
<p>Sobel kernels can be decomposed as the products of an averaging and a differentiation kernel, the compute the gradient with smoothing</p>
<p><span class="math inline">\(M_x = \begin{bmatrix} -1 &amp; 0 &amp; 1 \\ -2 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} \begin{bmatrix} -1 &amp; 0 &amp; 1 \end{bmatrix}\)</span></p>
<p>Other examples of edge detection operators include prewitt, krisch, and laplacian.</p>
<p>Example of a Sobel Operator in action</p>
<p><img src="images/M2_002.png"></p>
<p>Here is Sobel again, but with an extra threshold mask applied<br>
- f = (fx*fx)+(fy*fy) is the magnitude</p>
<p><img src="images/M2_003.png"></p>
<p><strong>Kirsch</strong></p>
<p>From wikipedia: https://en.wikipedia.org/wiki/Kirsch_operator</p>
<p>The Kirsch operator or Kirsch compass kernel is a non-linear edge detector that finds the maximum edge strength in a few predetermined directions. It is named after the computer scientist Russell Kirsch. The operator takes a single kernel mask and rotates it in 45 degree increments through all 8 compass directions: N, NW, W, SW, S, SE, E, and NE. The edge magnitude of the Kirsch operator is calculated as the maximum magnitude across all directions:</p>
<p><strong>Prewitt</strong></p>
<p>Is similar to Sobel but with a different mask. Wikipedia: https://en.wikipedia.org/wiki/Prewitt_operator</p>
<p><span class="math inline">\(M_x = \begin{bmatrix} -1 &amp; 0 &amp; 1 \\ -1 &amp; 0 &amp; 1 \\ -1 &amp; 0 &amp; 1 \end{bmatrix} \; \; \; M_y = \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; -1 &amp; -1 \end{bmatrix}\)</span></p>
<p><strong>Laplacian</strong></p>
<p>Is a second order derivative mask. Often it is combined with a gaussian mask to reduce the noise in an image</p>
<p><img src="images/M2_004.png"></p>
<hr>
</section>
</section>
<section id="m3-tensor-data-analysis" class="level1">
<h1>M3 Tensor Data Analysis</h1>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p><strong>Objectives</strong><br>
- Linear Algebra review - Define a tensor &amp; associated terminology - Apply inner and outer product - Perform basic tensor operations</p>
<p>A Tensor is a multidimensional Array:<br>
- Scalars ie 13 is a 1 by 1 dimensional tensor (lower case plain letters: x) - vectors ie [13,42,2022] is a 1 by 2 dim tensor (using bold face lowercase: <span class="math inline">\(\textbf{x}\)</span> ) + the (i)’th element is denoted <span class="math inline">\(x_{i}\)</span> - matrices are 2 dimensional - here we refer to a m x n matrix (uppercase bold <span class="math inline">\(\textbf{X}\)</span>) + the (i,j)’th element is denoted <span class="math inline">\(x_{ij}\)</span> + the i’th row is <span class="math inline">\(x_{i:}\)</span> + the j’th col is <span class="math inline">\(x_{:j}\)</span> - matrix sequence 3+ dimensional tensors (uppercase euler script <span class="math inline">\(\mathcal{X}\)</span>) + the (i,j,k)’th element is denoted <span class="math inline">\(x_{ijk}\)</span> + also the n’th matrix is denoted <span class="math inline">\(\textbf{X}^{(n)}\)</span></p>
<p><strong>Please use your judgement while reading these notes - i won’t be very strict with these notations</strong></p>
<p><strong>Order:</strong> Is the number of dimensions of a tensor, also known as <strong>ways</strong> or <strong>modes</strong><br>
- 0-order is a scalar - 1-order is a column or row - 2-order is a 2D matrix - 3-order is a 3D matrix or a sequence of matrices</p>
<p><strong>Fibers:</strong> A fiber, the higher order analogue of a row or column, is defined by fixing all indices except one. By convention matrix cols are 1st order fibers, matrix rows are 2nd order matrices</p>
<p>for example consider a 3D matrix<br>
- Mode-1(col) fibers: fix 2nd and 3rd indices and let the 1st vary <span class="math inline">\(x_{:jk}\)</span> - Mode-2(row) fibers: fix 1st and 3rd indices and let the 1st vary <span class="math inline">\(x_{i:k}\)</span> - Mode-3(tube) fibers: fix 1st and 2nd indices and let the 1st vary <span class="math inline">\(x_{ij:}\)</span></p>
<p><strong>Splices</strong> similar to fibres, fix all but one index. Geometrically these can be thought of as sheets of the matrix along one plane<br>
- <span class="math inline">\(x_{i::}\)</span> - Horizontal slices - <span class="math inline">\(x_{:j:}\)</span> - Vertical slices - <span class="math inline">\(x_{::k}\)</span> - Frontal slices</p>
<p><strong>NORM</strong> the norm of a tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span> is the square root of the sum of the squares of all its elements<br>
- <span class="math inline">\(\large || \mathcal{X} || = \sqrt{\sum_{i_1 = 1} \sum_{i_2 = 1} \cdots \sum_{i_N = 1} (x_{i_1,i_2,...,i_N}^2)}\)</span> - this is analogous to the matrix Frobenius norm denoted <span class="math inline">\(|| \textbf{A} ||_F\)</span> for matrix <span class="math inline">\(\textbf{A}\)</span></p>
</section>
<section id="basic-operations" class="level2">
<h2 class="anchored" data-anchor-id="basic-operations">Basic Operations</h2>
<p><img src="images/M3_005.png" width="350px" align="right"></p>
<p><strong>Outer Product</strong>:<br>
- A multi-way vector outer product is a tensor where each element is the product of corresponding elements in vectors + <span class="math inline">\(\large \mathcal{X} = a \circ b \circ c\)</span> - let a be a vector if size m and b be a vector of size n then the product is a matrix of size (m x n) + <span class="math inline">\(\large \mathbf{a} \circ \mathbf{b} = \begin{bmatrix} a_1 b_1 &amp; ... &amp; a_1 b_n \\ a_2 b_1 &amp; ... &amp; a_2 b_n \\ \cdots \\ a_m b_1 &amp; ... &amp; a_m b_n   \end{bmatrix} = \mathbf{X}\)</span></p>
<p><br clear="both"></p>
<p>Let’s look at another example, this comes from stack overflow: https://math.stackexchange.com/questions/973559/outer-product-of-two-matrices<br>
Let <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span> and <span class="math inline">\(\mathbf{B} = \begin{bmatrix} 5 &amp; 6 &amp; 7 \\ 8 &amp; 9 &amp; 10 \end{bmatrix}\)</span></p>
<p>Then<br>
<span class="math inline">\(\large \mathbf{A} \circ \mathbf{B} = \begin{bmatrix} a_{11} B &amp; \cdots &amp; a_{1n} B \\ \cdots \\ a_{m1} B &amp; \cdots &amp; a_{mn} B \\ \end{bmatrix} = \begin{bmatrix} \begin{pmatrix} 5 &amp; 6 &amp; 7 \\ 8 &amp; 9 &amp; 10 \end{pmatrix} &amp; \begin{pmatrix} 10 &amp; 12 &amp; 14 \\ 16 &amp; 18 &amp; 20 \end{pmatrix} \\ \begin{pmatrix} 15 &amp; 18 &amp; 21 \\ 24 &amp; 27 &amp; 30 \end{pmatrix} &amp; \begin{pmatrix} 20 &amp; 24 &amp; 28 \\ 32 &amp; 36 &amp; 40 \end{pmatrix} \\ \end{bmatrix}\)</span></p>
<p><strong>We shall see later that the outerproduct for tensors is just a generalization of the Kronecker product</strong></p>
<p><strong>Inner Product</strong>:<br>
- Suppose <span class="math inline">\(\mathcal{X},\mathcal{Y} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span> are same sized tensors. - Then their inner product is defined by the sum of the products of their entries + <span class="math inline">\(\large \langle \mathcal{X},\mathcal{Y} \rangle = \sum_{i_1 = 1} \sum_{i_2 = 1} \cdots \sum_{i_N = 1} x_{i_1,i_2,...,i_N} y_{i_1,i_2,...,i_N}^2\)</span></p>
<p>For two vectors this results in a <strong>Scalar</strong></p>
<p><strong>Matricization</strong><br>
1. Tensor Matricization: aka flattening or unfolding, unfolds an n-way tensor into a matrix 2. Mode-n matricization arranges the mode-n fibers as columns of a matrix, denoted <span class="math inline">\(\textbf{X}_{(n)}\)</span> 3. Vectorization of a tensor, denoted by vec(<span class="math inline">\(\mathcal{X}\)</span>), is transforming a tensor to a vector</p>
<section id="multiplication" class="level3">
<h3 class="anchored" data-anchor-id="multiplication">Multiplication</h3>
<p>N-Mode Product is the multiplication of a tensor by a matrix(or vector) in mode-n</p>
<p><strong>N-Mode (matrix) Product</strong><br>
- Consider a tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^N\)</span> and a matrix <span class="math inline">\(\mathbf{U} \in \mathcal{R}^{J \times n}\)</span> where n &lt; N - The n-mode product is written <span class="math inline">\(\mathcal{X} \times_n \mathbf{U}\)</span> and is of size <span class="math inline">\(I_1 \times ... \times I_{n-1} \times J \times I_{n+1} \times ... I_N\)</span> - We therefore have that + <span class="math inline">\(\large \mathcal{Y} = (\mathcal{X} \times_n \mathbf{U})_{i_1,...,i_{n-1},J,i_{n+1},...,i_N}\)</span> + <span class="math inline">\(\large \mathcal{Y} = \sum_{i_n = 1}^{I_n} x_{i_1,i_2,...,i_n,...,i_N} u_{ji_n}\)</span> + <span class="math inline">\(\large \mathbf{Y}_{(n)} = \mathbf{U} \mathbf{X}_{(n)}\)</span></p>
<p><img src="images/M3_001.png"></p>
<p><strong>N-Mode (vector) Product</strong><br>
- The n-mode vector product of a tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^N\)</span> with a vector <span class="math inline">\(v \in \mathcal{R}^{I_n}\)</span> is denoted by <span class="math inline">\(\mathcal{X} \; \bar{\times}_n \; \mathbf{U}\)</span> - The result is of order N-1, ie, Elementwise: + <span class="math inline">\(\large \mathcal{Y} = (\mathcal{X} \; \bar{\times}_n \; v)_{i_1,...,i_{n-1},i_{n+1},...,i_N}\)</span> + <span class="math inline">\(\large \mathcal{Y} = \sum_{i_n=1}^{I_n} x_{i_1,i_2,...,i_n,...,i_N} v_{ji_n}\)</span></p>
<p><img src="images/M3_002.png"></p>
</section>
<section id="kronecker-product" class="level3">
<h3 class="anchored" data-anchor-id="kronecker-product">Kronecker Product</h3>
<p>Kronecker Product of matrices <span class="math inline">\(\mathbf{A} \in \mathcal{R}^{I \times J}\)</span> and <span class="math inline">\(\mathbf{B} \in \mathcal{R}^{K \times L}\)</span> is denoted <span class="math inline">\(\mathbf{A} \otimes \mathbf{B}\)</span>. The result is a matrix of size <span class="math inline">\((IK) \times (JL)\)</span> and is defined as</p>
<p><span class="math display">\[\large \mathbf{A} \otimes \mathbf{B} =  
\begin{bmatrix}
a_{11} \mathbf{B} &amp; a_{12} \mathbf{B} &amp; \cdots &amp; a_{1J} \mathbf{B} \\
a_{21} \mathbf{B} &amp; a_{22} \mathbf{B} &amp; \cdots &amp; a_{2J} \mathbf{B} \\
\cdots &amp; \cdots &amp; \cdots \\
a_{I1} \mathbf{B} &amp; a_{I2} \mathbf{B} &amp; \cdots &amp; a_{IJ} \mathbf{B}
\end{bmatrix}
= [\mathbf{a_1} \otimes \mathbf{b_1} \;\; \mathbf{a_1} \otimes \mathbf{b_2} \;\; \cdots \;\; \mathbf{a_J} \otimes \mathbf{b_{L-1}} \;\; \mathbf{a_J} \otimes \mathbf{b_{L}}]\]</span></p>
<p>Example</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([ [<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>              ,[<span class="dv">3</span>,<span class="dv">4</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>              ,[<span class="dv">5</span>,<span class="dv">6</span>]])</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array([ [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>              ,[<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>]])</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>np.kron(A,B)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>array([[ <span class="dv">1</span>,  <span class="dv">2</span>,  <span class="dv">3</span>,  <span class="dv">2</span>,  <span class="dv">4</span>,  <span class="dv">6</span>],</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>       [ <span class="dv">4</span>,  <span class="dv">5</span>,  <span class="dv">6</span>,  <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">12</span>],</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>       [ <span class="dv">3</span>,  <span class="dv">6</span>,  <span class="dv">9</span>,  <span class="dv">4</span>,  <span class="dv">8</span>, <span class="dv">12</span>],</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>       [<span class="dv">12</span>, <span class="dv">15</span>, <span class="dv">18</span>, <span class="dv">16</span>, <span class="dv">20</span>, <span class="dv">24</span>],</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>       [ <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>,  <span class="dv">6</span>, <span class="dv">12</span>, <span class="dv">18</span>],</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>       [<span class="dv">20</span>, <span class="dv">25</span>, <span class="dv">30</span>, <span class="dv">24</span>, <span class="dv">30</span>, <span class="dv">36</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="khatri-rao-product" class="level3">
<h3 class="anchored" data-anchor-id="khatri-rao-product">Khatri-Rao Product</h3>
<p>Khatri-Rao product is the “matching columnwise” kronecker product.<br>
Given Matrices <span class="math inline">\(\mathbf{A} \in \mathcal{R}^{I \times K}\)</span> and <span class="math inline">\(\mathbf{B} \in \mathcal{R}^{J \times K}\)</span>, the Khatri-Rao product <span class="math inline">\(\mathbf{A} \odot \mathbf{B}\)</span> is a matrix of size <span class="math inline">\((IJ)\times(K)\)</span> computed by</p>
<ul>
<li><span class="math inline">\(\mathbf{A} \odot \mathbf{B} = [\mathbf{a_1} \otimes \mathbf{b_1} \; \mathbf{a_2} \otimes \mathbf{b_2} \; \cdots \; \mathbf{a_K} \otimes \mathbf{b_K}]\)</span></li>
<li>If <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> are vectors then the Khatri-Rao and kronecker products are identical
<ul>
<li>ie <span class="math inline">\(\mathbf{a} \otimes \mathbf{b} = \mathbf{a} \odot \mathbf{b}\)</span></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> linalg</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>             ,[<span class="dv">3</span>,<span class="dv">4</span>]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>             ,[<span class="dv">5</span>,<span class="dv">6</span>]])</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>            ,[<span class="dv">3</span>,<span class="dv">4</span>]])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">#tl.tenalg.khatri_rao(A,B)</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>linalg.khatri_rao(A,B)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>array([[ <span class="dv">1</span>,  <span class="dv">4</span>],</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>       [ <span class="dv">3</span>,  <span class="dv">8</span>],</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>       [ <span class="dv">3</span>,  <span class="dv">8</span>],</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>       [ <span class="dv">9</span>, <span class="dv">16</span>],</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>       [ <span class="dv">5</span>, <span class="dv">12</span>],</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>       [<span class="dv">15</span>, <span class="dv">24</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="hadamard-product" class="level3">
<h3 class="anchored" data-anchor-id="hadamard-product">Hadamard Product</h3>
<p>This is simple the elementwise multiplication of matrices <span class="math inline">\(\mathbf{A} \in \mathcal{R}^{I \times J}\)</span> and <span class="math inline">\(\mathbf{B} \in \mathcal{R}^{I \times J}\)</span> and is denoted <span class="math inline">\(\mathbf{A} \ast \mathbf{B}\)</span>. It results in a matrix of size <span class="math inline">\((I)\times(J)\)</span> computed by</p>
<p><span class="math display">\[\large \mathbf{A} \ast \mathbf{B} =  
\begin{bmatrix}
a_{11} b_{11} &amp; a_{12} b_{12} &amp; \cdots &amp; a_{1J} b_{1J} \\
a_{21} b_{21} &amp; a_{22} b_{22} &amp; \cdots &amp; a_{2J} b_{2J} \\
\cdots &amp; \cdots &amp; \cdots \\
a_{I1} b_{I1} &amp; a_{I2} b_{I2} &amp; \cdots &amp; a_{IJ} b_{IJ}
\end{bmatrix}\]</span></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>]])</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>]])</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>A<span class="op">*</span>B</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>array([[ <span class="dv">1</span>,  <span class="dv">4</span>],</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>       [ <span class="dv">9</span>, <span class="dv">16</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="tensor-decomp---cp-method" class="level2">
<h2 class="anchored" data-anchor-id="tensor-decomp---cp-method">Tensor Decomp - CP Method</h2>
<p>There’s a great little repo in bitbucket that illustrates many of these concepts from scratch<br>
- https://github.com/mohammadbashiri/tensor-decomposition-in-python</p>
<p><strong>Objectives</strong><br>
- Define Tensor Ranks - Define Rank-One Tensors - Perform CANDECOMP/PARFAC (CP) Decomposition - Identify low dimensional structure using CP</p>
<p>An N-Order Rank-one Tensor can be decomposed as the outer product of multiple vectors<br>
- For example a 3-Order Rank-one tensor <span class="math inline">\(X \in R^{I \times J \times K}\)</span> can be obtained as <span class="math inline">\(\mathcal{X} = a \circ b \circ c\)</span> where a,b,c are vectors.</p>
<p>CP Decomposition factorizes a tensor into a sum of component rank-one tensors<br>
- ie for <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{I \times J \times K}\)</span> - the CP decomposition is given by <span class="math inline">\(\mathcal{X} \approx \sum_{r=1}^R a_r \circ b_r \circ c_r\)</span> + here <span class="math inline">\(a_r \in \mathbb{R}^{I}\)</span>, <span class="math inline">\(b_r \in \mathbb{R}^{J}\)</span>, <span class="math inline">\(c_r \in \mathbb{R}^{K}\)</span></p>
<p>If R in the summation terms is also the rank of the tensor then the approximation is actually exact, ie equality. So for example for a rank-2 tensor we would need need two summations to achieve equality. In such a scenario the decomposition is also Unique. However finding this R is not so simple and straight forward.</p>
<div style="text-align: center;">
<img src="images/M3_003.png" align="center">
</div>
<p><strong>Rank of a Tensor</strong></p>
<p>The Rank of a tensor <span class="math inline">\(\mathcal{X}\)</span> is the smallest number of rank-one tensors whose some can generate <span class="math inline">\(\mathcal{X}\)</span></p>
<p><strong>NP-HARD WARNING</strong> Determining the rank of a tensor is an NP-Hard Search Problem. Some weaker upper exist that can help to solve the problem by restricting the search/decision space. In particular: <span class="math inline">\(rank(\mathcal{X}) \le min\{IJ, JK, IK \}\)</span></p>
<p>Factor Matrices can be created by concatenating the corresponding rank-one vectors from the rank-one tensors<br>
- For example<br>
- for <span class="math inline">\(A=[a_1 \; a_2 \; \cdots \; a_R]\)</span>, <span class="math inline">\(B=[b_1 \; b_2 \; \cdots \; b_R]\)</span>, + <span class="math inline">\(C = [c_1 \; c_2 \; \cdots \; c_R]\)</span> + then + <span class="math inline">\(\mathcal{X} \approx (a_1 \circ b_1 \circ c_1) + \cdots (a_R \circ b_R \circ c_R)\)</span></p>
<ul>
<li>Then the CP decomposition can be rewritten by factor matrices in matrix form:<br>
</li>
<li>$ = _{r=1}^R a_r b_r c_r $
<ul>
<li><span class="math inline">\(X_{(1)} \approx A(C \odot B)^T\)</span> <strong>NB</strong> this is the Khatri-Rao product</li>
<li><span class="math inline">\(X_{(2)} \approx B(C \odot A)^T\)</span></li>
<li><span class="math inline">\(X_{(3)} \approx C(B \odot A)^T\)</span></li>
</ul></li>
</ul>
<div style="text-align: center;">
<img src="images/M3_004.png" align="center">
</div>
<p>To see why this is so useful consider a tensor of dimension 100 by 100 by 100, with a rank of 5. Such a tensor would have around 1,000,000 elements. However because the rank is 5 we can apply the decomposition above to obtain a factorizing matrix of around 100 by 5, which requires about 500 elements. Three such factorizing matrices would total 1,500 elements … which is a far cry from the original 1 million.</p>
<p>One problem with this form of CP Decomposition is multiple solutions cause scaling. For example, if we divide the columns of A matrix by two, and then at the same time multiply the column of B matrix by 2, then they cancel out each other, and the resulting tensor would be the same. So we can get a new set of factor matrices for the same tensor.</p>
<p>To avoid this issue, one way would be to normalize the columns of the factor matrices. What this means is that the norm of each vector should be one. Then we can rewrite the CP Decomposition, using the scaling factors <span class="math inline">\(\Lambda\)</span></p>
<ul>
<li>$ = _{r=1}^R _r a_r b_r c_r $
<ul>
<li>and <span class="math inline">\(X_{(2)} \approx B \; \Lambda \; (C \odot A)^T\)</span></li>
<li>It should be noted that <span class="math inline">\(\Lambda\)</span> is a diagonal matrix with the lambda values along the diagonal</li>
</ul></li>
</ul>
<p>We can of course generalize this for n-th order tensors as<br>
- <span class="math inline">\(\large \mathcal{X} \approx [\lambda ; \; A^{(1)},A^{(2)},\cdots,A^{(n)}]\)</span> - <span class="math inline">\(\large = \sum_{r=1}^R \lambda_r a_r^{(1)} \circ a_r^{(2)} \circ \cdots \circ a_r^{(n)}\)</span></p>
<p>and<br>
- <span class="math inline">\(\large \mathbf{X}_{(k)} \approx A^{(k)} \; \Lambda \; (A^{(n)} \odot \cdots A^{(k-1)} \odot A^{(k+1)} \cdots \odot A^{(1)})^T\)</span></p>
<section id="cp-decomposition---computation" class="level3">
<h3 class="anchored" data-anchor-id="cp-decomposition---computation">CP-Decomposition - Computation</h3>
<p>CP decomposition can be obtained using a method similar to Least Squares<br>
- <span class="math inline">\(\large \underset{A,B,C,\lambda}{min} || \mathcal{X} - [\lambda ; \; A,B,C]||^2 = || \mathcal{X} - \sum_{r=1}^R \lambda_r a_r \circ b_r \circ c_r ||^2\)</span> + S.T. <span class="math inline">\(||a_r||=1\)</span>, <span class="math inline">\(||b_r||=1\)</span>, <span class="math inline">\(||c_r||=1\)</span>; for <span class="math inline">\(r=1,2,...,R\)</span> + <strong>NB</strong> the second, or last, term above is the reconstruction error</p>
<p><strong>WARNING: For some odd reason, a typo perhaps?, the class calls this the “Alternative Least Squares” however a quick google search yields only “Alternating Least Squares” which leads me to believe this is the real name</strong></p>
<p>The <strong>Alternative Least Squares</strong> is an algorithm for solving this optimization problem. ALS uses iteration to solve for the optimization problem for one matrix given all other matrices.</p>
<p>So for example given <span class="math inline">\(\mathbf{B}\)</span> and <span class="math inline">\(\mathbf{C}\)</span></p>
<p><span class="math inline">\(\large \tilde{A} = \mathbf{A} \Lambda = \underset{\tilde{A}}{min} || X_{(1)} - \tilde{A} (\mathbf{C} \odot \mathbf{B})^T ||_F \rightarrow \tilde{A} = X_{(1)} (\mathbf{H}^T \mathbf{H})^{-1} \mathbf{H}^T\)</span> ; where <span class="math inline">\(\large \mathbf{H} = (\mathbf{C} \odot \mathbf{B})^T\)</span></p>
<p>Then once <span class="math inline">\(\tilde{A}\)</span> is found we can compute <span class="math inline">\(\mathbf{A}\)</span> rather easily<br>
- <span class="math inline">\(\large a_r = \frac{\tilde{a}}{||\tilde{a}_r||}\)</span></p>
<p><strong>ALS Algorithm</strong></p>
<pre><code>Given an n-Mode tensor X and a rank R

initialize A^(1),A^(2),...,A^(n), in R^(I_n x R) with random entries
           # these are not exponents
           
    # Convergence here is defined as 
    #    some minimal change in the value of the objective function
    #    little to no change in the factor matrices after a defined number of iterations
    
repeat until convergence
    for k = 1 -&gt; n:
        # Compute hadamard product
        V = (A.T^(1) A^(1)) * ... * (A.T^(k-1) A^(k-1)) * (A.T^(k+1) A^(k+1)) * ... * (A.T^(n) A^(n))
        # Compute Khatri-Rao product
        A^(k) = X_(k) ( A^(n) o ... o A^(k+1) o A^(k-1) o ... o A^(1) ) ( V.T V )^-1 V.T
        Normalize the columns of A^(k) and store the norm as lambda
        end
        
Return A^(1),A^(2),...,A^(n), and lambda
</code></pre>
</section>
<section id="example-heat-transfer-data" class="level3">
<h3 class="anchored" data-anchor-id="example-heat-transfer-data">Example: Heat Transfer Data</h3>
<p>A series of images are generated from a heat transfer process defined by the equation<br>
- <span class="math inline">\(\large \frac{\partial S_i}{\partial t} = \alpha_i (\frac{\partial^2 S_i}{\partial x^2} + \frac{\partial^2 S_i}{\partial y^2})\)</span> - where the thermal diffusivity coefficient <span class="math inline">\(\large \alpha_i \sim uniform(0.5x10^{-5}, 1.5x10^{-5})\)</span> - and some iid noise is added to each pixel</p>
<div style="text-align: center;">
<img src="images/M3_006.png" align="center">
</div>
<p>For R=1 the columns <span class="math inline">\(a_1,b_1,c_1\)</span> are computed</p>
<div style="text-align: center;">
<img src="images/M3_007.png" align="center">
</div>
<p>We then choose R using the lowest AIC score<br>
- where AIC = <span class="math inline">\(2 || \mathcal{X} - \sum_{r=1}^R \lambda_r a_r \circ b_r \circ c_r ||^2 + 2k\)</span> - and R is upper bounded by min(IJ,IK,JK) = 210 + turns out R should be 4</p>
<div style="text-align: center;">
<img src="images/M3_008.png" align="center">
</div>
</section>
</section>
<section id="tucker-decomposition" class="level2">
<h2 class="anchored" data-anchor-id="tucker-decomposition">Tucker Decomposition</h2>
<ul>
<li>Perform Tucker Decomposition</li>
<li>Identify low dimensional structure using Tucker</li>
</ul>
<p>Tucker Decomposition decomposes a tensor into a core tensor multiplied (or transformed) by a set of factorizing matrices along each mode.</p>
<p>For example: a third-order tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{I \times J \times K}\)</span><br>
- <span class="math inline">\(\large \mathcal{X} \approx \mathcal{G} \times_1 A \times_2 B \times_3 C = \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_p \circ b_q \circ c_r = [\![ \mathcal{G};A,B,C]\!]\)</span> - where <span class="math inline">\(A \in \mathbb{R}^{I \times P}\)</span>, <span class="math inline">\(B \in \mathbb{R}^{J \times Q}\)</span>, <span class="math inline">\(C \in \mathbb{R}^{K \times R}\)</span>, <span class="math inline">\(\mathcal{G} \in \mathbb{R}^{P \times Q \times R}\)</span></p>
<div style="text-align: center;">
<img src="images/M3_009.png" align="center">
</div>
<p>Let’s Examine the equation in more detail<br>
<span class="math display">\[\Large \mathcal{X} \approx \mathcal{G} \times_1 A \times_2 B \times_3 C = \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_p \circ b_q \circ c_r = [\![ \mathcal{G};A,B,C]\!]\]</span></p>
<ul>
<li>The core tensor <span class="math inline">\(\mathcal{G}\)</span> captures the interaction among the different modes and since generally P&lt;I, Q&lt;J, and R&lt;I, the core tensor is considered as the compressed version of the original tensor <span class="math inline">\(\mathcal{X}\)</span></li>
<li>In most cases it is assumed that the factor matrices are column-wise orthogonal, however this is not a requirement.</li>
<li>CP-Decomposition can be seen as a special case of Tucker where the core matrix is super-diagonal and P=Q=R.</li>
</ul>
<p><strong>N-Rank of Tensor</strong></p>
<ul>
<li>Consider an n-mode tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{I_1 \times ... \times I_N}\)</span>. The n-rank of this tensor, denoted <span class="math inline">\(R_n = rank_n(\mathcal{X})\)</span> is defined by the column rank of the mode-n fibers, <span class="math inline">\(\mathbf{X}_{(n)}\)</span></li>
<li>The n-rank of a tensor is NOT the rank of a tensor that we discussed earlier.</li>
<li>Given a set of n-ranks, we can find the exact Tucker decomposition of <span class="math inline">\(\mathcal{X}\)</span> for this set</li>
<li>If the rank used for decomposition is smaller than the corresponding n-rank, truncated Tucker decomposition will be obtained.</li>
</ul>
<div style="text-align: center;">
<img src="images/M3_010.png" align="center">
</div>
<section id="hosvd---higher-order-svd" class="level3">
<h3 class="anchored" data-anchor-id="hosvd---higher-order-svd">HOSVD - Higher Order SVD</h3>
<p>HSOVD is a simple method for performing a Tucker decomposition. The idea here is to find the factor matrices for each mode seperately, in a manner such that the maximum variation of the mode is captured. This can be done by performning SVD decomposition for each mode-k fiber of the tensor and keeping the <span class="math inline">\(R_k\)</span> leading left singular values of the matrix <span class="math inline">\(\mathbf{X}_{(k)}\)</span>, denoted by <span class="math inline">\(\mathbf{A}^{(k)}\)</span></p>
<p>The core tensor can then be obtained by<br>
- <span class="math inline">\(\large \mathcal{G} = \mathcal{X} \times_1 A.T^{(1)} \times_2 A.T^{(2)} \times_3 \cdots \times_n A.T^{(n)}\)</span></p>
<p>Note that the truncated HOSVD is not optimal with respect to the least squared lack of fit measure.</p>
</section>
<section id="tucker-decomp-als-algo" class="level3">
<h3 class="anchored" data-anchor-id="tucker-decomp-als-algo">Tucker Decomp: ALS Algo</h3>
<p>Similar to previous example a natural approach to Tucker decomposition is the minimization of the squared error between the original tensor and the reconstructed one.</p>
<ul>
<li><span class="math inline">\(\Large \underset{\mathcal{G};A^{(1)},...,A^{(n)}}{min} || \mathcal{X} - [\![ \mathcal{G}; A^{(1)},...,A^{(n)} ]\!] ||^2\)</span></li>
<li>Where <span class="math inline">\(\large \mathcal{G} \in \mathbb{R}^{R_1 \times ... \times R_N}\)</span> and <span class="math inline">\(\mathbf{A}^{(k)} \in \mathbb{R}^{I_k \times R_k}\)</span> with <span class="math inline">\(\mathbf{A}^{(k)}\)</span> column-wise orthogonal for all k’s</li>
</ul>
<p>We will skip the derivation, however it can be shown that the above reduces to<br>
- <span class="math inline">\(\large \underset{\mathcal{G};A^{(1)},...,A^{(n)}}{min} || \mathcal{X} - [\![ \mathcal{G}; A^{(1)},...,A^{(n)} ]\!] ||^2\)</span> - <span class="math inline">\(\large = \underset{\mathcal{G};A^{(1)},...,A^{(n)}}{max} || \mathcal{X} \times_1 A.T^{(1)} \times_2 ... \times_n A.T^{(n)} ||^2\)</span> - <span class="math inline">\(\large = \underset{\mathcal{G};A^{(1)},...,A^{(n)}}{max} || A.T^{(k)} \mathcal{X}_{(k)} ( A.T^{(n)} \otimes ... \otimes A.T^{(k+1)} \otimes A.T^{(k-1)} \otimes ... \otimes A.T^{(1)}) ||^2\)</span> - which of course is the Khatri-Rao product we discussed earlier</p>
<p>Again, as mentioned above, <span class="math inline">\(\large \mathcal{G} \in \mathbb{R}^{R_1 \times ... \times R_N}\)</span> and <span class="math inline">\(\mathbf{A}^{(k)} \in \mathbb{R}^{I_k \times R_k}\)</span> with <span class="math inline">\(\mathbf{A}^{(k)}\)</span> column-wise orthogonal for all k’s</p>
<p>Using ALS and given all factor matrices except <span class="math inline">\(A^{(k)}\)</span>, we can apply SVD(Singular Value Decomposition) on <span class="math inline">\(\mathcal{X}_{(k)} ( A.T^{(n)} \otimes ... \otimes A.T^{(k+1)} \otimes A.T^{(k-1)} \otimes ... \otimes A.T^{(1)})\)</span> and keep the <span class="math inline">\(R_k\)</span> leading left singular values</p>
<p><span class="math display">\[\large \underset{A^{(k)}}{max} || \mathcal{X} \times_1 A.T^{(1)} \times_2 ... \times_n A.T^{(n)} ||^2\]</span> <span class="math display">\[= A.T^{(k)} \mathcal{X}_{(k)} ( A.T^{(n)} \otimes ... \otimes A.T^{(k+1)} \otimes A.T^{(k-1)} \otimes ... \otimes A.T^{(1)}) ||^2\]</span> <span class="math display">\[S.T: \mathbf{A}^{(k)} \in \mathbb{R}^{I_k \times R_k} \text{ and column-wise orthogonal for all k's }\]</span></p>
<p><span class="math inline">\(\color{red}{\text{Algorithm}}\)</span></p>
<pre><code>Given an n-mode tensor X and R_1,R_2,...,R_n

initialize A^(1),....,A^(n) in R^{I_n x R_n} using HOSVD

Repeat until convergence
    for k = 1 -&gt; n
        Y = X times_1 A.T^(1) times_2 A.T^(2) times_3 ... times_k-1 A.T^(k-1) times_k+1 A.T^(k+1) ... times_n A.T^(n) 
        A^(k) &lt;- R_k leading left singular values of Y_(k)
                 # ie the corresponding vectors
    end 
    G = X times_1 A.T^(1) times_2 A.T^(2) times_3 ... times_k-1 A.T^(k-1) times_k+1 A.T^(k+1) ... times_n A.T^(n) 

return G, A^(1),....,A^(n)</code></pre>
</section>
<section id="example-heat-transfer-v2" class="level3">
<h3 class="anchored" data-anchor-id="example-heat-transfer-v2">Example: Heat Transfer v2</h3>
<p>Recall from our earlier example</p>
<p>A series of images are generated from a heat transfer process defined by the equation<br>
- <span class="math inline">\(\large \frac{\partial S_i}{\partial t} = \alpha_i (\frac{\partial^2 S_i}{\partial x^2} + \frac{\partial^2 S_i}{\partial y^2})\)</span> - where the thermal diffusivity coefficient <span class="math inline">\(\large \alpha_i \sim uniform(0.5x10^{-5}, 1.5x10^{-5})\)</span> - then some iid noise is added to each pixel</p>
First we need to determine the appropriate Rank. To do this we try different combos and choose the lowest AIC
<div style="text-align: center;">
<img src="images/M3_011.png">
</div>
Then we can perform the decomposition<br>

<div style="text-align: center;">
<img src="images/M3_012.png">
</div>
</section>
</section>
<section id="tensor-applications-pt1" class="level2">
<h2 class="anchored" data-anchor-id="tensor-applications-pt1">Tensor Applications-Pt1</h2>
<ul>
<li>Determine applications of CP &amp; Tucker decomposition</li>
<li>Use Tucker and CP in regression problems</li>
<li>Perform regression analysis for scalar response and tensor predictors</li>
</ul>
<p>A wide range of high dimensional data including signals, windows, images or network data can by represented by tensors. As we discussed earlier, important information of high dimension data is embedded in a low dimensional space. Therefore, one intuitive way of finding this low dimensional structure or the low dimensional space is to perform tensor decompositions.</p>
<p>The first application we are going to discuss is degradation modeling and prediction of remaining lifetime of a system or a part. first let us define degradation.<br>
- Degradation is a gradual process of damage accumulation, which results in the failure of an engineering system. - This gradual process is often captured by a series of data points known as degradation signal.</p>
<p>For example, consider a ball bearing inside an electric motor. At an early stage of it’s life it’s in a healthy condition. Therefore, when it rotates the friction is minimum and does not create any vibration or heat.</p>
<p><img src="images/M3_013.png" width="450px" align="left"></p>
<p>You can see here this is a vibration signal, here we have very low vibration. But overtime, it starts degrading and the damage on the surface create more vibrations. For example, you can see there is one fault here and after that the vibration increases. When these vibrations crosses a threshold, as you can see, this is a threshold, then we can say the bearing is worn out and we should replace it.</p>
<p>Degradation modeling and prognostic is concerned with using these partially observed degradation signal, and try to estimate or predict the remaining life time of the system or the part. For example, at this point by observing this data can we predict what would be the remaining life at time of the data.</p>
<p>Or at this point by observing the data from the beginning until here, can we predict what is the remaining lifetime of the system. This is called degradation analysis and modelling.</p>
<p><br clear="both"></p>
<p><img src="images/M3_014.png" width="450px" align="left"></p>
<p>Sometimes a degradation data is in the form of image streams. For example, in some cases, the non-contact measurement devices require we use an infrared camera.</p>
<p>As you can see in this bearing example, an infrared camera can be used to capture the temperature of the housing where the bearings are located. When bearings start degrading, if you look at the sequence of images over time, we can see a clear upward trend in the images stream or the temperature represented by images stream.</p>
<p>It’s obvious here and it’s also obvious in this image video. The goal of this study is to use an image stream to predict the remaining lifetime of a system. As you can see an image stream can be presented by a third order tensor. So basically each image is a matrix, and we have a stream of images over time. So we can use a third order tensor to represent the data of image stream.</p>
<p><br clear="both"></p>
<p><img src="images/M3_015.png" width="450px" align="right"></p>
<p>At right is the setting of the problem. We have a training dataset as you can see, this training dataset includes image streams of n systems, and these are the image streams. As well as corresponding time to failure.</p>
<p>So we let this system run until failure, we capture the image stream and the corresponding time to failure. What we want to do, we like to use this data and build a regression model that uses image stream as an input, and can predict the time to failure as an output.</p>
<p>And then finally, we can use this regression model for a new system. And for the new system can capture, again, the image stream, and use this function to predict the remaining lifetime of the system at each time for any basic length of image history. As we can see this is a regression problem but it’s a bit different from regular regression problem, because the y or response variable is a scalar, however the input or predictors are not vectors any more. These are image streams of third order tensor.</p>
<br clear="both">
<div style="text-align: center;">
<img src="images/M3_016.png" align="center">
</div>
<p>At the top left is the prediction model that we build. We start with building the model, we assume that y, time to failure, has a mean and standard deviation and error. And what we do, we said the mean of this time to failure can be predicted by the set of signals that we have or the third order tensor that we have.</p>
<p>So similar to regression, we build the regression model, we have some coefficients, and the inner product of coefficient and predictors,<span class="math inline">\(\langle \mathcal{B}, \mathcal{S}_i \rangle\)</span> , that we have plus some intercept, gonna give us the mean value of this time to failure that can be used as prediction. Let’s look at the regression model here.</p>
<p>This is the inner product of two tensors. So whatever the dimension of the predictors are, we need to have the same dimension of coefficients or same number of coefficients. So the challenge is here, because the number of data points or the dimensionality of this predictor is high. And therefore, the number of parameters that need to estimate is going to be very high.</p>
<p>For example, let’s assume that each image is 20 by 20, and then we have 20 of those images. So the length of our signal or image stream is 8,000, 20 by 20 by 20. Therefore, the size of the coefficients is going to be, again, the same and 20 by 20 by 20. We also have one intercept and one sigma to estimate. So all together this would be 8,002 parameters. So we need to estimate 8,002 parameters, we need to have a very large sample size. And in addition to that we will have the risk of over fitting. So what would be the solution? We’re gonna explore the inherent low-dimensional structure of this high dimensional data as we discuss during this module.</p>
<p>Here is how we will approach this:</p>
<p><strong>Proposition</strong></p>
<p>Let’s suppose we have an image stream or third-order tensor <span class="math inline">\(\{\mathcal{S}_i\}_1^N\)</span><br>
- which can be expanded by <span class="math inline">\(\mathcal{S}_i = \tilde{\mathcal{S}}_i  \times_1 U_1 \times_2 U_2 \times_3 U_3\)</span> - where <span class="math inline">\(\tilde{\mathcal{S}}_i \in \mathbb{R}^{P_1 \times P_2 \times P_3}\)</span> is a low dimensional tensor - and matrices <span class="math inline">\(U_d \in \mathbb{P_d \times I_d}\)</span>, <span class="math inline">\(U_d^T U_d = I_{I_d},\; P_d &lt; I_d,\; d=1,2,3\)</span>.</p>
<p>If the co-efficient tensor, <span class="math inline">\(\mathcal{B}\)</span>,<br>
- is projected onto the tensor subspace spanned by <span class="math inline">\(\{ U_1, U_2, U_3 \}\)</span> ie <span class="math inline">\(\mathcal{B}_i = \tilde{\mathcal{B}}_i \times_1 U_1 \times_2 U_2 \times_3 U_3\)</span> + where <span class="math inline">\(\tilde{\mathcal{B}}_i\)</span> is the projected co-efficient tensor, - then <span class="math inline">\(\langle \mathcal{B} , \mathcal{S}_i \rangle = \langle \tilde{\mathcal{B}} , \tilde{\mathcal{S}}_i \rangle\)</span></p>
<div style="text-align: center;">
<img src="images/M3_017.png" align="center">
</div>
<p>the message of this proposition, it says: for the coefficients that you have, if you are only interested in finding the inner product of these two, which is a scalar. You can define a corresponding, a smaller coefficients sets of coefficients, such that the inner product of these to a smaller box and the bigger boxes are going to be exactly the same. So, therefore we can do a significant dimension reduction. Whatever dimension reduction we can perform on s we can also perform on the coefficients as well.</p>
<p>And instead of estimating a large number of coefficients, we can just estimate a small number of coefficients. For example, as you can see here, in the previous example for the 20 by 20 by 20, we need to estimate 1,000 coefficients. And let’s say the rank of S-matrix is ten, and what we are gonna do, we are gonna estimate 10 x 10 x 10 instead, and from 8,000 we can reduce the dimension to 1,000.</p>
<p>This was all done without losing information, because the inner product in our model is going to be exactly the same. However, there is one more problem, it’s still 1,002 parameters which is a large number to estimate. What we can do, we can have a second level of decomposition or dimension reduction using tensor decomposition that you’ll learn in this module. We can use either CP decomposition or Tucker decomposition. And we’ll see both of them in next few slides.</p>
<p><img src="images/M3_018.png" width="550px" align="left"></p>
<p>Let’s begin with CP decomposition. So we have a small set of tensor coefficients denoted by <span class="math inline">\(\tilde{\mathcal{B}}\)</span>, with dimension of P1, P2 and P3. Now, after the first level of the dimension reduction is 1,000. What we will do: we can say given the rank of this beta tilde, let’s say the rank is R which much is smaller than P1, P2, P3. We can perform CP decomposition and that’s the CPD composition. For example, if the rank is two then instead of estimating 1000, we only need to estimate three sets of 10 by 20, which is 60 parameters, and that’s a significant dimension reduction.</p>
<p>Mathematically speaking, we are interested in finding this inner product, and we want to perform CP decomposition on beta. And the matrix form of it, as you learned before can be represented by KR or Khatri-Rao products, and using a vector of 1s, <span class="math inline">\(1_R\)</span> with dimension of R. And that’s the vectorized version of sigma. So, now we need to estimate these tree matrices, each of these matrices are 10 by 20.</p>
<p><img src="images/M3_019.png" width="650px" align="right"></p>
<p>So the next step is the parameter estimation, with data dimension reduction by still need to estimate the parameters. So one way to estimate that is to use least-square, or we can use maximum likelihood estimation. If we know the likelihood function, for example, if we have a normal likelihood function, if observations follow a normal distribution, then the maximum likelihood estimation and least-square estimation are going to be the same. This is the likelihood function that we want to maximize, where F is the PDF function.</p>
<p>And N is the training sample size. As I said if it follows a normal distribution, we can easily show that minimizing, Maximizing this function, and minimizing, The least-square, Counterpart of it is going to be the same, so it would reduce to, This form. Vector of SI, and that’s the inner product, and then we want to minimize the, norm 2 of this function.</p>
<p>So this minimization, this maximization going to be the same. Okay, so one thing that we need to do, the problem is not convex, with some simple change of variable, we can make the problem convex. For example, instead of maximizing over sigma, we can maximize over sigma T low, which is 1 over sigma.</p>
<p>And we can also define alpha tilde which is alpha or a sigma. With this change of variable this is the new function that we want to maximize. Again, f gonna change depending on the PDF functions that we are interested in, the simplest one would be normal. So now that we have this optimization problem, how we can optimize it.</p>
<p>The method that we use here is block-wise optimization, meaning that we assume that all variables are given except for one block. For example, let’s say <span class="math inline">\(\tilde{B}_1\)</span> is a known and everything else here in this equation are given. Then we try to optimize with respect to this one.</p>
<p><img src="images/M3_020.png" width="650px" align="left"></p>
<p>We then repeat this iteratively for other variables until the algorithm converges here, as you can see this is a forward step algorithm. And we repeat this iterative algorithm onto convergence. In the first system we are maximizing with respect to <span class="math inline">\(\tilde{\alpha},\; \tilde{sigma}\)</span>. As you can see, given all these three, so the green color means that it’s given.</p>
<p>And then given beta B2, B3 sigma and alpha we want to maximize over B1 tilde, and then over B2 and over B3. Then we repeat these processor onto convergence. So it’s similar to the procedures that we saw for solving CPD composition, networking decomposition, block-wise optimization.</p>
<p>Now, again, the question is what would be the rank? So for finding rank similar to CPN and target decomposition that we had before, we can use BIC or AIC. So basically -2 times the likelihood function. Optimize likelihood function, and, N is the basically sample size, and P is the number of parameters that we are estimating.</p>
<p><img src="images/M3_021.png" width="650px" align="right"></p>
<p>As we mentioned earlier, we can use, Tucker instead of CP for the second level dimension reduction. If you use Tucker, then the original dimension of beta tilde is 1,000. We can write it down as some sort of outer product of a core tensor matrix, and the tensor product of some factoring matrix, so with the smaller dimension.</p>
<p>Let’s say, and as you know, Tucker has three ranks. So we have n’th rank in Tucker. Let’s say the rank for the first element is 2, for the second element is 1, and for the third one is 2 again. And the dimension of our core matrix is 2 1 2, therefore the total number of parameters that we are gonna estimate would be 20 + 10 + 20 + 4 which is 54.</p>
<p>That’s just for our number that we need to estimate. If we use Tucker decomposition, then the regression problem is reformulated as this one. This is the core tensor and these are the factorizing matrix with the tensor multiplication. Again, we can use block wise optimization and maximize the likelihood function similar to what we did we for CPD composition.</p>
<section id="example-heat-degradation-v3" class="level3">
<h3 class="anchored" data-anchor-id="example-heat-degradation-v3">Example: Heat Degradation v3</h3>
<p>Now let’s look at the case study. We have the degradation experiment for bearings. And we got us an infrared camera captures, the degradation images. Each image is 40 by 20 pixels and time-to-failure is some number between 15-55 minutes, in this example. So here is the result.</p>
<p><img src="images/M3_BearingDegradation.gif" width="550px" align="left"></p>
<p>So, the x axis here is the time. So over time we observe new observation, new images, and y-axis is absolute prediction error. So how much error do we have from the actual time to failure? And as you can see over time since we observe more images this prediction error decreases.</p>
<p>If you compare CP with Tucker decomposition, if you see p for damage reduction of the coefficient of Tucker, you’ll see that Tucker is less volatile. Overtime it has a overall beta performance, and the reason is because Tucker has more flexibility because it has three ranks, whereas CP we only have one rank. <br clear="both"> <img src="images/M3_022.png" width="550px" align="right"></p>
<p>Here is for all of the test data that we have, summarizing the result, the x axis is the observation percentile. Like how much of, or how many images, or how much of the image extreme has been observed. For example 20% means 20% of this image stream has been observed. And y-axis is absolute mean prediction error in the left figure. In the right figure we have a similar thing, but y-axis absolute mean prediction error for the variance, variance of mean prediction error.</p>
<p>So, we have different methods, Tucker, CPD composition. We also have regular principle component analysis, we have functional principle component analysis, we have B-spline, these are basically you learn in the first module.</p>
<p>But if you look at the performance you will see that the CP decomposition and Tucker decomposition are outperforming other methods, especially Tucker decomposition. Again, Tucker has a better performance because it has more flexibility. But, again, it’s a little difficult to find the rank of Tucker because it’s combinatorics problem we need to, it’s not just one rank, it’s three ranks in this case, and we need to try different combinations.</p>
</section>
</section>
<section id="tensor-applications-pt2" class="level2">
<h2 class="anchored" data-anchor-id="tensor-applications-pt2">Tensor Applications-Pt2</h2>
<p>Let’s continue looking tensor analysis applications.<br>
- Applications of CP and Tucker decomposition in a regression context - Building regression models for tensor response and scalar predictors. + In the previous lesson we saw a scalar response and tensor predictors</p>
<section id="process-optimization" class="level3">
<h3 class="anchored" data-anchor-id="process-optimization">Process Optimization</h3>
<p>The second application that we’re studying in this lesson is on process optimization. More specifically in measuring and turning process. In turning process the speed and cutting depth are two important factors that affect the dimensional accuracy of produced parts. The dimensional accuracy after producing a part can be also measured by some technologies and devices known as coordinate measurement machines, or CMM for short.</p>
<div style="text-align: center;">
<img src="images/M3_023.png" align="center">
</div>
<p>And the outcome of this measurements process is a point cloud as you can see here in this figure. These point club data can be represented by a second order tensor or matrix where each row of the matrix, so if I want to show that’s the matrix X, each row relates to one slice of this point cloud or one layer of the point cloud. And the columns represent the x, y location in a Cartesian coordinate or radius and angle in a polar system. So it could be the location of this point, in the Cartesian that’s x, y, or it is the theta and R which is the radius and angle in a polar system.</p>
<p>Our data is a set of experiments with different cut depth and also speed levels (see below table). For each setting combination, we repeated the experiments or produced ten parts. At tight are the different point clouds of the produced parts. For example as we can see when, cutting depth increases and the speed increases as you can see, it’s not very uniform any more.</p>
<div style="text-align: center;">
<img src="images/M3_024.png" align="center">
</div>
<p>The goal of this study is to build a function, <span class="math inline">\(y = f(depth,speed)\)</span> that explains the relationship between speed and depth as an input, and dimension accuracy as the output. Additionally, we also want to find the optimum setting that results in least deviation from the nominal share. So we want to estimate this f function. Also we want to find the optimal value of x1 and x2 which is depth and speed, such that the deviation of y and the nominal y, This deviation is minimized. So we have a more uniformly produced part.</p>
<p>Here is the mathematical setting.<br>
- We have tensor response: <span class="math inline">\(\mathcal{Y} \in R^{210 \times 64 \times 90}\)</span> + where 90 is the number of samples (9 different combinations repeated 10 times) - With a Scalar input: <span class="math inline">\(\mathbf{X} \in R^{90 \times 2}\)</span> - Our Regression formulation is - <span class="math inline">\(\large \mathcal{y} = \mathcal{A} \times_3 \mathbf{X} + \epsilon\)</span> + <span class="math inline">\(\mathcal{y}\)</span> is the expected response + <span class="math inline">\(\mathbf{X}\)</span> is our scalar input + <span class="math inline">\(\epsilon\)</span> is the noise term - <span class="math inline">\(\mathcal{A}\)</span> is the coefficient matrix - Although we don’t know <span class="math inline">\(\mathcal{A}\)</span> we do know it must be <span class="math inline">\(210 \times 64 \times 2 = 26,880\)</span> - As you may suspect we won’t attack this problem head on. Rather than try to estimate all 26K we will use tucker decomposition first to reduce <span class="math inline">\(\mathcal{A}\)</span> to something more manageble.</p>
<p>As shown in previous sections<br>
- Let <span class="math inline">\(\mathcal{A} = \mathcal{B} \times_1 \mathbf{U}^{(1)} \times_2 \mathbf{U}^{(2)} + \epsilon_A\)</span> - where <span class="math inline">\(\mathbf{U}^{(k)}\)</span> are our orthogonal factor matrices - and <span class="math inline">\(\mathcal{B} \in \mathbb{R}^{P_1 \times P_2 \times p}\)</span> is the core tensor with <span class="math inline">\(P_k &lt; I_k\)</span></p>
<p>So basically what we’re gonna say, we say that the A matrix, which is gonna be multiplied by X, can be represented in a lower dimension using Tucker Decomposition as we’ve seen. That B is going to be a smaller core tensor and U1 and U2 are factorizing matrices.</p>
<div style="text-align: center;">
<img src="images/M3_025.png" align="center">
</div>
<p>Using Tucker, let’s say P1 and P2 which is the rank of the Tucker, both of them are the same and they are equal to 2. And this can give us a huge dimension reduction, why? Because this is going to be 2 by 210; The second one would be (2)(64), and the last one is (2)(2)(2). A small p is dimension of the number of parameters, or the scalars that we have which is speed and depth and that is 2. All together this is around 556. So from 27,000, now the number of parameters to be estimated is down to around 600. Now it’s manageable to do the estimation.</p>
<p>Now we need to estimate is the U1, B, and U2 because the A matrix is unknown.</p>
<p>The next steps are those we’ve seen before<br>
- Given the Tucker decomposition: <span class="math inline">\(\mathcal{A} = \mathcal{B} \times_1 \mathbf{U}^{(1)} \times_2 \mathbf{U}^{(2)}\)</span> - And the Tensor Regression: <span class="math inline">\(\mathbf{Y} = \mathcal{A} \times_3 \mathbf{X} + \epsilon\)</span> - we can formulate our least square optimiztion, objective function, formula as + <span class="math inline">\(\large \hat{\mathcal{B}} = \underset{\mathcal{B}}{argmin} || \mathcal{Y} - \mathcal{B} \times_1 \mathbf{U}^{(1)} \times_2 \mathbf{U}^{(2)} \times_3 \mathbf{X} ||^2\)</span> + All we’ve done here is put the Tucker expression for <span class="math inline">\(\mathcal{A}\)</span> into the tensor regression formula + We want to minimize the tensor norm of the actual value and the estimated value. This part is basically A tensor. And this minimization should be done with respect to beta as well as U1 and U2 which are factorizing matrices.</p>
<p>The next step is to combine the Tucker Decomposition and Tensor Regression into a single closed form solution.<br>
- <span class="math inline">\(\large \hat{\mathcal{B}} = \mathcal{Y} \times_1 (\mathbf{U}^{{(1)}^T} \mathbf{U}^{(1)})^{-1} \mathbf{U}^{{(1)}^T} \times_2 (\mathbf{U}^{{(2)}^T} \mathbf{U}^{(2)})^{-1} \mathbf{U}^{{(2)}^T} \times_3 (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T\)</span> - So this is nice eh? We have a closed form solution we can use to compute the solution for beta - Now we just need to figure out how to compute the tucker decomposition tensors</p>
<p>To compute <span class="math inline">\(\mathbf{U}^{(1)}\)</span> and <span class="math inline">\(\mathbf{U}^{(2)}\)</span> we will take a two step approach</p>
<p><strong>Step 1:</strong><br>
Find the core matrix using Tucker Decomposition<br>
- <span class="math inline">\(\large \{\mathscr{S},\mathbf{U}^{(1)},\mathbf{U}^{(2)}\} = \underset{\mathcal{S,U^1,U^2}}{argmin} || \mathscr{Y} - \mathscr{S} \times_1 \mathbf{U}^{(1)} \times_2 \mathbf{U}^{(2)} ||^2\)</span></p>
<p><strong>Step 2:</strong><br>
Next regress the core tensor on X<br>
- <span class="math inline">\(\large \hat{\mathcal{B}} = \mathscr{S} \times_1 (\mathbf{U}^{{(1)}^T} \mathbf{U}^{(1)})^{-1} \times_2 (\mathbf{U}^{{(2)}^T} \mathbf{U}^{(2)})^{-1} \times_3 (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T\)</span> - Recall that the <span class="math inline">\(\mathbf{U}^{(k)}\)</span> tensors are orthogonal, so <span class="math inline">\((\mathbf{U}^{{(1)}^T} \mathbf{U}^{(1)})\)</span> is simply 1. - so we can eliminate them to simplify our equation - <span class="math inline">\(\large \hat{\mathcal{B}} = \mathscr{S}  \times_3 (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T\)</span></p>
<p>Looking at the last expression we see that our estimate only depends on the core tensor, <span class="math inline">\(\mathscr{S}\)</span> and the design matrix, <span class="math inline">\((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T\)</span>.</p>
<p>In the turning case study that we had, we used that point cloud data, and we learned the optimal bases and the coefficients using the Tucker Decomposition. And as you can see, this is the tensor regression coefficient related to depth, and this is the tensor regression coefficient corresponding to speed.</p>
<p>Recall that we didn’t simply wish to estimate the coefficient. We also wanted to find the optimal settings, for depth and speed, such that the deviation from nominal value is minimized. So we will create another optimization problem and objective function. - $ || {} + _3 - r_t ||^2 $ Such That <span class="math inline">\(\sigma \le \sigma_0,\; 1 \le \mathbf{x} \le \mathbf{u}\)</span> + r is the nominal radius + u is the upper bound on x - the deviation</p>
<p>We want to minimize the deviation from the nominal value. And we want to minimize this objective function with respect to x. And we know the range of x. This is the lower bound and upper bound that shows the range of speed and cutting depth.</p>
<p>So if we solve this optimization problem we can find that the optimal setting happens when the speed is 80, and when the cutting depth is 0.8250. And we can reduce the shape variation by 65% using these settings. And here is the simulated surface if we followed the optimal setting of the speed and depth.As you can see, it’s much more uniform than the samples that we produced.</p>
<div style="text-align: center;">
<img src="images/M3_026.png" align="center">
</div>
<hr>
</section>
</section>
</section>
<section id="m4-optimization-pt1" class="level1">
<h1>M4 Optimization Pt1</h1>
<ul>
<li>Define Optimization</li>
<li>Identify applications of optimization in analytics</li>
<li>Differentiate between different types of optimization problems</li>
<li>Define convex optimization</li>
</ul>
<p><strong>Definition</strong><br>
Optimization is concerned with finding the minimum or maximum of an objective function under constraints. For example</p>
<p><img src="images/M4_001.png" width="450px" align="left"></p>
<p>An objective function: <span class="math inline">\(\underset{x}{argmin} f_0(x)\)</span></p>
<p>Constraints<br>
- <span class="math inline">\(f_i(x) \le 0, \; i=1,...,k\)</span> an inequality constraint - <span class="math inline">\(h_j(x) = 0, \; j=1,...,J\)</span> h functions define the equality constraints</p>
<p>In the figure at left you see the contour lines of an <span class="math inline">\(f_0\)</span> function. The contour lines denote the value of f functions on all points is going to be the same. Those with a smaller radius have a smaller value of f function.</p>
<p><br clear="both"></p>
<p>Many statistical and machine learning models are formulated as optimization problem. Some examples include</p>
<p><strong>Maximum Likelihood Estimation</strong><br>
- in MLE, we want to find an estimator that maximizes the likelihood function, or log likelihood function in this case. - <span class="math inline">\(\large \underset{x}{argmax} = \sum_1^n \log p_x(\xi_i)\)</span></p>
<p><strong>Regularization/Penalization</strong><br>
- in regularization or regularized regression, we want to minimize a loss function, as well as some penalty term attached to this loss function. - <span class="math inline">\(\large min f(x) = \frac{1}{n} \sum_1^n I(x;(\xi_i,y_i)) + \lambda p(x)\)</span></p>
<p><strong>Regularized Logistic Regression</strong><br>
- we want to find this coefficient’s x by minimizing this objective function - <span class="math inline">\(\large f(x) = -\frac{1}{N} \sum_1^N \log(1+\exp(y_i x^T \xi_i )) + \mu ||x||_1\)</span></p>
<p><strong>Support Vector Machine</strong><br>
- where we want to maximize the margin between support vectors and the classified, whether we have a linear or non-linear classify that we have. - <span class="math inline">\(\large \underset{x}{argmin} \frac{1}{2} ||x||^2 + C \sum_1^n max(1-y_i(x^T \xi_i);0)\)</span></p>
<p><strong>Matrix Completion</strong><br>
- <span class="math inline">\(\large \underset{L,R}{min} \sum_{(u,v) \in E} \{ (L_u R_v^T - M_{uv})^2 + \mu_u ||L_u||_F^2 + \mu_v ||L_v||_F^2 \}\)</span></p>
<p><strong>Types Of Optimization</strong></p>
<p>Continuous vs Discrete<br>
- Convex and nonconvex optimization - Combinatorial optimization and mixed integer programming</p>
<p>Deterministic vs Stochastic<br>
- All variables and coefficients are deterministic<br>
- Stochastic programming and Robust optimization deal with stochastic variables</p>
<p>Static Vs Dynamic<br>
- Static does not consider the time element - Dynamic programming, stochastic controls and markov decision processes</p>
<p><strong>In this module specifically, we focus on continuous, convex, deterministic, and static optimization models</strong></p>
<section id="convex-optimization" class="level2">
<h2 class="anchored" data-anchor-id="convex-optimization">Convex Optimization</h2>
<p>Convex optimization, if both objective function and constraints are convex, then we have a convex optimization model. There are well developed, reliable, and efficient algorithms for convex problems. Many of the statistical and machine learning techniques are also convex, that’s why we focus on convex optimization in this module.</p>
<p>General Formulation:<br>
- <span class="math inline">\(\large \underset{x}{minimize} f(x)\)</span> Such that: <span class="math inline">\(f_i(x) \le 0, \mathbf{A}x = b\)</span></p>
<p>Let’s begin with some terminology</p>
<p>Convex Set: is a set of points or line segments between two points which is also in the set<br>
- ie for <span class="math inline">\(\large x_1,x_2 \in C \text{ and } 0 \le \theta \le 1 \implies \theta x_1 + (1-\theta)x_2 \in C\)</span> - basically this means that there can’t be holes or gaps in the convex set. - For any two points in C there must exist a straight line that is also in C</p>
<p>Convex function:<br>
- <span class="math inline">\(f\)</span> is convex if dom(<span class="math inline">\(f\)</span>) is a convex set (dom(<span class="math inline">\(f\)</span>) = domain of <span class="math inline">\(f\)</span> aka feasible set)<br>
- and <span class="math inline">\(\large  f(\theta x_1 + (1-\theta)x_2) \le \theta f(x_1) + (1-\theta)f(x_2)\)</span> - <span class="math inline">\(\forall x_1,x_2 \in dom(f), \text{ and } 0 \le \theta \le 1\)</span></p>
<p><strong>Strictly Convex</strong>:<br>
- <span class="math inline">\(f\)</span> is convex if dom(<span class="math inline">\(f\)</span>) is a convex set - and <span class="math inline">\(\large  f(\theta x_1 + (1-\theta)x_2) \lt \theta f(x_1) + (1-\theta)f(x_2)\)</span> - <span class="math inline">\(\forall x_1,x_2 \in dom(f), \text{ and } 0 \le \theta \le 1\)</span> - notice the strict inequality here</p>
<p>All this of course begs the question: How can we verify that a function <span class="math inline">\(f\)</span> is convex?</p>
<p>For this we will need the following</p>
<ul>
<li><ol type="1">
<li>A differentiable function <span class="math inline">\(f\)</span> in convex IFF <span class="math inline">\(\forall x,y \in C \;\; f(y) \ge f(x) + \nabla f(x)^T (y-x)\)</span></li>
</ol>
<ul>
<li>Note that the term on the right is the taylor expansion which is an approximation</li>
</ul></li>
<li><ol start="2" type="1">
<li>A twice differentiable function <span class="math inline">\(f\)</span> if convex IFF <span class="math inline">\(\nabla^2 f \ge 0\)</span></li>
</ol>
<ul>
<li>Note that the term on the left may also be called the Hessian matrix when working with higher dimensions</li>
<li>if the hessian is positive semi-definite then we can say that the function is convex</li>
</ul></li>
</ul>
<p><strong>Properties of Convexity</strong><br>
- 1) Any locally optimal point of a convex problem is globally optimal<br>
- 2) x is optimal iff <span class="math inline">\(\nabla f(x)^T (y-x) \ge 0\)</span> for all feasible y</p>
<p>So in the next two lessons, we are gonna learn review various methods for solving optimization problems</p>
<p>First Order Methods<br>
- gradient descent, - accelerated gradient descent algorithms, - stochastic gradient descent.</p>
<p>Second Order Methods<br>
- Newton method, - quasi-Newton method, - and BFGS algorithm ( Broyden-Fletcher-Goldfarb-Shanno )</p>
</section>
<section id="first-order-methods" class="level2">
<h2 class="anchored" data-anchor-id="first-order-methods">First-Order Methods</h2>
<p>In this section we focus on optimization algorithms that use the first order derivatives to locate the optimal solution.</p>
<p>Topics/Goals<br>
- Understanding first order methods - Using gradient descent - Using accelerated algorithms - Using stochastic gradient descent</p>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<p>Suppose we have a function <span class="math inline">\(f\)</span> that we wish to minimize. We are given that <span class="math inline">\(f\)</span> is continuous and twice differentiable. In a perfect world where the <span class="math inline">\(f\)</span> equation is known and there exists a closed form for the derivative our lives will be easy. However, this is rarely case and so we will often turn to iterative methods.</p>
<p>One simple approach we may take is to choose a random point <span class="math inline">\(x_0\)</span> and solve for <span class="math inline">\(f\)</span>, then we may choose another point <span class="math inline">\(x_1\)</span> and evaluate <span class="math inline">\(f\)</span> again. If <span class="math inline">\(f\)</span> decreases we keep <span class="math inline">\(x_1\)</span> and try again. ie Choose another point in the direction of <span class="math inline">\(x_0 -&gt; x_1\)</span>, and evaluate <span class="math inline">\(f\)</span>. What we have done here is move in the direction of the negative gradient. This method is known as Gradient descent.<br>
<img src="images/M4_002.png" width="350px" align="left"></p>
<ul>
<li>i.e.&nbsp;<span class="math inline">\(x^{(k+1)} = x^{(k)} - t_k \nabla f(x^{(k)})\)</span>
<ul>
<li>where <span class="math inline">\(x^{(k)}\)</span> is the starting point</li>
<li><span class="math inline">\(t_k\)</span> is our step size</li>
<li>and <span class="math inline">\(\nabla f(x^{(k)})\)</span> is the gradient value</li>
</ul></li>
<li>verbally: the next value is the original value less the step size times the gradient evaluated at the original value</li>
</ul>
<p><br clear="both"></p>
<p><strong>Gradient Descent Algorithm</strong></p>
<pre><code>init x_0 = random number 
     k &lt;- 0

While || gradient(f(x_k)) || &gt;= epsilon
    x_k+1 = x_k - t_k * gradient(f(x_k)) 
    k &lt;- k+1
end while
return x_k
</code></pre>
<p>Let’s look at a simple example.</p>
<p>Consider <span class="math inline">\(f(x) = x^2\)</span>. I’m sure you’re well aware that the min is at 0. Let’s take <span class="math inline">\(t_k = 0.25\)</span>, we begin at <span class="math inline">\(x_0 = 1\)</span> which yields <span class="math inline">\(f = 1\)</span>. Next we use our formula compute <span class="math inline">\(x_1 = (1) - (0.25)(2(1)) = 1 - 0.5 = 0.5\)</span>. For the next iteration we have <span class="math inline">\(x_2 = (0.5) - (0.25)(2(0.5)) = 0.5 - 0.25 = 0.25\)</span>. Feel free to repeate this process to convince yourself it’s converging to 0.</p>
<p>Our example demonstrates an interesting nuance. The choice of step size <span class="math inline">\(t_k\)</span> is very important. If it’s too large then it will jump the min value and convergence will take a long time. If it’s too small again it takes a long time to converge. There are two possible approaches to choosing <span class="math inline">\(t_k\)</span>, both of which try to adapt to the function.</p>
<div style="text-align: center;">
<img src="images/M4_003.png" align="center">
</div>
<p>Let’s take a closer look at the exact line search. To implement this we would insert some amendments to the gradient search algorithm provided above</p>
<pre><code>init x_0 = random number 
     k &lt;- 0
     t &lt;- 1                # so we can update it
     
## While || gradient(f(x_k)) || &gt;= epsilon           # old code
while f(x_k + t_k grad(f(x_k))) &gt; f(x_k)          # new code
    t_i = t_k * c  (c in (0,1))                   # adapt the t
    x_k+1 = x_k - t_i * gradient(f(x_k)) 
    k &lt;- k+1
end while
return x_k
</code></pre>
<p><strong>TODO</strong> confirm the above. It was hacked together</p>
<p>Let’s now turn our attention to Backtracking</p>
<pre><code>init x_0 = random number 
     k &lt;- 1                   # previously we began at 0
     t = 1                    # new var - no longer a constant
While || gradient(f(x_k)) || &gt;= epsilon
    x_k+1 = x_k - t_k * gradient(f(x_k)) 
    k &lt;- k+1
    t &lt;- t/2**(k)        ## new piece
end while
return x_k</code></pre>
<p><strong>Convergence Rate</strong><br>
The convergence rate is how quickly we can get to a given tolerance, where quickness is measured in iterations.<br>
- <span class="math inline">\(f(x^k) - f^* \le \epsilon_k\)</span></p>
<p>For a given precision <span class="math inline">\(\epsilon\)</span>, what is the number of iterations required for <span class="math inline">\(\underset{1 \le t \le k}{min} \epsilon_t &lt; \epsilon\)</span>?</p>
<p>We break the answer into three cases<br>
<span class="math inline">\(\underset{k\to\infty}{\lim} \frac{\epsilon_{k+1}}{\epsilon} =
\begin{cases}
0 &amp; \text{ superlinear }    \epsilon_k = e^{-e^k} \\
\in (0,1) &amp; \text{ linear } \epsilon_k = e^{-k}   \\
1 &amp; \text{ sublinear }      \epsilon_k = 1/k \\
\end{cases}\)</span></p>
<p>As it turns out the convergence rate for a standard GD algorithm is sublinear. However there is an improvement we will look at in the next section</p>
</section>
<section id="accelerated-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="accelerated-gradient-descent">Accelerated Gradient Descent</h3>
<p><strong>Recall-Review</strong><br>
- <span class="math inline">\(f\)</span> is called an L-Lipschitz function over a set S with respect to a norm <span class="math inline">\(||.||\)</span><br>
- if <span class="math inline">\(\forall u,w \in S \text{ we have } |f(u)-f(w)| \le L||u-w||\)</span> - Intuitively, L is a measure of how fast the function can change + ref: https://homes.cs.washington.edu/~marcotcr/blog/lipschitz/</p>
<p>Assume that <span class="math inline">\(f\)</span> is an L-Lipschitz function. A simple version of Nesterov’s Accelerated Gradient Descent algorithm is obtained by iterating the following two equations in each step.<br>
- <span class="math inline">\(\large x^{k+1} = y^k - \frac{1}{k} \nabla f(y^k)\)</span> - <span class="math inline">\(\large y^{k} = x^k + \frac{k-1}{k+2} (x^k - x^{k-1})\)</span></p>
<p>Accelerated Gradient Descent achieves a convergence of <span class="math inline">\(\frac{1}{k^2}\)</span> which is optimal</p>
<ul>
<li><span class="math inline">\(\large O(\frac{1}{\epsilon})\)</span> Gradient descent</li>
<li><span class="math inline">\(\large O(\sqrt{\frac{1}{\epsilon}})\)</span> Accelerated gradient descent</li>
</ul>
</section>
<section id="stochastic-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>A stochastic gradient descent is another first order method which is specifically useful for fitting least Square estimate or maximum likelihood estimate with large datasets, when the sample size is large.</p>
<p>Assume that the function that you want to optimize can be written as sum of n convex functions which are differentiable.<br>
- ie <span class="math inline">\(\large F(x) = \sum_{i=1}^n f_i(x)\)</span>, and <span class="math inline">\(f_i(x)\)</span> are convex and differentiable - For example, to estimate the mean of a population, a natural loss function to be minimized is <span class="math inline">\(F(x) = \sum_{i=1}^n (y_i - x)^2\)</span>. if analytic then we can solve it to see that the optimal value for this is <span class="math inline">\(\bar{y}\)</span>, which is a sample average of y.</p>
<p>Gradient Descent needs to calculate the gradient for all n functions which is quite expensive as n grows large. Stochastic Gradient Descent, however, randomly selects one function (observation) to update the estimate of <span class="math inline">\(F\)</span></p>
<p><strong>Algorithm: Stochastic Gradient Descent</strong></p>
<pre><code>init x_1 

for k = 1 -&gt; K 
    for i = 1 -&gt; n 
        sample an observation i uniformly at random
        update x_{k+1} &lt;- x_k - \alpha \nabla f_i(x_k)

return x_K</code></pre>
<p>Sometimes we used only portion of data or observations to run this stochastic gradient descent. And it showing us it may perform better compare to using all of data. This algorithm is called stochastic gradient descent mini-batch.</p>
<p><strong>Algorithm: Mini-Batch Stochastic Gradient Descent</strong></p>
<pre><code>init x_1 

for k = 1 -&gt; K 
    b &lt;- select random m observation
    for i = 1 -&gt; m 
        sample an observation i uniformly at random
        update x_{k+1} &lt;- x_k - \alpha \nabla f_i(x_k)
    
return x_K</code></pre>
<p><strong>Notice that the inner loop is over the mini-batch. Not the entire sample in the previous example</strong></p>
<p>Comparison for strongly convex functions</p>
<table class="table">
<thead>
<tr class="header">
<th>method</th>
<th style="text-align: right;">iterations</th>
<th style="text-align: right;">cost/iteration</th>
<th style="text-align: right;">total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GD</td>
<td style="text-align: right;">O(log(1/e))</td>
<td style="text-align: right;">O(n)</td>
<td style="text-align: right;">O(n log(1/e))</td>
</tr>
<tr class="even">
<td>SGD</td>
<td style="text-align: right;">O(1/e)</td>
<td style="text-align: right;">O(1)</td>
<td style="text-align: right;">O(1/e)</td>
</tr>
</tbody>
</table>
<p>Let’s say we have a strongly convex function. For a strongly convex function, the number of equations that we need to reach the precision epsilon is log 1/e. Note that if it is just convex function, then we would have 1/e , but for a strongly convex function, it’s log(1/e).</p>
<p>For a stochastic gradient descent, it is 1 over epsilon. So as you can see, gradient descent is better cuz it is log 1 over epsilon, however. Let’s look at the cost per iteration. The cost of computation per iteration is in order of n, because we have n functions.</p>
<p>However for our stochastic gradient descent, we uses only one of them, so the cost is 1. So if I want to calculate the total cost, which is the cost per iteration and the number of iterations that we have. For gradient descent is in order of n times log 1 over epsilon.</p>
<p>And for stochastic gradient descent is in order of 1 over epsilon. So as you can see, when n increases for a small n, gradient remains good. But when n increases at some point, this stochastic gradient descent would be better depending on what would be the desired precision.</p>
<p>20.30m</p>
<p>The mean population estimation as discussed before.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co">############################### %Data generation</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>beta0 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>beta1 <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> beta0<span class="op">+</span>beta1<span class="op">*</span>x<span class="op">+</span>np.random.randn(n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># % Closed form solution</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.hstack([np.ones((n<span class="op">+</span><span class="dv">1</span>, <span class="dv">1</span>)), x[:, <span class="va">None</span>]])</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.linalg.lstsq(X, y)[<span class="dv">0</span>]</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># defining the objective function</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randn(<span class="dv">2</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> J(b): <span class="cf">return</span> ((X<span class="op">@</span>b<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="co">############################### SGD Algorithm</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> J(b)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>fL <span class="op">=</span> [f]</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> time.time()</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> k <span class="op">&lt;=</span> K:</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.random.choice(n, size<span class="op">=</span>n, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> indices:</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">*</span>X[s]<span class="op">*</span>(y[s]<span class="op">-</span>X[s]<span class="op">@</span>b)</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>        b <span class="op">-=</span> <span class="fl">0.001</span><span class="op">*</span>grad</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> J(b)</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    fL.append(f)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> k<span class="op">+</span><span class="dv">1</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> time.time()<span class="op">-</span>t</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The second example is for fitting linear regression.<br>
- <span class="math inline">\(\large y_i = \beta_0 + \beta_1 x_i + \epsilon_i\)</span> for <span class="math inline">\(i = 1,...,n\)</span> - Objective Function: <span class="math inline">\(\large \underset{\beta_0,\beta_1}{min} \sum (y_i, - \beta_0 - \beta_1 x_i)^2\)</span> - Closed form solution: <span class="math inline">\(\large \hat{\beta} = (X^T X)^{-1} X^T y\)</span></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co">############################### %Data generation</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>beta0 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>beta1 <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> beta0<span class="op">+</span>beta1<span class="op">*</span>x<span class="op">+</span>np.random.randn(n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.exp(linear)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(linear))</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.hstack([np.ones((n<span class="op">+</span><span class="dv">1</span>, <span class="dv">1</span>)), x[:, <span class="va">None</span>]])</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randn(<span class="dv">2</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.exp(X[i]<span class="op">@</span>b)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    logistic <span class="op">=</span> np.log(z)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>z)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    s1 <span class="op">=</span> y[i]<span class="op">*</span>logistic</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    s2 <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>y[i])<span class="op">*</span>logistic</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    f <span class="op">+=</span> s1<span class="op">+</span>s2</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>fL <span class="op">=</span> [f]</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> time.time()</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> k <span class="op">&lt;=</span> K:</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.random.choice(n, size<span class="op">=</span>n, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> indices:</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> X[s, :]<span class="op">*</span>(y[s]<span class="op">-</span>np.exp(X[s]<span class="op">@</span>b)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(X[s]<span class="op">@</span>b)))</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        b <span class="op">+=</span> grad</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    fun <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    s1 <span class="op">=</span> y<span class="op">*</span>np.log(np.exp(X<span class="op">@</span>b)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(X<span class="op">@</span>b)))</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>    s2 <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span>np.log(<span class="dv">1</span><span class="op">-</span>np.exp(X<span class="op">@</span>b)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(X<span class="op">@</span>b)))</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> s1.<span class="bu">sum</span>()<span class="op">+</span>s2.<span class="bu">sum</span>()</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>    fL.append(f)</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>    k <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="second-order-methods" class="level2">
<h2 class="anchored" data-anchor-id="second-order-methods">Second Order Methods</h2>
<p>In this section we focus on second-order methods. These build upon the first order methods by including the second derivatives.</p>
<p>Our goals are as follows<br>
- Understand second order methods - Use Newton’s method - Use Gauss-Newton Method - Use Quasi-Newton method: BFGS Algorithm</p>
<p>We begin with Newton’s method</p>
<section id="newtons-method" class="level3">
<h3 class="anchored" data-anchor-id="newtons-method">Newton’s Method</h3>
<p>Let’s assume we have a function <span class="math inline">\(f\)</span> that is continuous and twice differentiable, with a goal to find the <span class="math inline">\(\underset{x}{min} f(x)\)</span>.<br>
We are also given that<br>
- <span class="math inline">\(\nabla f(x) = [ \frac{\partial f(x)}{\partial x} ]\)</span> is the gradient vector - <span class="math inline">\(\nabla^2 f(x)\)</span> is the Hessian matrix ( second-order partial derivatives ) + The first element of this matrix is the secondary derivative with respect to x1. + The second element would be the partial derivative with respect to x1 and x2, and so on and so forth. + The last element in the first row is the second derivative with respect to x1 and xn. - $f(x’) = f(x) + (x’ - x)^T f(x) + (x’ - x)^T ^2 f(x) (x’ - x) $ is the Taylor expansion of <span class="math inline">\(f\)</span> + x’ here is just another point - Not the derivative of x</p>
<p><img src="images/M4_004.png" width="450px" align="left"></p>
<p>To find the root using Newton we solve for <span class="math inline">\(f(x) = 0\)</span> ( this would be a first-order Newton )<br>
- we use: <span class="math inline">\(\large x^{(k)} = x^{(k-1)} - \frac{f(x)}{f'(x)}\)</span></p>
<p>Suppose we wanted to find <span class="math inline">\(f'(x) = 0\)</span>? Then we can simply modify our formula above to get a second-order method.<br>
- we use <span class="math inline">\(\large x^{(k)} = x^{(k-1)} - \frac{f'(x)}{f''(x)}\)</span><br>
- or for n-dim vector valued functions - <span class="math inline">\(\large x^{(k)} = x^{(k-1)} - \nabla f(x) (\nabla^2 f(x))^{-1}\)</span></p>
<p><br clear="both"></p>
<p><strong>Algorithm: Newton’s Method</strong></p>
<pre><code>init x0, f_x, grad(f_x), 2nd_grad(f_x)
     a = 1                                # step_size
     l = 10^-1                            # damping factor - makes the parabola around x steeper
     e = ?                                # Tolerance
     
                          # ie max(Omega) &lt; e
repeat until convergence (||Omega||_infty &lt; e )  
    solve ( 2nd_grad(f_x) + l * I ) * Omega = -1*grad(f_x) for Omega
                              # I is just the identity matrix - otherwise the math doesn't work 
    k = k+1
    x_k = x_k-1 + a * Omega
    
    while (f(x_k) &gt; f(x_k-1) )              # serves to find/apply the best step size
        a = 0.1*a
        x_k = x_k-1 + a * Omega
    end
    a = a^0.5
end

ret x_k</code></pre>
<p>Let’s take another look at our example from earlier<br>
- <span class="math inline">\(\large min f(x) = 4 x_1^2 + 2 x_2^2 - 4 x_1 x_2\)</span> - With first partials + <span class="math inline">\(\nabla f(x) = \begin{bmatrix} 8 x_1 - 4 x_2 \\ 4 x_2 - 4 x_1 \end{bmatrix}\)</span> - and second partials + <span class="math inline">\(\nabla^2 f(x) = \begin{bmatrix} 8 &amp; - 4 \\ -4 &amp; 4 \end{bmatrix}\)</span></p>
<p>Using Newton this should yield an optimal value <span class="math inline">\(x^* = 1.0e-09\)</span> at point (0.0421,0.1180) in under 3 iterations. Compare this example to the first-order method, gradient descent. Remember that gradient descent for solving the same problem, used 224 iterations to solve the problem, However, here with only two iterations, we can find the optimal solution. The main difference here is using information of the second derivative, which help us reach the optimal point very quickly. But the caveat is in many cases, finding the second degree iteration is not going to be that easy.</p>
</section>
<section id="gauss-newton-method" class="level3">
<h3 class="anchored" data-anchor-id="gauss-newton-method">Gauss-Newton Method</h3>
<p>When the function <span class="math inline">\(f\)</span> is in quadratic form then the Gauss-Newton method may be used to approximate the Newton method. It is particularly useful because it doesn’t require knowing the hessian which is sometimes too difficult to compute.</p>
<p>Assume <span class="math inline">\(f(x)\)</span> is a continuous function<br>
- <span class="math inline">\(\large f(x) = g(x)^T g(x)\)</span> (this comes from the assumption of being quadratic) - where <span class="math inline">\(\large g(x) = [g_1(x) \cdots g_d(x)]^T \in \mathbb{R}^d\)</span></p>
<p>We define the Jacobian Matrix as (this is the first-order partial of <span class="math inline">\(f\)</span> for each variable )<br>
<span class="math inline">\(\large \mathbf{J}_{g(x)} = \begin{bmatrix}  \frac{\partial g_1(x)}{\partial x_1} &amp; \cdots &amp; \frac{\partial g_1(x)}{\partial x_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial g_d(x)}{\partial x_1} &amp; \cdots &amp; \frac{\partial g_d(x)}{\partial x_n} \end{bmatrix}\)</span></p>
<p>Using the Jacobian we can then compute the gradient &amp; Hessian as<br>
- <span class="math inline">\(\large \nabla f(x) = 2 \mathbf{J}_{g(x)}^T g(x)\)</span> - <span class="math inline">\(\large \nabla f(x) = 2 \mathbf{J}_{g(x)}^T \mathbf{J}_{g(x)} + 2 \mathbf{J}_{g(x)}^T \nabla^2 g(x)\)</span></p>
<p>When <span class="math inline">\(\nabla^2 g(x) \approx 0\)</span> we can approximate the hessian using just the first term<br>
- <span class="math inline">\(\large \nabla f(x) = 2 \mathbf{J}_{g(x)}^T \mathbf{J}_{g(x)}\)</span></p>
<p>Putting all this into Newton’s method<br>
- <span class="math inline">\(\large x^{(k)} = x^{(k-1)} - (\mathbf{J}_{g(x)}^T \mathbf{J}_{g(x)})^{-1} \mathbf{J}_{g(x)}^T g(x)\)</span> - instead of using the Hessian, now we are using Jacobian</p>
<p>The algorithm for all intents and purposes is the same as before, except we must solve for this new equation instead</p>
<pre><code># Original Newton Method
solve ( 2nd_grad(f_x) + l * I ) * Omega = -1*grad(f_x) for Omega

# new version
solve ( J_g^T J_g + l * I) * Omega = -J_g^T            for Omega</code></pre>
<p>Let’s look at an example. Gauss-Newton is often used for nonlinear functions so we look at an exponential regression example<br>
- Objective function: <span class="math inline">\(\min f(\alpha,\beta) = \sum_{i=1}^N (y_i - \alpha e^{\beta x_i})^2\)</span> - <span class="math inline">\(g(\alpha,\beta) = [y - \alpha e^{\beta x_i})]_{n \times 1}\)</span> - $J_{g(,)} = [-e^{x} - x e^{x})]_{n } $</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate some data</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span><span class="op">;</span> truealpha <span class="op">=</span> <span class="dv">5</span><span class="op">;</span> truebeta <span class="op">=</span> <span class="fl">0.5</span><span class="op">;</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> truealpha<span class="op">*</span>np.exp(truebeta<span class="op">*</span>x)<span class="op">+</span>np.random.randn(n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># %Gauss-Newton Method</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.random.randn(<span class="dv">2</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>gfun <span class="op">=</span> <span class="kw">lambda</span> theta: y<span class="op">-</span>theta[<span class="dv">0</span>]<span class="op">*</span>np.exp(theta[<span class="dv">1</span>]<span class="op">*</span>x)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>g0 <span class="op">=</span> gfun(theta)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>Jfun <span class="op">=</span> <span class="kw">lambda</span> theta: np.hstack([<span class="op">-</span>np.exp(theta[<span class="dv">1</span>]<span class="op">*</span>x)[:,<span class="va">None</span>], <span class="op">-</span>(theta[<span class="dv">0</span>]<span class="op">*</span>x<span class="op">*</span>np.exp(theta[<span class="dv">1</span>]<span class="op">*</span>x))[:,<span class="va">None</span>]])</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>J0 <span class="op">=</span> Jfun(theta)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>ffun <span class="op">=</span> <span class="kw">lambda</span> theta: ((y<span class="op">-</span>theta[<span class="dv">0</span>]<span class="op">*</span>np.exp(theta[<span class="dv">1</span>]<span class="op">*</span>x))<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>f0 <span class="op">=</span> ffun(theta)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>fL <span class="op">=</span> [f0]</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>lam  <span class="op">=</span> <span class="fl">1e-10</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>Omega <span class="op">=</span> <span class="op">-</span>np.linalg.solve( J0.T<span class="op">@</span>J0 <span class="op">+</span> lam<span class="op">*</span>np.eye(<span class="dv">2</span>), J0.T<span class="op">@</span>g0)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>tol <span class="op">=</span> <span class="fl">1e-10</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> Omega.<span class="bu">max</span>() <span class="op">&gt;</span> tol: <span class="co"># should probabaly use np.abs(Omega).max()?</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    theta_ <span class="op">=</span> theta <span class="op">+</span> alpha<span class="op">*</span>Omega</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> ffun(theta_)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> f <span class="op">&gt;</span> f0:</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> <span class="fl">0.1</span><span class="op">*</span>alpha<span class="op">;</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        theta_ <span class="op">=</span> theta <span class="op">+</span> alpha<span class="op">*</span>Omega</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        f <span class="op">=</span> ffun(theta_)        </span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> alpha<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_.copy()</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    g0 <span class="op">=</span> gfun(theta)</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    J0 <span class="op">=</span> Jfun(theta)</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    f0 <span class="op">=</span> f.copy()</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>    fL.append(f0)</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    Omega <span class="op">=</span> <span class="op">-</span>np.linalg.solve( J0.T<span class="op">@</span>J0 <span class="op">+</span> lam<span class="op">*</span>np.eye(<span class="dv">2</span>), J0.T<span class="op">@</span>g0)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> k<span class="op">+</span><span class="dv">1</span><span class="op">;</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="quasi-newton-method-bfgs" class="level3">
<h3 class="anchored" data-anchor-id="quasi-newton-method-bfgs">Quasi-Newton Method (BFGS)</h3>
<p>The last second order method that we’ll learn here is Quasi-Newton method, Quasi-Newton methods are used where we can not analytically calculate the Hessian matrix. So what we do, we numerically approximate the Hessian matrix based on the data from previous iterations. Specifically previous iterations data and gradients which are given here. For 1D case, it’s easier to understand for 1D case, if we use iterations 1 and 2 data, we can show that the Hessian can be obtained by this equation.</p>
<div style="text-align: center;">
<img src="images/M4_005.png" align="center">
</div>
<p>And for nD case, this is the equation for Hessian, and this is the equation for inverse of Hessian. So as you can see for calculating the Hessian, we’re using the gradients of previous iterations, as well as the previous points.</p>
<div style="text-align: center;">
<img src="images/M4_006.png" align="center">
</div>
<p>Here’s an example of BFS in action - <span class="math inline">\(\large min f(x_1,x_2) = e^{x_1-1} + e^{-x_2+1} + (x_1 - x_2)^2\)</span> - <span class="math inline">\(\large \nabla f(x_1,x_2) = \begin{bmatrix} e^{x_1-1} + 2(x_1 - x_2) \\ -e^{-x_2+1} - 2(x_1 - x_2) \end{bmatrix}\)</span></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data Generation</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> np.random.randn(<span class="dv">2</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>ffun <span class="op">=</span> <span class="kw">lambda</span> x: np.exp(x[<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span>)<span class="op">+</span>np.exp(<span class="op">-</span>x[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>)<span class="op">+</span>(x[<span class="dv">0</span>]<span class="op">-</span>x[<span class="dv">1</span>])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>f0 <span class="op">=</span> ffun(x0)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Algorithm</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>ffun_vec <span class="op">=</span> <span class="kw">lambda</span> x: np.exp(x[:,<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span>)<span class="op">+</span>np.exp(<span class="op">-</span>x[:,<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>)<span class="op">+</span> (x[:,<span class="dv">0</span>]<span class="op">-</span>x[:,<span class="dv">1</span>])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>fL <span class="op">=</span> [f0]</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>gfun <span class="op">=</span> <span class="kw">lambda</span> x: np.array([np.exp(x[<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span>)<span class="op">+</span><span class="dv">2</span><span class="op">*</span>(x[<span class="dv">0</span>]<span class="op">-</span>x[<span class="dv">1</span>]), <span class="op">-</span>np.exp(<span class="op">-</span>x[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>)<span class="op">-</span><span class="dv">2</span><span class="op">*</span>(x[<span class="dv">0</span>]<span class="op">-</span>x[<span class="dv">1</span>])])</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>g0 <span class="op">=</span> gfun(x0)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>Hinv <span class="op">=</span> np.eye(<span class="dv">2</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>Omega <span class="op">=</span> (<span class="op">-</span>Hinv<span class="op">@</span>g0)[<span class="va">None</span>,:]</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>tol <span class="op">=</span><span class="fl">1e-10</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> np.<span class="bu">abs</span>(Omega).<span class="bu">max</span>() <span class="op">&gt;</span> tol: <span class="co"># probably meant np.abs(Omega).max() &gt; tol?</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    alphas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1001</span>)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    newXs <span class="op">=</span> x0<span class="op">+</span>alphas[:,<span class="va">None</span>]<span class="op">*</span>Omega</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    lineSearchVals <span class="op">=</span> ffun_vec(newXs)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.argmin(lineSearchVals)</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    Omega <span class="op">=</span> alphas[ind]<span class="op">*</span>Omega</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> (x0<span class="op">+</span>Omega).ravel()</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    g1 <span class="op">=</span> gfun(x)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> (g1<span class="op">-</span>g0)[:,<span class="va">None</span>]</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>    Hinv <span class="op">=</span> (np.eye(<span class="dv">2</span>)<span class="op">-</span>(y<span class="op">@</span>Omega)<span class="op">/</span>(Omega<span class="op">@</span>y)).T<span class="op">@</span>Hinv<span class="op">@</span>(np.eye(<span class="dv">2</span>)<span class="op">-</span>(y<span class="op">@</span>Omega)<span class="op">/</span>(Omega<span class="op">@</span>y))<span class="op">+</span>(Omega.T<span class="op">@</span>Omega)<span class="op">/</span>(Omega<span class="op">@</span>y)</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    x0 <span class="op">=</span> x<span class="op">;</span></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    f0 <span class="op">=</span> ffun(x0)</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>    fL.append(f0)</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>    g0 <span class="op">=</span> gfun(x0)</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>    Omega <span class="op">=</span> (<span class="op">-</span>Hinv<span class="op">@</span>g0)[<span class="va">None</span>,:]</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> k<span class="op">+</span><span class="dv">1</span><span class="op">;</span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<img src="images/M1_006.png" width="450px" align="right"><br clear="both"> <span class="math inline">\(\large \underset{c_,c_1}{\min} \sum_{i=1}^n \sum_{j=1}^{m_i} W(\frac{t_{ij} - t}{h}) \left{ s_i(t_{ij}) - c_0 - (t - t_{ij})c_1 \right}^2\)</span> <img src="images/M2_001.png" width="350px" align="left" padding="3px;" margin="3px;">
<div style="text-align: center;">
<img src="images/M3_003.png" align="center">
</div>
<p><span class="math inline">\(\color{red}{\text{Algorithm}}\)</span></p>
</section>
</section>
</section>
<section id="misc" class="level1">
<h1>Misc</h1>
<p><span class="math inline">\(\hat{\beta} = (X^T X)^{-1} X^T y\)</span></p>
<p><span class="math inline">\(\large  d_k = \frac{ (x-e_k)_+^3 - (x-e_K)_+^3  }{ e_K - e_k}\)</span> for k = 1,2,…,K-2</p>
<p><span class="math inline">\(\hat{\beta} = (X^T X)^{-1} X^T y\)</span></p>
<div id="44b1ded1-d65b-4197-abba-b1bb66b3ac03" class="cell" data-tags="[]" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>img src<span class="op">=</span><span class="st">"images/M1_003.png"</span> width<span class="op">=</span><span class="st">"550px"</span> align<span class="op">=</span><span class="st">"right"</span><span class="op">/&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>SyntaxError: invalid syntax (3639231075.py, line 1)</code></pre>
</div>
</div>
<div id="42701c9d-50c3-432f-aba4-e3937c0ea1fe" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&lt;img src="images/M1_001.png" align="left" /&gt;</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&lt;br clear="both"&gt;</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">#$\hat{\beta} = (X^T X)^{-1} X^T y$</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co">#\begin{array}{lr}</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co">#        ||R_{k}-R_{i}||^{2}, &amp; \text{if } i \neq k\\</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co">#        ||\triangle_{i}||^{2}, &amp; \text{if } i\leq k</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co">#    \end{array}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>SyntaxError: invalid syntax (2575403236.py, line 1)</code></pre>
</div>
</div>
<div id="1f3a3adf-17c0-454e-abdc-c8785ab4b6f6" class="cell" data-tags="[]" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Warning: Cannot change to a different GUI toolkit: widget. Using notebook instead.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/javascript">
/* Put everything inside the global mpl namespace */
/* global mpl */
window.mpl = {};

mpl.get_websocket_type = function () {
    if (typeof WebSocket !== 'undefined') {
        return WebSocket;
    } else if (typeof MozWebSocket !== 'undefined') {
        return MozWebSocket;
    } else {
        alert(
            'Your browser does not have WebSocket support. ' +
                'Please try Chrome, Safari or Firefox ≥ 6. ' +
                'Firefox 4 and 5 are also supported but you ' +
                'have to enable WebSockets in about:config.'
        );
    }
};

mpl.figure = function (figure_id, websocket, ondownload, parent_element) {
    this.id = figure_id;

    this.ws = websocket;

    this.supports_binary = this.ws.binaryType !== undefined;

    if (!this.supports_binary) {
        var warnings = document.getElementById('mpl-warnings');
        if (warnings) {
            warnings.style.display = 'block';
            warnings.textContent =
                'This browser does not support binary websocket messages. ' +
                'Performance may be slow.';
        }
    }

    this.imageObj = new Image();

    this.context = undefined;
    this.message = undefined;
    this.canvas = undefined;
    this.rubberband_canvas = undefined;
    this.rubberband_context = undefined;
    this.format_dropdown = undefined;

    this.image_mode = 'full';

    this.root = document.createElement('div');
    this.root.setAttribute('style', 'display: inline-block');
    this._root_extra_style(this.root);

    parent_element.appendChild(this.root);

    this._init_header(this);
    this._init_canvas(this);
    this._init_toolbar(this);

    var fig = this;

    this.waiting = false;

    this.ws.onopen = function () {
        fig.send_message('supports_binary', { value: fig.supports_binary });
        fig.send_message('send_image_mode', {});
        if (fig.ratio !== 1) {
            fig.send_message('set_device_pixel_ratio', {
                device_pixel_ratio: fig.ratio,
            });
        }
        fig.send_message('refresh', {});
    };

    this.imageObj.onload = function () {
        if (fig.image_mode === 'full') {
            // Full images could contain transparency (where diff images
            // almost always do), so we need to clear the canvas so that
            // there is no ghosting.
            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);
        }
        fig.context.drawImage(fig.imageObj, 0, 0);
    };

    this.imageObj.onunload = function () {
        fig.ws.close();
    };

    this.ws.onmessage = this._make_on_message_function(this);

    this.ondownload = ondownload;
};

mpl.figure.prototype._init_header = function () {
    var titlebar = document.createElement('div');
    titlebar.classList =
        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';
    var titletext = document.createElement('div');
    titletext.classList = 'ui-dialog-title';
    titletext.setAttribute(
        'style',
        'width: 100%; text-align: center; padding: 3px;'
    );
    titlebar.appendChild(titletext);
    this.root.appendChild(titlebar);
    this.header = titletext;
};

mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};

mpl.figure.prototype._root_extra_style = function (_canvas_div) {};

mpl.figure.prototype._init_canvas = function () {
    var fig = this;

    var canvas_div = (this.canvas_div = document.createElement('div'));
    canvas_div.setAttribute('tabindex', '0');
    canvas_div.setAttribute(
        'style',
        'border: 1px solid #ddd;' +
            'box-sizing: content-box;' +
            'clear: both;' +
            'min-height: 1px;' +
            'min-width: 1px;' +
            'outline: 0;' +
            'overflow: hidden;' +
            'position: relative;' +
            'resize: both;' +
            'z-index: 2;'
    );

    function on_keyboard_event_closure(name) {
        return function (event) {
            return fig.key_event(event, name);
        };
    }

    canvas_div.addEventListener(
        'keydown',
        on_keyboard_event_closure('key_press')
    );
    canvas_div.addEventListener(
        'keyup',
        on_keyboard_event_closure('key_release')
    );

    this._canvas_extra_style(canvas_div);
    this.root.appendChild(canvas_div);

    var canvas = (this.canvas = document.createElement('canvas'));
    canvas.classList.add('mpl-canvas');
    canvas.setAttribute(
        'style',
        'box-sizing: content-box;' +
            'pointer-events: none;' +
            'position: relative;' +
            'z-index: 0;'
    );

    this.context = canvas.getContext('2d');

    var backingStore =
        this.context.backingStorePixelRatio ||
        this.context.webkitBackingStorePixelRatio ||
        this.context.mozBackingStorePixelRatio ||
        this.context.msBackingStorePixelRatio ||
        this.context.oBackingStorePixelRatio ||
        this.context.backingStorePixelRatio ||
        1;

    this.ratio = (window.devicePixelRatio || 1) / backingStore;

    var rubberband_canvas = (this.rubberband_canvas = document.createElement(
        'canvas'
    ));
    rubberband_canvas.setAttribute(
        'style',
        'box-sizing: content-box;' +
            'left: 0;' +
            'pointer-events: none;' +
            'position: absolute;' +
            'top: 0;' +
            'z-index: 1;'
    );

    // Apply a ponyfill if ResizeObserver is not implemented by browser.
    if (this.ResizeObserver === undefined) {
        if (window.ResizeObserver !== undefined) {
            this.ResizeObserver = window.ResizeObserver;
        } else {
            var obs = _JSXTOOLS_RESIZE_OBSERVER({});
            this.ResizeObserver = obs.ResizeObserver;
        }
    }

    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {
        var nentries = entries.length;
        for (var i = 0; i < nentries; i++) {
            var entry = entries[i];
            var width, height;
            if (entry.contentBoxSize) {
                if (entry.contentBoxSize instanceof Array) {
                    // Chrome 84 implements new version of spec.
                    width = entry.contentBoxSize[0].inlineSize;
                    height = entry.contentBoxSize[0].blockSize;
                } else {
                    // Firefox implements old version of spec.
                    width = entry.contentBoxSize.inlineSize;
                    height = entry.contentBoxSize.blockSize;
                }
            } else {
                // Chrome <84 implements even older version of spec.
                width = entry.contentRect.width;
                height = entry.contentRect.height;
            }

            // Keep the size of the canvas and rubber band canvas in sync with
            // the canvas container.
            if (entry.devicePixelContentBoxSize) {
                // Chrome 84 implements new version of spec.
                canvas.setAttribute(
                    'width',
                    entry.devicePixelContentBoxSize[0].inlineSize
                );
                canvas.setAttribute(
                    'height',
                    entry.devicePixelContentBoxSize[0].blockSize
                );
            } else {
                canvas.setAttribute('width', width * fig.ratio);
                canvas.setAttribute('height', height * fig.ratio);
            }
            /* This rescales the canvas back to display pixels, so that it
             * appears correct on HiDPI screens. */
            canvas.style.width = width + 'px';
            canvas.style.height = height + 'px';

            rubberband_canvas.setAttribute('width', width);
            rubberband_canvas.setAttribute('height', height);

            // And update the size in Python. We ignore the initial 0/0 size
            // that occurs as the element is placed into the DOM, which should
            // otherwise not happen due to the minimum size styling.
            if (fig.ws.readyState == 1 && width != 0 && height != 0) {
                fig.request_resize(width, height);
            }
        }
    });
    this.resizeObserverInstance.observe(canvas_div);

    function on_mouse_event_closure(name) {
        /* User Agent sniffing is bad, but WebKit is busted:
         * https://bugs.webkit.org/show_bug.cgi?id=144526
         * https://bugs.webkit.org/show_bug.cgi?id=181818
         * The worst that happens here is that they get an extra browser
         * selection when dragging, if this check fails to catch them.
         */
        var UA = navigator.userAgent;
        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);
        if(isWebKit) {
            return function (event) {
                /* This prevents the web browser from automatically changing to
                 * the text insertion cursor when the button is pressed. We
                 * want to control all of the cursor setting manually through
                 * the 'cursor' event from matplotlib */
                event.preventDefault()
                return fig.mouse_event(event, name);
            };
        } else {
            return function (event) {
                return fig.mouse_event(event, name);
            };
        }
    }

    canvas_div.addEventListener(
        'mousedown',
        on_mouse_event_closure('button_press')
    );
    canvas_div.addEventListener(
        'mouseup',
        on_mouse_event_closure('button_release')
    );
    canvas_div.addEventListener(
        'dblclick',
        on_mouse_event_closure('dblclick')
    );
    // Throttle sequential mouse events to 1 every 20ms.
    canvas_div.addEventListener(
        'mousemove',
        on_mouse_event_closure('motion_notify')
    );

    canvas_div.addEventListener(
        'mouseenter',
        on_mouse_event_closure('figure_enter')
    );
    canvas_div.addEventListener(
        'mouseleave',
        on_mouse_event_closure('figure_leave')
    );

    canvas_div.addEventListener('wheel', function (event) {
        if (event.deltaY < 0) {
            event.step = 1;
        } else {
            event.step = -1;
        }
        on_mouse_event_closure('scroll')(event);
    });

    canvas_div.appendChild(canvas);
    canvas_div.appendChild(rubberband_canvas);

    this.rubberband_context = rubberband_canvas.getContext('2d');
    this.rubberband_context.strokeStyle = '#000000';

    this._resize_canvas = function (width, height, forward) {
        if (forward) {
            canvas_div.style.width = width + 'px';
            canvas_div.style.height = height + 'px';
        }
    };

    // Disable right mouse context menu.
    canvas_div.addEventListener('contextmenu', function (_e) {
        event.preventDefault();
        return false;
    });

    function set_focus() {
        canvas.focus();
        canvas_div.focus();
    }

    window.setTimeout(set_focus, 100);
};

mpl.figure.prototype._init_toolbar = function () {
    var fig = this;

    var toolbar = document.createElement('div');
    toolbar.classList = 'mpl-toolbar';
    this.root.appendChild(toolbar);

    function on_click_closure(name) {
        return function (_event) {
            return fig.toolbar_button_onclick(name);
        };
    }

    function on_mouseover_closure(tooltip) {
        return function (event) {
            if (!event.currentTarget.disabled) {
                return fig.toolbar_button_onmouseover(tooltip);
            }
        };
    }

    fig.buttons = {};
    var buttonGroup = document.createElement('div');
    buttonGroup.classList = 'mpl-button-group';
    for (var toolbar_ind in mpl.toolbar_items) {
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) {
            /* Instead of a spacer, we start a new button group. */
            if (buttonGroup.hasChildNodes()) {
                toolbar.appendChild(buttonGroup);
            }
            buttonGroup = document.createElement('div');
            buttonGroup.classList = 'mpl-button-group';
            continue;
        }

        var button = (fig.buttons[name] = document.createElement('button'));
        button.classList = 'mpl-widget';
        button.setAttribute('role', 'button');
        button.setAttribute('aria-disabled', 'false');
        button.addEventListener('click', on_click_closure(method_name));
        button.addEventListener('mouseover', on_mouseover_closure(tooltip));

        var icon_img = document.createElement('img');
        icon_img.src = '_images/' + image + '.png';
        icon_img.srcset = '_images/' + image + '_large.png 2x';
        icon_img.alt = tooltip;
        button.appendChild(icon_img);

        buttonGroup.appendChild(button);
    }

    if (buttonGroup.hasChildNodes()) {
        toolbar.appendChild(buttonGroup);
    }

    var fmt_picker = document.createElement('select');
    fmt_picker.classList = 'mpl-widget';
    toolbar.appendChild(fmt_picker);
    this.format_dropdown = fmt_picker;

    for (var ind in mpl.extensions) {
        var fmt = mpl.extensions[ind];
        var option = document.createElement('option');
        option.selected = fmt === mpl.default_extension;
        option.innerHTML = fmt;
        fmt_picker.appendChild(option);
    }

    var status_bar = document.createElement('span');
    status_bar.classList = 'mpl-message';
    toolbar.appendChild(status_bar);
    this.message = status_bar;
};

mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {
    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,
    // which will in turn request a refresh of the image.
    this.send_message('resize', { width: x_pixels, height: y_pixels });
};

mpl.figure.prototype.send_message = function (type, properties) {
    properties['type'] = type;
    properties['figure_id'] = this.id;
    this.ws.send(JSON.stringify(properties));
};

mpl.figure.prototype.send_draw_message = function () {
    if (!this.waiting) {
        this.waiting = true;
        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));
    }
};

mpl.figure.prototype.handle_save = function (fig, _msg) {
    var format_dropdown = fig.format_dropdown;
    var format = format_dropdown.options[format_dropdown.selectedIndex].value;
    fig.ondownload(fig, format);
};

mpl.figure.prototype.handle_resize = function (fig, msg) {
    var size = msg['size'];
    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {
        fig._resize_canvas(size[0], size[1], msg['forward']);
        fig.send_message('refresh', {});
    }
};

mpl.figure.prototype.handle_rubberband = function (fig, msg) {
    var x0 = msg['x0'] / fig.ratio;
    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;
    var x1 = msg['x1'] / fig.ratio;
    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;
    x0 = Math.floor(x0) + 0.5;
    y0 = Math.floor(y0) + 0.5;
    x1 = Math.floor(x1) + 0.5;
    y1 = Math.floor(y1) + 0.5;
    var min_x = Math.min(x0, x1);
    var min_y = Math.min(y0, y1);
    var width = Math.abs(x1 - x0);
    var height = Math.abs(y1 - y0);

    fig.rubberband_context.clearRect(
        0,
        0,
        fig.canvas.width / fig.ratio,
        fig.canvas.height / fig.ratio
    );

    fig.rubberband_context.strokeRect(min_x, min_y, width, height);
};

mpl.figure.prototype.handle_figure_label = function (fig, msg) {
    // Updates the figure title.
    fig.header.textContent = msg['label'];
};

mpl.figure.prototype.handle_cursor = function (fig, msg) {
    fig.canvas_div.style.cursor = msg['cursor'];
};

mpl.figure.prototype.handle_message = function (fig, msg) {
    fig.message.textContent = msg['message'];
};

mpl.figure.prototype.handle_draw = function (fig, _msg) {
    // Request the server to send over a new figure.
    fig.send_draw_message();
};

mpl.figure.prototype.handle_image_mode = function (fig, msg) {
    fig.image_mode = msg['mode'];
};

mpl.figure.prototype.handle_history_buttons = function (fig, msg) {
    for (var key in msg) {
        if (!(key in fig.buttons)) {
            continue;
        }
        fig.buttons[key].disabled = !msg[key];
        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);
    }
};

mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {
    if (msg['mode'] === 'PAN') {
        fig.buttons['Pan'].classList.add('active');
        fig.buttons['Zoom'].classList.remove('active');
    } else if (msg['mode'] === 'ZOOM') {
        fig.buttons['Pan'].classList.remove('active');
        fig.buttons['Zoom'].classList.add('active');
    } else {
        fig.buttons['Pan'].classList.remove('active');
        fig.buttons['Zoom'].classList.remove('active');
    }
};

mpl.figure.prototype.updated_canvas_event = function () {
    // Called whenever the canvas gets updated.
    this.send_message('ack', {});
};

// A function to construct a web socket function for onmessage handling.
// Called in the figure constructor.
mpl.figure.prototype._make_on_message_function = function (fig) {
    return function socket_on_message(evt) {
        if (evt.data instanceof Blob) {
            var img = evt.data;
            if (img.type !== 'image/png') {
                /* FIXME: We get "Resource interpreted as Image but
                 * transferred with MIME type text/plain:" errors on
                 * Chrome.  But how to set the MIME type?  It doesn't seem
                 * to be part of the websocket stream */
                img.type = 'image/png';
            }

            /* Free the memory for the previous frames */
            if (fig.imageObj.src) {
                (window.URL || window.webkitURL).revokeObjectURL(
                    fig.imageObj.src
                );
            }

            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(
                img
            );
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        } else if (
            typeof evt.data === 'string' &&
            evt.data.slice(0, 21) === 'data:image/png;base64'
        ) {
            fig.imageObj.src = evt.data;
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }

        var msg = JSON.parse(evt.data);
        var msg_type = msg['type'];

        // Call the  "handle_{type}" callback, which takes
        // the figure and JSON message as its only arguments.
        try {
            var callback = fig['handle_' + msg_type];
        } catch (e) {
            console.log(
                "No handler for the '" + msg_type + "' message type: ",
                msg
            );
            return;
        }

        if (callback) {
            try {
                // console.log("Handling '" + msg_type + "' message: ", msg);
                callback(fig, msg);
            } catch (e) {
                console.log(
                    "Exception inside the 'handler_" + msg_type + "' callback:",
                    e,
                    e.stack,
                    msg
                );
            }
        }
    };
};

function getModifiers(event) {
    var mods = [];
    if (event.ctrlKey) {
        mods.push('ctrl');
    }
    if (event.altKey) {
        mods.push('alt');
    }
    if (event.shiftKey) {
        mods.push('shift');
    }
    if (event.metaKey) {
        mods.push('meta');
    }
    return mods;
}

/*
 * return a copy of an object with only non-object keys
 * we need this to avoid circular references
 * https://stackoverflow.com/a/24161582/3208463
 */
function simpleKeys(original) {
    return Object.keys(original).reduce(function (obj, key) {
        if (typeof original[key] !== 'object') {
            obj[key] = original[key];
        }
        return obj;
    }, {});
}

mpl.figure.prototype.mouse_event = function (event, name) {
    if (name === 'button_press') {
        this.canvas.focus();
        this.canvas_div.focus();
    }

    // from https://stackoverflow.com/q/1114465
    var boundingRect = this.canvas.getBoundingClientRect();
    var x = (event.clientX - boundingRect.left) * this.ratio;
    var y = (event.clientY - boundingRect.top) * this.ratio;

    this.send_message(name, {
        x: x,
        y: y,
        button: event.button,
        step: event.step,
        modifiers: getModifiers(event),
        guiEvent: simpleKeys(event),
    });

    return false;
};

mpl.figure.prototype._key_event_extra = function (_event, _name) {
    // Handle any extra behaviour associated with a key event
};

mpl.figure.prototype.key_event = function (event, name) {
    // Prevent repeat events
    if (name === 'key_press') {
        if (event.key === this._key) {
            return;
        } else {
            this._key = event.key;
        }
    }
    if (name === 'key_release') {
        this._key = null;
    }

    var value = '';
    if (event.ctrlKey && event.key !== 'Control') {
        value += 'ctrl+';
    }
    else if (event.altKey && event.key !== 'Alt') {
        value += 'alt+';
    }
    else if (event.shiftKey && event.key !== 'Shift') {
        value += 'shift+';
    }

    value += 'k' + event.key;

    this._key_event_extra(event, name);

    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });
    return false;
};

mpl.figure.prototype.toolbar_button_onclick = function (name) {
    if (name === 'download') {
        this.handle_save(this, null);
    } else {
        this.send_message('toolbar_button', { name: name });
    }
};

mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {
    this.message.textContent = tooltip;
};

///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////
// prettier-ignore
var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError("Constructor requires 'new' operator");i.set(this,e)}function h(){throw new TypeError("Function is not a constructor")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line
mpl.toolbar_items = [["Home", "Reset original view", "fa fa-home", "home"], ["Back", "Back to previous view", "fa fa-arrow-left", "back"], ["Forward", "Forward to next view", "fa fa-arrow-right", "forward"], ["", "", "", ""], ["Pan", "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect", "fa fa-arrows", "pan"], ["Zoom", "Zoom to rectangle\nx/y fixes axis", "fa fa-square-o", "zoom"], ["", "", "", ""], ["Download", "Download plot", "fa fa-floppy-o", "download"]];

mpl.extensions = ["eps", "jpeg", "pgf", "pdf", "png", "ps", "raw", "svg", "tif", "webp"];

mpl.default_extension = "png";/* global mpl */

var comm_websocket_adapter = function (comm) {
    // Create a "websocket"-like object which calls the given IPython comm
    // object with the appropriate methods. Currently this is a non binary
    // socket, so there is still some room for performance tuning.
    var ws = {};

    ws.binaryType = comm.kernel.ws.binaryType;
    ws.readyState = comm.kernel.ws.readyState;
    function updateReadyState(_event) {
        if (comm.kernel.ws) {
            ws.readyState = comm.kernel.ws.readyState;
        } else {
            ws.readyState = 3; // Closed state.
        }
    }
    comm.kernel.ws.addEventListener('open', updateReadyState);
    comm.kernel.ws.addEventListener('close', updateReadyState);
    comm.kernel.ws.addEventListener('error', updateReadyState);

    ws.close = function () {
        comm.close();
    };
    ws.send = function (m) {
        //console.log('sending', m);
        comm.send(m);
    };
    // Register the callback with on_msg.
    comm.on_msg(function (msg) {
        //console.log('receiving', msg['content']['data'], msg);
        var data = msg['content']['data'];
        if (data['blob'] !== undefined) {
            data = {
                data: new Blob(msg['buffers'], { type: data['blob'] }),
            };
        }
        // Pass the mpl event to the overridden (by mpl) onmessage function.
        ws.onmessage(data);
    });
    return ws;
};

mpl.mpl_figure_comm = function (comm, msg) {
    // This is the function which gets called when the mpl process
    // starts-up an IPython Comm through the "matplotlib" channel.

    var id = msg.content.data.id;
    // Get hold of the div created by the display call when the Comm
    // socket was opened in Python.
    var element = document.getElementById(id);
    var ws_proxy = comm_websocket_adapter(comm);

    function ondownload(figure, _format) {
        window.open(figure.canvas.toDataURL());
    }

    var fig = new mpl.figure(id, ws_proxy, ondownload, element);

    // Call onopen now - mpl needs it, as it is assuming we've passed it a real
    // web socket which is closed, not our websocket->open comm proxy.
    ws_proxy.onopen();

    fig.parent_element = element;
    fig.cell_info = mpl.find_output_cell("<div id='" + id + "'></div>");
    if (!fig.cell_info) {
        console.error('Failed to find cell for figure', id, fig);
        return;
    }
    fig.cell_info[0].output_area.element.on(
        'cleared',
        { fig: fig },
        fig._remove_fig_handler
    );
};

mpl.figure.prototype.handle_close = function (fig, msg) {
    var width = fig.canvas.width / fig.ratio;
    fig.cell_info[0].output_area.element.off(
        'cleared',
        fig._remove_fig_handler
    );
    fig.resizeObserverInstance.unobserve(fig.canvas_div);

    // Update the output cell to use the data from the current canvas.
    fig.push_to_output();
    var dataURL = fig.canvas.toDataURL();
    // Re-enable the keyboard manager in IPython - without this line, in FF,
    // the notebook keyboard shortcuts fail.
    IPython.keyboard_manager.enable();
    fig.parent_element.innerHTML =
        '<img src="' + dataURL + '" width="' + width + '">';
    fig.close_ws(fig, msg);
};

mpl.figure.prototype.close_ws = function (fig, msg) {
    fig.send_message('closing', msg);
    // fig.ws.close()
};

mpl.figure.prototype.push_to_output = function (_remove_interactive) {
    // Turn the data on the canvas into data in the output cell.
    var width = this.canvas.width / this.ratio;
    var dataURL = this.canvas.toDataURL();
    this.cell_info[1]['text/html'] =
        '<img src="' + dataURL + '" width="' + width + '">';
};

mpl.figure.prototype.updated_canvas_event = function () {
    // Tell IPython that the notebook contents must change.
    IPython.notebook.set_dirty(true);
    this.send_message('ack', {});
    var fig = this;
    // Wait a second, then push the new image to the DOM so
    // that it is saved nicely (might be nice to debounce this).
    setTimeout(function () {
        fig.push_to_output();
    }, 1000);
};

mpl.figure.prototype._init_toolbar = function () {
    var fig = this;

    var toolbar = document.createElement('div');
    toolbar.classList = 'btn-toolbar';
    this.root.appendChild(toolbar);

    function on_click_closure(name) {
        return function (_event) {
            return fig.toolbar_button_onclick(name);
        };
    }

    function on_mouseover_closure(tooltip) {
        return function (event) {
            if (!event.currentTarget.disabled) {
                return fig.toolbar_button_onmouseover(tooltip);
            }
        };
    }

    fig.buttons = {};
    var buttonGroup = document.createElement('div');
    buttonGroup.classList = 'btn-group';
    var button;
    for (var toolbar_ind in mpl.toolbar_items) {
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) {
            /* Instead of a spacer, we start a new button group. */
            if (buttonGroup.hasChildNodes()) {
                toolbar.appendChild(buttonGroup);
            }
            buttonGroup = document.createElement('div');
            buttonGroup.classList = 'btn-group';
            continue;
        }

        button = fig.buttons[name] = document.createElement('button');
        button.classList = 'btn btn-default';
        button.href = '#';
        button.title = name;
        button.innerHTML = '<i class="fa ' + image + ' fa-lg"></i>';
        button.addEventListener('click', on_click_closure(method_name));
        button.addEventListener('mouseover', on_mouseover_closure(tooltip));
        buttonGroup.appendChild(button);
    }

    if (buttonGroup.hasChildNodes()) {
        toolbar.appendChild(buttonGroup);
    }

    // Add the status bar.
    var status_bar = document.createElement('span');
    status_bar.classList = 'mpl-message pull-right';
    toolbar.appendChild(status_bar);
    this.message = status_bar;

    // Add the close button to the window.
    var buttongrp = document.createElement('div');
    buttongrp.classList = 'btn-group inline pull-right';
    button = document.createElement('button');
    button.classList = 'btn btn-mini btn-primary';
    button.href = '#';
    button.title = 'Stop Interaction';
    button.innerHTML = '<i class="fa fa-power-off icon-remove icon-large"></i>';
    button.addEventListener('click', function (_evt) {
        fig.handle_close(fig, {});
    });
    button.addEventListener(
        'mouseover',
        on_mouseover_closure('Stop Interaction')
    );
    buttongrp.appendChild(button);
    var titlebar = this.root.querySelector('.ui-dialog-titlebar');
    titlebar.insertBefore(buttongrp, titlebar.firstChild);
};

mpl.figure.prototype._remove_fig_handler = function (event) {
    var fig = event.data.fig;
    if (event.target !== this) {
        // Ignore bubbled events from children.
        return;
    }
    fig.close_ws(fig, {});
};

mpl.figure.prototype._root_extra_style = function (el) {
    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.
};

mpl.figure.prototype._canvas_extra_style = function (el) {
    // this is important to make the div 'focusable
    el.setAttribute('tabindex', 0);
    // reach out to IPython and tell the keyboard manager to turn it's self
    // off when our div gets focus

    // location in version 3
    if (IPython.notebook.keyboard_manager) {
        IPython.notebook.keyboard_manager.register_events(el);
    } else {
        // location in version 2
        IPython.keyboard_manager.register_events(el);
    }
};

mpl.figure.prototype._key_event_extra = function (event, _name) {
    // Check for shift+enter
    if (event.shiftKey && event.which === 13) {
        this.canvas_div.blur();
        // select the cell after this one
        var index = IPython.notebook.find_cell_index(this.cell_info[0]);
        IPython.notebook.select(index + 1);
    }
};

mpl.figure.prototype.handle_save = function (fig, _msg) {
    fig.ondownload(fig, null);
};

mpl.find_output_cell = function (html_output) {
    // Return the cell and output element which can be found *uniquely* in the notebook.
    // Note - this is a bit hacky, but it is done because the "notebook_saving.Notebook"
    // IPython event is triggered only after the cells have been serialised, which for
    // our purposes (turning an active figure into a static one), is too late.
    var cells = IPython.notebook.get_cells();
    var ncells = cells.length;
    for (var i = 0; i < ncells; i++) {
        var cell = cells[i];
        if (cell.cell_type === 'code') {
            for (var j = 0; j < cell.output_area.outputs.length; j++) {
                var data = cell.output_area.outputs[j];
                if (data.data) {
                    // IPython >= 3 moved mimebundle to data attribute of output
                    data = data.data;
                }
                if (data['text/html'] === html_output) {
                    return [cell, data, j];
                }
            }
        }
    }
};

// Register the function which deals with the matplotlib target/channel.
// The kernel may be null if the page has been refreshed.
if (IPython.notebook.kernel !== null) {
    IPython.notebook.kernel.comm_manager.register_target(
        'matplotlib',
        mpl.mpl_figure_comm
    );
}

</script>
</div>
<div class="cell-output cell-output-display">
<div id="cb1342a5-5207-4a1d-b908-4048a9efce35"></div>
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>